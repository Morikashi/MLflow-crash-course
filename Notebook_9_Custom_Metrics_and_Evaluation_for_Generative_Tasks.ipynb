{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 09: Custom Metrics and Evaluation for Generative Tasks\n",
    "\n",
    "Welcome to Notebook 9! In [Notebook 6](MLflow_06_Evaluating_and_Benchmarking_LLMs_with_MLflow.ipynb), we explored standard evaluation metrics for LLMs. However, generative AI tasks often require more nuanced assessment beyond what metrics like ROUGE or BERTScore can capture [5, 7]. The 'quality' of generated text can be highly subjective and task-dependent.\n",
    "\n",
    "This notebook delves into **Custom Metrics for Generative Tasks**. We'll learn how to:\n",
    "- Understand the limitations of standard metrics for certain generative AI qualities.\n",
    "- Define and implement **heuristic-based custom metrics** tailored to specific requirements (e.g., length constraints, keyword presence) [2, 3, 6].\n",
    "- Explore the concept and implementation of **LLM-as-a-Judge custom metrics**, where another LLM is used to score the output based on defined criteria (e.g., helpfulness, coherence, adherence to style) [1, 3, 8].\n",
    "- Integrate these custom metrics into the `mlflow.evaluate()` workflow.\n",
    "- Analyze these richer evaluation results in the MLflow UI to gain deeper insights into model performance.\n",
    "\n",
    "![Custom Evaluation Concept](https://www.comet.com/wp-content/uploads/2023/11/LLM-Eval-Taxonomy.png)\n",
    "\n",
    "By defining what truly matters for your specific use case, custom metrics empower you to build better, more reliable, and more aligned generative AI applications [4].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Recap: Limitations of Standard Generative AI Metrics](#recap-standard-metric-limitations)\n",
    "2. [Setting Up the Custom Evaluation Environment](#setting-up-custom-eval-env)\n",
    "    - [Installing Libraries](#installing-libraries-custom-eval)\n",
    "    - [Ollama and LLM Setup (for Judge and Evaluated LLMs)](#ollama-llm-setup-custom-eval)\n",
    "    - [Configuring MLflow](#configuring-mlflow-custom-eval)\n",
    "3. [Task, Dataset, and Models Under Evaluation](#task-dataset-model-custom-eval)\n",
    "    - [Task: Text Summarization (Revisited)](#task-text-summarization-revisited)\n",
    "    - [Dataset: `openai/summarize_from_feedback` (TLDR subset)](#dataset-summarize-feedback-revisited)\n",
    "    - [Models to Evaluate: `gemma2:2b` and `phi3:mini` (from Ollama)](#models-to-evaluate)\n",
    "    - [Judge LLM: `qwen2:1.5b` (from Ollama)](#judge-llm-setup)\n",
    "4. [Defining Custom Heuristic-Based Metrics](#defining-custom-heuristic-metrics)\n",
    "    - [Using `mlflow.metrics.make_metric`](#mlflow-make-metric)\n",
    "    - [Example 1: Summary Length Ratio](#custom-metric-length-ratio)\n",
    "    - [Example 2: Keyword Presence Check](#custom-metric-keyword-presence)\n",
    "5. [Defining Custom LLM-as-a-Judge Metrics](#defining-custom-llm-judge-metrics)\n",
    "    - [Concept: LLM as an Evaluator](#concept-llm-as-judge)\n",
    "    - [Example: \"Summary Helpfulness\" Judge (using `qwen2:1.5b`)](#custom-metric-helpfulness-judge)\n",
    "        - Defining the Judge LLM Prompt and Rating Scale [1]\n",
    "        - Implementing the `eval_fn` to call the Judge LLM\n",
    "6. [Evaluating with Custom Metrics using `mlflow.evaluate()`](#evaluating-with-custom-metrics)\n",
    "    - [Preparing Evaluation Data with `custom_expected` fields [2, 6]](#preparing-eval-data-custom)\n",
    "    - [Running the Evaluation for `gemma2:2b` and `phi3:mini`](#running-evaluation-custom)\n",
    "7. [Analyzing Custom Metric Results in MLflow UI](#analyzing-custom-results-mlflow-ui)\n",
    "8. [Best Practices for Developing Custom Metrics [2, 4, 6]](#best-practices-custom-metrics)\n",
    "9. [Key Takeaways](#key-takeaways-custom-eval)\n",
    "10. [Engaging Resources and Further Reading](#resources-further-reading-custom-eval)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: Limitations of Standard Generative AI Metrics\n",
    "\n",
    "In [Notebook 6](MLflow_06_Evaluating_and_Benchmarking_LLMs_with_MLflow.ipynb), we used metrics like ROUGE and BERTScore. While valuable, they primarily measure surface-level lexical overlap or semantic similarity with reference texts. They might not fully capture [5, 7]:\n",
    "- **Factual Correctness/Faithfulness:** Does the generation accurately reflect provided context (if any) or known facts?\n",
    "- **Coherence and Readability:** Is the text well-structured and easy to understand?\n",
    "- **Adherence to Instructions/Style:** Does the model follow specific formatting, tone, or persona requirements?\n",
    "- **Helpfulness/Relevance:** Is the output actually useful or relevant to the user's query or task?\n",
    "- **Creativity and Novelty:** For creative tasks, are the outputs original and engaging?\n",
    "- **Absence of Undesirable Content:** Metrics for toxicity, bias, PII leakage [5].\n",
    "\n",
    "**Custom metrics** allow us to define evaluation criteria that are more closely aligned with these nuanced aspects and specific business goals [2, 4].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Custom Evaluation Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow \"transformers>=4.30.0\" datasets evaluate \"langchain>=0.1.0\" langchain_community langchain_core langchain_ollama pydantic tiktoken rouge_score bert_score sentencepiece accelerate\n",
    "# Ensure transformers is compatible with the models, langchain for ChatOllama\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset # Ensure Dataset is imported\n",
    "from transformers import pipeline # Using pipeline for easier model wrapping\n",
    "from mlflow.metrics import make_metric, MetricValue # For custom heuristic metrics [3]\n",
    "# from mlflow.models.evaluation.base import EvaluationResult # Not strictly needed for this example\n",
    "from langchain_ollama.chat_models import ChatOllama # For LLM-as-a-Judge\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import re # For keyword checking\n",
    "import json # For LLM-as-Judge response parsing\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "import transformers\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama and LLM Setup (for Judge and Evaluated LLMs)\n",
    "Ensure Ollama is installed and running. We'll need to pull `qwen2:1.5b` (for the judge), `gemma2:2b`, and `phi3:mini` (for evaluation).\n",
    "\n",
    "In your terminal, run:\n",
    "`ollama pull qwen2:1.5b`\n",
    "`ollama pull gemma2:2b`\n",
    "`ollama pull phi3:mini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_judge_model_name = \"qwen2:1.5b\"\n",
    "ollama_eval_model_1_name = \"gemma2:2b\"\n",
    "ollama_eval_model_2_name = \"phi3:mini\"\n",
    "judge_llm = None\n",
    "\n",
    "try:\n",
    "    judge_llm = ChatOllama(\n",
    "        model=ollama_judge_model_name, \n",
    "        temperature=0.1, # Low temperature for consistent judging\n",
    "        keep_alive=\"5m\",\n",
    "        format=\"json\" # Request JSON output from judge for easier parsing of scores/reasoning\n",
    "    )\n",
    "    response_test = judge_llm.invoke([SystemMessage(content=\"Output a JSON with a key 'status' and value 'ok'.\"), HumanMessage(content=\"Test prompt.\")])\n",
    "    print(f\"Judge LLM ({ollama_judge_model_name}) connected. Test response: {response_test.content[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Judge LLM ({ollama_judge_model_name}): {e}. Ensure Ollama is running and model is pulled.\")\n",
    "    judge_llm = None # Set to None if connection fails, so dependent cells can check\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns')\n",
    "experiment_name = \"LLM_Custom_Metrics_Summarization_Ollama\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task, Dataset, and Models Under Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Text Summarization (Revisited)\n",
    "We'll stick to text summarization to demonstrate custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: `openai/summarize_from_feedback` (TLDR subset)\n",
    "Reusing the dataset from Notebook 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"openai/summarize_from_feedback\"\n",
    "dataset_config_name = \"tldr\"\n",
    "num_eval_samples = 20 # Smaller subset for quicker custom metric development\n",
    "\n",
    "try:\n",
    "    eval_dataset_full = load_dataset(dataset_name, dataset_config_name, split=\"validation\")\n",
    "    eval_dataset = eval_dataset_full.select(range(num_eval_samples))\n",
    "    print(f\"Loaded {len(eval_dataset)} samples from '{dataset_name}/{dataset_config_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}. Using dummy data.\")\n",
    "    dummy_data = {\n",
    "        'info': [{'post': 'This is a long post about the benefits of MLflow for MLOps. MLflow helps track experiments, package models, and manage the ML lifecycle efficiently, with features for reproducibility.'}] * num_eval_samples,\n",
    "        'summaries': [[{'text': 'MLflow is great for MLOps providing efficiency.'}]] * num_eval_samples\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(dummy_data)\n",
    "    print(f\"Using dummy dataset with {len(eval_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to Evaluate: `gemma2:2b` and `phi3:mini` (from Ollama)\n",
    "We'll evaluate these two pre-trained models available via Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge LLM: `qwen2:1.5b` (from Ollama)\n",
    "Our LLM-as-a-Judge metrics will be powered by `qwen2:1.5b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to create a wrapped model for mlflow.evaluate\n",
    "class OllamaSummarizationWrapper:\n",
    "    def __init__(self, ollama_model_name, prompt_template=\"Summarize the following text concisely:\\n\\n{text}\\n\\nSummary:\", max_new_toks=100):\n",
    "        self.ollama_model_name = ollama_model_name\n",
    "        self.prompt_template = prompt_template\n",
    "        self.max_new_tokens = max_new_toks\n",
    "        self.llm = ChatOllama(model=ollama_model_name, temperature=0, keep_alive=\"1m\") # Keep alive shorter during eval\n",
    "        print(f\"Initialized Ollama wrapper for {ollama_model_name}\")\n",
    "\n",
    "    def predict(self, X_df):\n",
    "        if isinstance(X_df, pd.DataFrame):\n",
    "            texts_to_summarize = X_df['inputs'].tolist()\n",
    "        elif isinstance(X_df, pd.Series):\n",
    "            texts_to_summarize = X_df.tolist()\n",
    "        else:\n",
    "            raise ValueError(\"Input to predict should be a Pandas DataFrame or Series with an 'inputs' column.\")\n",
    "\n",
    "        summaries = []\n",
    "        for text_input in texts_to_summarize:\n",
    "            prompt = self.prompt_template.format(text=text_input)\n",
    "            try:\n",
    "                # Note: ChatOllama's invoke might not directly support max_new_tokens in the same way a pipeline does.\n",
    "                # It's better to rely on the model's default generation length or control it via system prompt if possible,\n",
    "                # or use a LangChain chain that explicitly handles output parsing and length.\n",
    "                # For simplicity here, we'll assume the LLM gives a reasonable summary length or we post-process.\n",
    "                response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "                summary = response.content.strip()\n",
    "                # Crude truncation if needed (not ideal, but for consistency in demo)\n",
    "                summary = \" \".join(summary.split()[:self.max_new_tokens]) \n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction with {self.ollama_model_name} for input '{text_input[:50]}...': {e}\")\n",
    "                summary = \"Error generating summary.\"\n",
    "            summaries.append(summary)\n",
    "        return pd.Series(summaries)\n",
    "\n",
    "print(f\"Models to evaluate: {ollama_eval_model_1_name}, {ollama_eval_model_2_name}\")\n",
    "print(f\"Judge LLM: {ollama_judge_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Custom Heuristic-Based Metrics\n",
    "These are the same heuristic metrics as before: Summary Length Ratio and Keyword Presence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `mlflow.metrics.make_metric`\n",
    "The `eval_fn` takes `predictions` and `targets` and returns `mlflow.metrics.MetricValue` [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Summary Length Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_length_ratio_eval_fn(predictions, targets, **kwargs):\n",
    "    ratios = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        len_pred = len(str(pred).split()) \n",
    "        len_target = len(str(target).split())\n",
    "        if len_target == 0:\n",
    "            ratios.append(0.0 if len_pred > 0 else 1.0)\n",
    "        else:\n",
    "            ratios.append(len_pred / len_target)\n",
    "    \n",
    "    return MetricValue(\n",
    "        scores=ratios, \n",
    "        aggregate_results={\n",
    "            \"mean_length_ratio\": np.mean(ratios),\n",
    "            \"std_dev_length_ratio\": np.std(ratios)\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_length_ratio_metric = make_metric(\n",
    "    eval_fn=summary_length_ratio_eval_fn,\n",
    "    greater_is_better=False, \n",
    "    name=\"summary_length_ratio\"\n",
    ")\n",
    "print(\"Custom metric 'summary_length_ratio' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Keyword Presence Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_presence_eval_fn(predictions, targets, custom_expected_list):\n",
    "    scores = []\n",
    "    details = [] \n",
    "\n",
    "    for i, pred_obj in enumerate(predictions):\n",
    "        pred = str(pred_obj) # Ensure prediction is a string\n",
    "        current_custom_expected = custom_expected_list[i]\n",
    "        required_keywords = current_custom_expected.get(\"required_keywords\", [])\n",
    "        if not required_keywords:\n",
    "            scores.append(1.0)\n",
    "            details.append({\"found\": [], \"missing\": [], \"all_required\": []})\n",
    "            continue\n",
    "\n",
    "        found_count = 0\n",
    "        found_kws = []\n",
    "        missing_kws = []\n",
    "        pred_lower = pred.lower()\n",
    "        for kw in required_keywords:\n",
    "            if re.search(r'\\b' + re.escape(kw.lower()) + r'\\b', pred_lower):\n",
    "                found_count += 1\n",
    "                found_kws.append(kw)\n",
    "            else:\n",
    "                missing_kws.append(kw)\n",
    "        \n",
    "        scores.append(found_count / len(required_keywords) if required_keywords else 1.0)\n",
    "        details.append({\"found\": found_kws, \"missing\": missing_kws, \"all_required\": required_keywords})\n",
    "\n",
    "    all_found_keywords_overall = sum([len(d['found']) for d in details])\n",
    "    all_required_keywords_overall = sum([len(d['all_required']) for d in details])\n",
    "    overall_hit_rate = all_found_keywords_overall / all_required_keywords_overall if all_required_keywords_overall > 0 else 1.0\n",
    "\n",
    "    return MetricValue(\n",
    "        scores=scores,\n",
    "        aggregate_results={\n",
    "            \"mean_keyword_hit_rate\": np.mean(scores),\n",
    "            \"overall_keyword_hit_rate\": overall_hit_rate\n",
    "        }\n",
    "    )\n",
    "\n",
    "keyword_presence_metric = make_metric(\n",
    "    eval_fn=keyword_presence_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"keyword_presence_score\"\n",
    ")\n",
    "print(\"Custom metric 'keyword_presence_score' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining Custom LLM-as-a-Judge Metrics\n",
    "We use `qwen2:1.5b` as our judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: LLM as an Evaluator\n",
    "The judge LLM (`qwen2:1.5b`) gets the input, generated output, and a prompt instructing it how to score based on criteria like coherence, relevance, or helpfulness [1, 3, 8]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \"Summary Helpfulness\" Judge (using `qwen2:1.5b`)\n",
    "This judge will assess the helpfulness of the generated summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Judge LLM Prompt and Rating Scale [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_JUDGE_HELPFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant tasked with evaluating the helpfulness of a generated summary.\n",
    "Consider the original text and the generated summary provided below.\n",
    "Rate the helpfulness of the *generated summary* on a scale of 1 to 5, where:\n",
    "1: Not helpful at all. The summary is irrelevant, nonsensical, or completely misses the main points.\n",
    "2: Slightly helpful. The summary touches on some aspects but is largely incomplete or unclear.\n",
    "3: Moderately helpful. The summary captures some main points but could be significantly improved in clarity or conciseness.\n",
    "4: Very helpful. The summary is clear, concise, and accurately reflects the main essence of the original text.\n",
    "5: Extremely helpful. The summary is outstanding, perfectly capturing the core message with excellent clarity and conciseness.\n",
    "\n",
    "Original Text:\n",
    "---BEGIN ORIGINAL TEXT---\n",
    "{original_text}\n",
    "---END ORIGINAL TEXT---\n",
    "\n",
    "Generated Summary:\n",
    "---BEGIN GENERATED SUMMARY---\n",
    "{generated_summary}\n",
    "---END GENERATED SUMMARY---\n",
    "\n",
    "Based on the criteria, provide your evaluation ONLY as a JSON object with two keys: \"score\" (an integer from 1 to 5) and \"reasoning\" (a brief explanation for your score, max 30 words).\n",
    "Example JSON: {{\"score\": 4, \"reasoning\": \"The summary is quite clear and captures the main points well, making it useful.\"}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the `eval_fn` to call the Judge LLM (`qwen2:1.5b`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_helpfulness_judge_eval_fn(predictions, targets, inputs):\n",
    "    if judge_llm is None:\n",
    "        print(\"Judge LLM (qwen2:1.5b) not available, skipping helpfulness metric.\")\n",
    "        return MetricValue(scores=[np.nan]*len(predictions), aggregate_results={\"mean_helpfulness_score\": np.nan})\n",
    "    \n",
    "    scores = []\n",
    "    # all_reasonings = [] # Could collect these for artifact logging if desired\n",
    "\n",
    "    for i, generated_summary_obj in enumerate(predictions):\n",
    "        generated_summary = str(generated_summary_obj) # Ensure string\n",
    "        original_text = str(inputs[i]) # Ensure string\n",
    "        \n",
    "        prompt_for_judge = CUSTOM_JUDGE_HELPFULNESS_PROMPT_TEMPLATE.format(\n",
    "            original_text=original_text,\n",
    "            generated_summary=generated_summary\n",
    "        )\n",
    "        \n",
    "        score = 0 # Default score in case of error\n",
    "        reasoning = \"Judge LLM call or parsing failed.\"\n",
    "        try:\n",
    "            # Using SystemMessage for role and HumanMessage for the task prompt\n",
    "            judge_response_msg = judge_llm.invoke([\n",
    "                SystemMessage(content=\"You are an AI assistant that provides evaluations in JSON format according to the user's instructions.\"),\n",
    "                HumanMessage(content=prompt_for_judge)\n",
    "            ])\n",
    "            judge_response_content = judge_response_msg.content\n",
    "            \n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', judge_response_content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    parsed_response = json.loads(json_match.group(0))\n",
    "                    score = int(parsed_response.get(\"score\", 0))\n",
    "                    reasoning = parsed_response.get(\"reasoning\", \"No reasoning provided.\")\n",
    "                else:\n",
    "                    print(f\"Warning: Judge LLM (qwen2:1.5b) did not output valid JSON for item {i}. Response: {judge_response_content}\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Judge LLM (qwen2:1.5b) output for item {i} was not valid JSON: {judge_response_content}\")\n",
    "            except Exception as e_parse:\n",
    "                print(f\"Warning: Error parsing judge (qwen2:1.5b) response for item {i}: {e_parse}. Response: {judge_response_content}\")\n",
    "\n",
    "        except Exception as e_judge_call:\n",
    "            print(f\"Error calling Judge LLM (qwen2:1.5b) for item {i}: {e_judge_call}\")\n",
    "            \n",
    "        scores.append(score)\n",
    "        # all_reasonings.append({\"input\": original_text, \"prediction\": generated_summary, \"score\": score, \"reasoning\": reasoning})\n",
    "\n",
    "    valid_scores = [s for s in scores if isinstance(s, (int, float)) and 0 < s <= 5] # Filter out 0s from errors\n",
    "    mean_score = np.mean(valid_scores) if valid_scores else np.nan\n",
    "\n",
    "    return MetricValue(\n",
    "        scores=scores, \n",
    "        aggregate_results={\"mean_helpfulness_score\": mean_score}\n",
    "    )\n",
    "\n",
    "summary_helpfulness_judge_metric = make_metric(\n",
    "    eval_fn=summary_helpfulness_judge_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"summary_helpfulness_qwen2_1.5b_judge\"\n",
    ")\n",
    "print(\"Custom LLM-as-a-Judge metric 'summary_helpfulness_qwen2_1.5b_judge' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating with Custom Metrics using `mlflow.evaluate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Evaluation Data with `custom_expected` fields [2, 6]\n",
    "For the `keyword_presence_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_custom = pd.DataFrame({\n",
    "    \"inputs\": [entry['info']['post'] for entry in eval_dataset],\n",
    "    \"targets\": [entry['summaries'][0]['text'] for entry in eval_dataset],\n",
    "    \"custom_expected\": [\n",
    "        {\"required_keywords\": [\"MLflow\", \"lifecycle\"]} if \"mlflow\" in entry['info']['post'].lower() \n",
    "        else {\"required_keywords\": [\"summary\", \"text\"]} # Default keywords\n",
    "        for entry in eval_dataset\n",
    "    ]\n",
    "})\n",
    "print(\"Evaluation DataFrame with 'custom_expected' prepared:\")\n",
    "print(eval_df_custom.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation for `gemma2:2b` and `phi3:mini`\n",
    "We'll iterate through our selected models and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate_ollama = {\n",
    "    ollama_eval_model_1_name: OllamaSummarizationWrapper(ollama_eval_model_1_name),\n",
    "    ollama_eval_model_2_name: OllamaSummarizationWrapper(ollama_eval_model_2_name)\n",
    "}\n",
    "\n",
    "custom_metrics_list = [\n",
    "    summary_length_ratio_metric,\n",
    "    keyword_presence_metric,\n",
    "    summary_helpfulness_judge_metric \n",
    "]\n",
    "\n",
    "for model_key, wrapped_model_instance in models_to_evaluate_ollama.items():\n",
    "    if wrapped_model_instance.llm is None: # Check if Ollama model initialized in wrapper\n",
    "        print(f\"Skipping evaluation for {model_key} as its LLM failed to initialize.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Evaluating model: {model_key} ---\")\n",
    "    with mlflow.start_run(run_name=f\"Eval_{model_key.replace(':', '_')}_Summarization_CustomMetrics\") as run:\n",
    "        mlflow.log_param(\"model_name\", model_key)\n",
    "        mlflow.log_param(\"evaluation_task\", \"text-summarization-custom\")\n",
    "        mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name}\")\n",
    "        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "        mlflow.log_param(\"judge_llm_for_helpfulness\", ollama_judge_model_name if judge_llm else \"N/A\")\n",
    "        mlflow.set_tag(\"evaluation_type\", \"custom_metrics_focused\")\n",
    "        mlflow.set_tag(\"model_source\", \"Ollama\")\n",
    "\n",
    "        print(f\"Starting mlflow.evaluate for {model_key} with custom metrics...\")\n",
    "        try:\n",
    "            custom_eval_results = mlflow.evaluate(\n",
    "                model=wrapped_model_instance,\n",
    "                data=eval_df_custom.copy(), # Pass a copy to be safe\n",
    "                targets=\"targets\",\n",
    "                feature_names=[\"inputs\"], \n",
    "                model_type=\"text-summarization\", # This helps mlflow pick some default metrics if available\n",
    "                extra_metrics=custom_metrics_list,\n",
    "                # Example: To only use custom metrics and disable defaults, one might need to clear default metrics first or check API.\n",
    "                # For now, we'll let it add defaults like ROUGE if it can infer them.\n",
    "            )\n",
    "            print(f\"\\nCustom evaluation results for {model_key}:\")\n",
    "            for metric_name, value in custom_eval_results.metrics.items():\n",
    "                print(f\"  {metric_name}: {value}\")\n",
    "            \n",
    "            if custom_eval_results.artifacts and \"eval_results_table.json\" in custom_eval_results.artifacts:\n",
    "                 print(f\"  Detailed evaluation table artifact path: {custom_eval_results.artifacts['eval_results_table.json'].uri}\")\n",
    "        \n",
    "        except Exception as e_eval:\n",
    "            print(f\"Error during mlflow.evaluate for {model_key}: {e_eval}\")\n",
    "            mlflow.log_text(str(e_eval), f\"evaluation_error_{model_key}.txt\")\n",
    "    clear_gpu_cache() # Clear cache between model evaluations if on GPU\n",
    "\n",
    "# Clean up judge LLM if it was loaded\n",
    "if judge_llm is not None:\n",
    "    # Depending on langchain/ollama, direct cleanup might not be exposed.\n",
    "    # keep_alive=\"1m\" helps Ollama unload it automatically after a short period.\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Custom Metric Results in MLflow UI\n",
    "\n",
    "Launch the MLflow UI (`mlflow ui`) and navigate to the `LLM_Custom_Metrics_Summarization_Ollama` experiment.\n",
    "\n",
    "- **Compare Runs:** Select the runs for `gemma2:2b` and `phi3:mini` and click \"Compare\".\n",
    "- **Metrics Section:** You'll see your custom metrics:\n",
    "    - `summary_length_ratio/mean_length_ratio`\n",
    "    - `keyword_presence_score/mean_keyword_hit_rate`\n",
    "    - `summary_helpfulness_qwen2_1.5b_judge/mean_helpfulness_score`\n",
    "    - ... and any default metrics MLflow added.\n",
    "- **Analyze:** Which model performed better on helpfulness according to `qwen2:1.5b`? Which had a better length profile or keyword coverage for this task?\n",
    "- **Artifacts:** The `eval_results_table.json` for each run will show per-sample scores for all metrics, allowing you to drill down into specific examples where models differed or where the judge provided interesting reasoning (if you were to log the judge's reasoning as a separate artifact).\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)\n",
    "\n",
    "This comparative view, enriched by custom metrics, offers a much deeper understanding than relying on generic scores alone.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Developing Custom Metrics [2, 4, 6]\n",
    "\n",
    "- **Define Clear Criteria:** Your custom metric should measure a well-defined, specific aspect of quality or performance that matters for your application [4].\n",
    "- **Iterative Development [2, 6]:**\n",
    "    1.  **Generate an \"Answer Sheet\":** Run your model on an evaluation dataset once and save its predictions.\n",
    "    2.  **Develop Metric `eval_fn`:** Write your custom metric function and test it directly on the saved predictions/inputs.\n",
    "    3.  **Validate with `mlflow.evaluate` on Answer Sheet:** Run `mlflow.evaluate` using the pre-generated answer sheet (by *not* passing the `model` argument but providing the answer sheet as `data` with predictions).\n",
    "    4.  **Full Evaluation:** Run `mlflow.evaluate` with the actual model.\n",
    "- **Consider Cost and Latency:** LLM-as-a-Judge metrics add cost (judge LLM tokens) and latency. Heuristic metrics are faster.\n",
    "- **Judge LLM Reliability & Bias:** The quality of LLM-as-a-Judge metrics depends on the judge LLM's capability and its own potential biases. Use a strong judge model and clear, unbiased prompts. Consider using multiple judge LLMs or averaging scores.\n",
    "- **Combine with Standard Metrics and Human Review:** Custom metrics provide additional dimensions but shouldn't entirely replace standard metrics or qualitative human assessment.\n",
    "- **Version Your Metrics:** Metric definitions can evolve. Track their versions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "This notebook has equipped you with advanced techniques for evaluating generative AI models:\n",
    "\n",
    "- **Tailored Evaluation:** You can now define custom metrics that precisely measure what's important for your specific generative AI task.\n",
    "- **Heuristic Metrics:** Implemented programmatic custom metrics (length ratio, keyword presence) using `mlflow.metrics.make_metric`.\n",
    "- **LLM-as-a-Judge:** Implemented a custom LLM-as-a-Judge metric (using `qwen2:1.5b`) to assess \"helpfulness.\"\n",
    "- **Integration with `mlflow.evaluate`:** Seamlessly incorporated these diverse custom metrics into the MLflow evaluation workflow.\n",
    "- **Deeper Insights:** Custom metrics, when analyzed in MLflow, provide a richer understanding of model performance beyond standard scores.\n",
    "\n",
    "Mastering custom evaluation is crucial for effectively iterating on and improving your generative AI models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Engaging Resources and Further Reading\n",
    "\n",
    "- **MLflow Documentation:**\n",
    "    - [MLflow LLM Evaluate - Custom Metrics Section](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#create-custom-heuristic-based-llm-evaluation-metrics) [3]\n",
    "    - [MLflow `make_metric` API](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.make_metric)\n",
    "- **Databricks Documentation (Mosaic AI Agent Evaluation):**\n",
    "    - [Custom Metrics Guide (Conceptual Overlap with MLflow)](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html) [2, 6]\n",
    "- **Cloud Provider Custom Metrics (for conceptual understanding of LLM-as-a-Judge structure):**\n",
    "    - [AWS Bedrock - Custom Metrics for GenAI Evaluation](https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/) [1]\n",
    "    - [Google Vertex AI - Evaluating Generative AI](https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models) (see also the LinkedIn article [8] referencing Google SDK)\n",
    "- **General Best Practices:**\n",
    "    - [Microsoft Tech Community: Evaluating generative AI: Best practices for developers](https://techcommunity.microsoft.com/blog/azuredevcommunityblog/evaluating-generative-ai-best-practices-for-developers/4271488) [4]\n",
    "    - [DataRobot Blog: Design and Monitor Custom Metrics for Generative AI](https://www.datarobot.com/blog/design-and-monitor-custom-metrics-for-generative-ai-use-cases-in-datarobot-ai-platform/) [5]\n",
    "\n",
    "--- \n",
    "\n",
    "Excellent work! You've now explored a critical aspect of maturing your generative AI development process by implementing and using custom evaluation metrics.\n",
    "\n",
    "**Coming Up Next (Notebook 10):** We'll aim to synthesize several concepts by building a more comprehensive End-to-End GenAI Application, potentially combining RAG with function-calling agents, all tracked and managed with MLflow.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
