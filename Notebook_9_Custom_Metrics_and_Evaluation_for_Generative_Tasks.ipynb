{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 09: Custom Metrics and Evaluation for Generative Tasks\n",
    "\n",
    "Welcome to Notebook 9! In [Notebook 6](MLflow_06_Evaluating_and_Benchmarking_LLMs_with_MLflow.ipynb), we explored standard evaluation metrics for LLMs. However, generative AI tasks often require more nuanced assessment beyond what metrics like ROUGE or BERTScore can capture [5, 7]. The 'quality' of generated text can be highly subjective and task-dependent.\n",
    "\n",
    "This notebook delves into **Custom Metrics for Generative Tasks**. We'll learn how to:\n",
    "- Understand the limitations of standard metrics for certain generative AI qualities.\n",
    "- Define and implement **heuristic-based custom metrics** tailored to specific requirements (e.g., length constraints, keyword presence) [2, 3, 6].\n",
    "- Explore the concept and implementation of **LLM-as-a-Judge custom metrics**, where another LLM (like Qwen3-1.7B) is used to score the output based on defined criteria (e.g., helpfulness, coherence) [1, 3, 8].\n",
    "- Evaluate different LLMs (like Gemma-2B and DeepSeek-Coder-1.3B-Instruct) using these custom metrics within the `mlflow.evaluate()` workflow.\n",
    "- Analyze these richer evaluation results in the MLflow UI to gain deeper insights into model performance.\n",
    "\n",
    "![Custom Evaluation Concept](https://www.comet.com/wp-content/uploads/2023/11/LLM-Eval-Taxonomy.png)\n",
    "\n",
    "By defining what truly matters for your specific use case, custom metrics empower you to build better, more reliable, and more aligned generative AI applications [4].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Recap: Limitations of Standard Generative AI Metrics](#recap-standard-metric-limitations)\n",
    "2. [Setting Up the Custom Evaluation Environment](#setting-up-custom-eval-env)\n",
    "    - [Installing Libraries](#installing-libraries-custom-eval)\n",
    "    - [Ollama and LLM Setup (Judge LLM: Qwen3-1.7B)](#ollama-llm-setup-custom-eval)\n",
    "    - [Configuring MLflow](#configuring-mlflow-custom-eval)\n",
    "3. [Task, Dataset, and Models Under Evaluation](#task-dataset-model-custom-eval)\n",
    "    - [Task: Text Summarization (Revisited)](#task-text-summarization-revisited)\n",
    "    - [Dataset: `openai/summarize_from_feedback` (TLDR subset)](#dataset-summarize-feedback-revisited)\n",
    "    - [Models to Evaluate: Gemma-2B and DeepSeek-Coder-1.3B-Instruct](#models-to-evaluate-new)\n",
    "4. [Defining Custom Heuristic-Based Metrics](#defining-custom-heuristic-metrics)\n",
    "    - [Using `mlflow.metrics.make_metric`](#mlflow-make-metric)\n",
    "    - [Example 1: Summary Length Ratio](#custom-metric-length-ratio)\n",
    "    - [Example 2: Keyword Presence Check](#custom-metric-keyword-presence)\n",
    "5. [Defining Custom LLM-as-a-Judge Metrics (with Qwen3-1.7B Judge)](#defining-custom-llm-judge-metrics)\n",
    "    - [Concept: LLM as an Evaluator](#concept-llm-as-judge)\n",
    "    - [Example: General Summary Helpfulness Judge](#custom-metric-summary-helpfulness)\n",
    "        - Defining the Judge LLM Prompt and Rating Scale [1]\n",
    "        - Implementing the `eval_fn` to call the Qwen3-1.7B Judge LLM\n",
    "6. [Evaluating Gemma-2B and DeepSeek-Coder with Custom Metrics](#evaluating-with-custom-metrics)\n",
    "    - [Preparing Evaluation Data with `custom_expected` fields [2, 6]](#preparing-eval-data-custom)\n",
    "    - [Running Evaluation for Gemma-2B](#running-evaluation-gemma)\n",
    "    - [Running Evaluation for DeepSeek-Coder-1.3B-Instruct](#running-evaluation-deepseek)\n",
    "7. [Analyzing Custom Metric Results in MLflow UI](#analyzing-custom-results-mlflow-ui)\n",
    "8. [Best Practices for Developing Custom Metrics [2, 4, 6]](#best-practices-custom-metrics)\n",
    "9. [Key Takeaways](#key-takeaways-custom-eval)\n",
    "10. [Engaging Resources and Further Reading](#resources-further-reading-custom-eval)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: Limitations of Standard Generative AI Metrics\n",
    "\n",
    "In [Notebook 6](MLflow_06_Evaluating_and_Benchmarking_LLMs_with_MLflow.ipynb), we used metrics like ROUGE and BERTScore. While valuable, they primarily measure surface-level lexical overlap or semantic similarity with reference texts. They might not fully capture [5, 7]:\n",
    "- **Factual Correctness/Faithfulness:** Does the generation accurately reflect provided context (if any) or known facts?\n",
    "- **Coherence and Readability:** Is the text well-structured and easy to understand?\n",
    "- **Adherence to Instructions/Style:** Does the model follow specific formatting, tone, or persona requirements?\n",
    "- **Helpfulness or Relevance:** Is the output actually useful or relevant to the user's query or task?\n",
    "- **Creativity and Novelty:** For creative tasks, are the outputs original and engaging?\n",
    "- **Absence of Undesirable Content:** Metrics for toxicity, bias, PII leakage [5].\n",
    "\n",
    "**Custom metrics** allow us to define evaluation criteria that are more closely aligned with these nuanced aspects and specific business goals [2, 4].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Custom Evaluation Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow \"transformers>=4.51.0\" datasets evaluate peft bitsandbytes sentencepiece accelerate rouge_score bert_score langchain langchain_community langchain_core langchain_ollama\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset # Ensure Dataset is imported\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "# PEFT might not be needed if we are evaluating base models from Ollama directly\n",
    "# from peft import PeftModel \n",
    "from mlflow.metrics import make_metric, MetricValue # For custom heuristic metrics [3]\n",
    "from mlflow.models.evaluation.base import EvaluationResult # For structuring custom eval output\n",
    "from langchain_ollama.chat_models import ChatOllama # For LLM-as-a-Judge\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import re # For keyword checking\n",
    "import json # For parsing judge output\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "import transformers\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama and LLM Setup (Judge LLM: Qwen3-1.7B)\n",
    "We'll use `Qwen3-1.7B` as our judge LLM. Ensure Ollama is running and this model is pulled (`ollama pull qwen3:1.7b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_judge_model_name = \"qwen3:1.7b\" # Qwen3-1.7B for judging [3, 4]\n",
    "judge_llm = None\n",
    "try:\n",
    "    judge_llm = ChatOllama(\n",
    "        model=ollama_judge_model_name, \n",
    "        temperature=0.1, # Low temperature for consistent judging\n",
    "        keep_alive=\"5m\",\n",
    "        format=\"json\" # Request JSON output from judge for easier parsing of scores/reasoning\n",
    "    )\n",
    "    # Test with a simple prompt expecting JSON\n",
    "    judge_test_prompt = \"Provide a JSON object with a key 'greeting' and value 'hello judge'.\"\n",
    "    response_test = judge_llm.invoke([SystemMessage(content=\"You are an AI assistant that responds in JSON.\"), \n",
    "                                        HumanMessage(content=judge_test_prompt)])\n",
    "    print(f\"Judge LLM ({ollama_judge_model_name}) connected. Test response: {response_test.content[:100]}...\")\n",
    "    # Check if response is valid JSON (simple check)\n",
    "    try:\n",
    "        json.loads(response_test.content)\n",
    "        print(\"Judge LLM produced valid JSON for test prompt.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Judge LLM test response was not valid JSON. Check model's JSON mode capability.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Judge LLM ({ollama_judge_model_name}): {e}. Ensure Ollama is running and model is pulled.\")\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns')\n",
    "experiment_name = \"LLM_Custom_Metrics_Generative_Tasks_NewModels\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task, Dataset, and Models Under Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Text Summarization (Revisited)\n",
    "We'll stick to text summarization to demonstrate custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: `openai/summarize_from_feedback` (TLDR subset)\n",
    "Reusing the dataset from Notebook 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_hf = \"openai/summarize_from_feedback\"\n",
    "dataset_config_name_hf = \"tldr\"\n",
    "num_eval_samples = 20 # Smaller subset for quicker custom metric development\n",
    "\n",
    "try:\n",
    "    eval_dataset_full = load_dataset(dataset_name_hf, dataset_config_name_hf, split=\"validation\")\n",
    "    eval_dataset = eval_dataset_full.select(range(num_eval_samples))\n",
    "    print(f\"Loaded {len(eval_dataset)} samples from '{dataset_name_hf}/{dataset_config_name_hf}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}. Using dummy data.\")\n",
    "    dummy_data_dict = {\n",
    "        'info': [{'post': 'This is a long post about the benefits of MLflow. It helps track experiments and manage the ML lifecycle efficiently, with features for reproducibility.'}] * num_eval_samples,\n",
    "        'summaries': [[{'text': 'MLflow is great for MLOps providing efficiency.'}]] * num_eval_samples\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(dummy_data_dict)\n",
    "    print(f\"Using dummy dataset with {len(eval_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to Evaluate: Gemma-2B and DeepSeek-Coder-1.3B-Instruct\n",
    "We will evaluate two different models available via Ollama:\n",
    "1.  **`gemma:2b`**: Google's Gemma 2B model.\n",
    "2.  **`deepseek-coder:1.3b-instruct`**: A smaller, instruction-tuned version of DeepSeek Coder.\n",
    "\n",
    "Ensure you have these models pulled in Ollama: \n",
    "`ollama pull gemma:2b`\n",
    "`ollama pull deepseek-coder:1.3b-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a helper function to get a wrapped model pipeline for evaluation, similar to Notebook 6, but using Ollama models directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaSummarizationWrapper:\n",
    "    def __init__(self, ollama_model_tag, prompt_template=\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\", max_new_toks=100):\n",
    "        self.ollama_model_tag = ollama_model_tag\n",
    "        self.prompt_template = prompt_template\n",
    "        self.max_new_tokens = max_new_toks\n",
    "        self.llm_client = None\n",
    "        try:\n",
    "            self.llm_client = ChatOllama(model=ollama_model_tag, temperature=0, keep_alive=\"1m\")\n",
    "            # Test connection\n",
    "            self.llm_client.invoke(\"ping\") \n",
    "            print(f\"Successfully initialized Ollama client for {ollama_model_tag}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Ollama client for {ollama_model_tag}: {e}\")\n",
    "            # Allow predict to fail gracefully if client is None\n",
    "\n",
    "    def predict(self, X_df):\n",
    "        if not self.llm_client:\n",
    "            return pd.Series([\"Error: Ollama client not initialized\"] * len(X_df))\n",
    "            \n",
    "        if isinstance(X_df, pd.DataFrame):\n",
    "            input_texts = X_df['inputs'].tolist()\n",
    "        elif isinstance(X_df, pd.Series):\n",
    "            input_texts = X_df.tolist()\n",
    "        else:\n",
    "            raise ValueError(\"Input to predict should be a Pandas DataFrame or Series with an 'inputs' column.\")\n",
    "\n",
    "        results = []\n",
    "        for text_input in input_texts:\n",
    "            formatted_prompt = self.prompt_template.format(text=text_input)\n",
    "            try:\n",
    "                # For summarization, we usually want the LLM to complete the prompt.\n",
    "                # The ChatOllama invoke expects a list of messages or a string.\n",
    "                # We'll treat the formatted_prompt as the user message.\n",
    "                response = self.llm_client.invoke([HumanMessage(content=formatted_prompt)], \n",
    "                                                 # Ollama through Langchain doesn't directly take max_new_tokens in invoke\n",
    "                                                 # It would be part of model parameters or specific call options if supported\n",
    "                                                 # For now, we rely on the LLM's default output length or its training for summarization\n",
    "                                                 # Or, we could try to set num_predict in ChatOllama if that influences length\n",
    "                                                 config={'configurable': {'num_predict': self.max_new_tokens}} if hasattr(self.llm_client, 'configurable') else None\n",
    "                                                )\n",
    "                summary = response.content.strip()\n",
    "                results.append(summary)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction with {self.ollama_model_tag} for input '{text_input[:50]}...': {e}\")\n",
    "                results.append(\"Error during prediction\")\n",
    "        return pd.Series(results)\n",
    "\n",
    "print(\"OllamaSummarizationWrapper defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Custom Heuristic-Based Metrics\n",
    "These metrics are identical to those in the previous version of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `mlflow.metrics.make_metric`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Summary Length Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_length_ratio_eval_fn(predictions, targets, **kwargs):\n",
    "    ratios = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        len_pred = len(pred.split()) # Word count\n",
    "        len_target = len(target.split()) # Word count\n",
    "        if len_target == 0:\n",
    "            ratios.append(0.0 if len_pred > 0 else 1.0) # Avoid division by zero\n",
    "        else:\n",
    "            ratios.append(len_pred / len_target)\n",
    "    \n",
    "    return MetricValue(\n",
    "        scores=ratios, # Per-row scores\n",
    "        aggregate_results={\n",
    "            \"mean_length_ratio\": np.mean(ratios),\n",
    "            \"std_dev_length_ratio\": np.std(ratios)\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_length_ratio_metric = make_metric(\n",
    "    eval_fn=summary_length_ratio_eval_fn,\n",
    "    greater_is_better=False, # Ideal ratio is close to 1\n",
    "    name=\"summary_length_ratio\"\n",
    ")\n",
    "print(\"Custom metric 'summary_length_ratio' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Keyword Presence Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_presence_eval_fn(predictions, targets, custom_expected_list):\n",
    "    scores = [] \n",
    "    details = [] \n",
    "    for i, pred in enumerate(predictions):\n",
    "        required_keywords = custom_expected_list[i].get(\"required_keywords\", [])\n",
    "        if not required_keywords:\n",
    "            scores.append(1.0)\n",
    "            details.append({\"found\": [], \"missing\": [], \"all_required\": []})\n",
    "            continue\n",
    "        found_count = 0\n",
    "        found_kws = []\n",
    "        missing_kws = []\n",
    "        pred_lower = pred.lower()\n",
    "        for kw in required_keywords:\n",
    "            if re.search(r'\\b' + re.escape(kw.lower()) + r'\\b', pred_lower):\n",
    "                found_count += 1\n",
    "                found_kws.append(kw)\n",
    "            else:\n",
    "                missing_kws.append(kw)\n",
    "        scores.append(found_count / len(required_keywords) if required_keywords else 1.0)\n",
    "        details.append({\"found\": found_kws, \"missing\": missing_kws, \"all_required\": required_keywords})\n",
    "    all_found_keywords_overall = sum([len(d['found']) for d in details])\n",
    "    all_required_keywords_overall = sum([len(d['all_required']) for d in details])\n",
    "    overall_hit_rate = all_found_keywords_overall / all_required_keywords_overall if all_required_keywords_overall > 0 else 1.0\n",
    "    return MetricValue(\n",
    "        scores=scores,\n",
    "        aggregate_results={\n",
    "            \"mean_keyword_hit_rate\": np.mean(scores),\n",
    "            \"overall_keyword_hit_rate\": overall_hit_rate\n",
    "        }\n",
    "    )\n",
    "\n",
    "keyword_presence_metric = make_metric(\n",
    "    eval_fn=keyword_presence_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"keyword_presence_score\"\n",
    ")\n",
    "print(\"Custom metric 'keyword_presence_score' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining Custom LLM-as-a-Judge Metrics (with Qwen3-1.7B Judge)\n",
    "We'll use our configured `judge_llm` (Qwen3-1.7B) to evaluate summary helpfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: LLM as an Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: General Summary Helpfulness Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Judge LLM Prompt and Rating Scale [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_HELPFULNESS_JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant evaluating the helpfulness of a generated summary for a given original text.\n",
    "Score the helpfulness of the summary on a scale of 1 to 5:\n",
    "1: Not helpful at all. The summary is irrelevant, nonsensical, or completely misses the main points.\n",
    "2: Slightly helpful. The summary touches on some aspects but is largely incomplete or unclear.\n",
    "3: Moderately helpful. The summary captures some main points but could be significantly clearer or more comprehensive.\n",
    "4: Very helpful. The summary is clear, concise, and accurately reflects most of the important information.\n",
    "5: Extremely helpful. The summary is excellent, perfectly capturing the essence of the text in a clear and concise manner.\n",
    "\n",
    "Consider factors like clarity, conciseness, relevance to the original text, and whether the summary effectively conveys the main takeaways.\n",
    "\n",
    "Original Text:\n",
    "---BEGIN ORIGINAL TEXT---\n",
    "{original_text}\n",
    "---END ORIGINAL TEXT---\n",
    "\n",
    "Generated Summary:\n",
    "---BEGIN GENERATED SUMMARY---\n",
    "{generated_summary}\n",
    "---END GENERATED SUMMARY---\n",
    "\n",
    "Based on the criteria, provide your evaluation ONLY as a JSON object with two keys: \"score\" (an integer from 1 to 5) and \"reasoning\" (a brief explanation for your score, max 50 words).\n",
    "Example JSON: {{\"score\": 4, \"reasoning\": \"The summary is clear and captures main points well.\"}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the `eval_fn` to call the Qwen3-1.7B Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_helpfulness_judge_eval_fn(predictions, targets, inputs):\n",
    "    if judge_llm is None:\n",
    "        print(\"Judge LLM (Qwen3-1.7B) not available, skipping helpfulness metric.\")\n",
    "        return MetricValue(scores=[np.nan]*len(predictions), aggregate_results={\"mean_helpfulness_score\": np.nan})\n",
    "    \n",
    "    scores = []\n",
    "    for i, generated_summary in enumerate(predictions):\n",
    "        original_text = inputs[i]\n",
    "        prompt_for_judge = SUMMARY_HELPFULNESS_JUDGE_PROMPT_TEMPLATE.format(\n",
    "            original_text=original_text,\n",
    "            generated_summary=generated_summary\n",
    "        )\n",
    "        try:\n",
    "            judge_response_msg = judge_llm.invoke([\n",
    "                SystemMessage(content=\"You are an AI assistant that evaluates summaries and responds in JSON format according to the user's instructions.\"), \n",
    "                HumanMessage(content=prompt_for_judge)\n",
    "            ])\n",
    "            judge_response_content = judge_response_msg.content\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', judge_response_content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    parsed_response = json.loads(json_match.group(0))\n",
    "                    score = int(parsed_response.get(\"score\", 0))\n",
    "                else:\n",
    "                    print(f\"Warning (Helpfulness Judge): No JSON found in judge response: {judge_response_content}\")\n",
    "                    score = 0 \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning (Helpfulness Judge): Output for item {i} was not valid JSON: {judge_response_content}\")\n",
    "                score = 0 \n",
    "            except Exception as e_parse:\n",
    "                print(f\"Warning (Helpfulness Judge): Error parsing judge response: {e_parse}. Response: {judge_response_content}\")\n",
    "                score = 0\n",
    "        except Exception as e_judge_call:\n",
    "            print(f\"Error calling Judge LLM for item {i}: {e_judge_call}\")\n",
    "            score = 0 \n",
    "        scores.append(score)\n",
    "    valid_scores = [s for s in scores if isinstance(s, (int, float)) and not np.isnan(s) and s > 0]\n",
    "    mean_score = np.mean(valid_scores) if valid_scores else np.nan\n",
    "    return MetricValue(\n",
    "        scores=scores, \n",
    "        aggregate_results={\"mean_helpfulness_score_qwen3_1.7b_judge\": mean_score}\n",
    "    )\n",
    "\n",
    "summary_helpfulness_metric_qwen3_judge = make_metric(\n",
    "    eval_fn=summary_helpfulness_judge_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"summary_helpfulness_qwen3_1.7b_judge\"\n",
    ")\n",
    "\n",
    "print(\"Custom LLM-as-a-Judge metric 'summary_helpfulness_qwen3_1.7b_judge' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating Gemma-2B and DeepSeek-Coder with Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Evaluation Data with `custom_expected` fields [2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_custom = pd.DataFrame({\n",
    "    \"inputs\": [entry['info']['post'] for entry in eval_dataset],\n",
    "    \"targets\": [entry['summaries'][0]['text'] for entry in eval_dataset],\n",
    "    \"custom_expected\": [\n",
    "        {\"required_keywords\": [\"MLflow\", \"lifecycle\"]} if \"mlflow\" in entry['info']['post'].lower() \n",
    "        else {\"required_keywords\": [\"recipe\", \"cook\"]} if \"recipe\" in entry['info']['post'].lower() \n",
    "        else {\"required_keywords\": [\"summary\", \"text\"]} \n",
    "        for entry in eval_dataset\n",
    "    ]\n",
    "})\n",
    "print(\"Evaluation DataFrame with 'custom_expected' prepared:\")\n",
    "print(eval_df_custom.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Evaluation for Gemma-2B\n",
    "Ensure `ollama pull gemma:2b` has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_eval_gemma = \"gemma:2b\"\n",
    "wrapped_gemma_model = OllamaSummarizationWrapper(model_to_eval_gemma)\n",
    "\n",
    "if wrapped_gemma_model.llm_client: # Check if Ollama client initialized successfully\n",
    "    with mlflow.start_run(run_name=f\"Eval_{model_to_eval_gemma.replace(':', '_')}_Summarization_CustomMetrics\") as run:\n",
    "        mlflow.log_param(\"model_name\", model_to_eval_gemma)\n",
    "        mlflow.log_param(\"evaluation_task\", \"text-summarization-custom\")\n",
    "        mlflow.log_param(\"dataset_name\", f\"{dataset_name_hf}/{dataset_config_name_hf}\")\n",
    "        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "        mlflow.set_tag(\"model_family\", \"Gemma\")\n",
    "\n",
    "        print(f\"Starting mlflow.evaluate for {model_to_eval_gemma} with custom metrics...\")\n",
    "        try:\n",
    "            gemma_eval_results = mlflow.evaluate(\n",
    "                model=wrapped_gemma_model,\n",
    "                data=eval_df_custom,\n",
    "                targets=\"targets\",\n",
    "                feature_names=[\"inputs\"],\n",
    "                model_type=\"text-summarization\", \n",
    "                extra_metrics=[\n",
    "                    summary_length_ratio_metric,\n",
    "                    keyword_presence_metric,\n",
    "                    summary_helpfulness_metric_qwen3_judge\n",
    "                ],\n",
    "                evaluator_config={ \"text-summarization\": { \"metrics\": [\"rouge1\"]} }\n",
    "            )\n",
    "            print(f\"\\nCustom evaluation results for {model_to_eval_gemma}:\")\n",
    "            for metric_name, value in gemma_eval_results.metrics.items():\n",
    "                print(f\"  {metric_name}: {value}\")\n",
    "        except Exception as e_eval:\n",
    "            print(f\"Error during mlflow.evaluate for {model_to_eval_gemma}: {e_eval}\")\n",
    "            mlflow.log_text(str(e_eval), \"evaluation_error.txt\")\n",
    "else:\n",
    "    print(f\"Skipping evaluation for {model_to_eval_gemma} as Ollama client failed to initialize.\")\n",
    "\n",
    "del wrapped_gemma_model\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Evaluation for DeepSeek-Coder-1.3B-Instruct\n",
    "Ensure `ollama pull deepseek-coder:1.3b-instruct` has been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_eval_deepseek = \"deepseek-coder:1.3b-instruct\"\n",
    "wrapped_deepseek_model = OllamaSummarizationWrapper(model_to_eval_deepseek)\n",
    "\n",
    "if wrapped_deepseek_model.llm_client:\n",
    "    with mlflow.start_run(run_name=f\"Eval_{model_to_eval_deepseek.replace(':', '_')}_Summarization_CustomMetrics\") as run:\n",
    "        mlflow.log_param(\"model_name\", model_to_eval_deepseek)\n",
    "        mlflow.log_param(\"evaluation_task\", \"text-summarization-custom\")\n",
    "        mlflow.log_param(\"dataset_name\", f\"{dataset_name_hf}/{dataset_config_name_hf}\")\n",
    "        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "        mlflow.set_tag(\"model_family\", \"DeepSeek\")\n",
    "\n",
    "        print(f\"Starting mlflow.evaluate for {model_to_eval_deepseek} with custom metrics...\")\n",
    "        try:\n",
    "            deepseek_eval_results = mlflow.evaluate(\n",
    "                model=wrapped_deepseek_model,\n",
    "                data=eval_df_custom,\n",
    "                targets=\"targets\",\n",
    "                feature_names=[\"inputs\"],\n",
    "                model_type=\"text-summarization\", \n",
    "                extra_metrics=[\n",
    "                    summary_length_ratio_metric,\n",
    "                    keyword_presence_metric,\n",
    "                    summary_helpfulness_metric_qwen3_judge\n",
    "                ],\n",
    "                evaluator_config={ \"text-summarization\": { \"metrics\": [\"rouge1\"]} }\n",
    "            )\n",
    "            print(f\"\\nCustom evaluation results for {model_to_eval_deepseek}:\")\n",
    "            for metric_name, value in deepseek_eval_results.metrics.items():\n",
    "                print(f\"  {metric_name}: {value}\")\n",
    "        except Exception as e_eval:\n",
    "            print(f\"Error during mlflow.evaluate for {model_to_eval_deepseek}: {e_eval}\")\n",
    "            mlflow.log_text(str(e_eval), \"evaluation_error.txt\")\n",
    "else:\n",
    "    print(f\"Skipping evaluation for {model_to_eval_deepseek} as Ollama client failed to initialize.\")\n",
    "\n",
    "del wrapped_deepseek_model\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Custom Metric Results in MLflow UI\n",
    "\n",
    "Launch `mlflow ui` and navigate to the `LLM_Custom_Metrics_Generative_Tasks_NewModels` experiment.\n",
    "\n",
    "- **Compare Runs:** Select the runs for Gemma and DeepSeek Coder and click \"Compare.\"\n",
    "- **Metrics Section:** Analyze your custom metrics:\n",
    "    - `summary_length_ratio/...`\n",
    "    - `keyword_presence_score/...`\n",
    "    - `summary_helpfulness_qwen3_1.7b_judge/mean_helpfulness_score_qwen3_1.7b_judge`\n",
    "- **Artifacts:** The `eval_results_table.json` (and `.html`) will show per-sample scores for all metrics, including custom ones. This allows for detailed error analysis and understanding of where each model excels or struggles according to your specific criteria.\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)\n",
    "\n",
    "This comparative view, enriched by your custom metrics, provides a powerful dashboard for model selection and improvement efforts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Developing Custom Metrics [2, 4, 6]\n",
    "\n",
    "- **Define Clear Criteria:** Your custom metric should measure a well-defined, specific aspect of quality or performance that matters for your application [4].\n",
    "- **Iterative Development [2, 6]:** Develop and test your metric logic on a static set of model outputs before integrating into full evaluation runs.\n",
    "- **Consider Cost and Latency:** LLM-as-a-Judge metrics are insightful but can be slower and more expensive than heuristic ones.\n",
    "- **Judge LLM Reliability:** The quality of LLM-as-a-Judge scores depends on the judge LLM's capability and the clarity of its instructions. Experiment with different judge models and prompts.\n",
    "- **Combine with Standard Metrics and Human Review:** Custom metrics augment, not replace, other evaluation methods. A holistic view is best.\n",
    "- **Version Your Metrics:** Metric definitions can evolve; version control them alongside your code and models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "This notebook has empowered you to go beyond standard evaluations for generative AI:\n",
    "\n",
    "- **Tailored Evaluation:** You can now define custom metrics that precisely measure what's important for your specific generative AI task using diverse LLMs like Gemma and DeepSeek Coder.\n",
    "- **Heuristic Metrics:** Implemented programmatic custom metrics using `mlflow.metrics.make_metric`.\n",
    "- **LLM-as-a-Judge with Qwen3-1.7B:** Leveraged a more powerful LLM (Qwen3-1.7B) to act as a judge for qualitative aspects like helpfulness.\n",
    "- **Integration with `mlflow.evaluate`:** Seamlessly incorporated these custom metrics into the MLflow evaluation workflow for different Ollama-served models.\n",
    "- **Deeper Insights:** Recognized that custom metrics, analyzed in MLflow, offer a richer understanding of model performance compared to standard metrics alone.\n",
    "\n",
    "Mastering custom evaluation is key to effectively iterating on and improving your generative AI models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Engaging Resources and Further Reading\n",
    "\n",
    "- **MLflow Documentation:**\n",
    "    - [MLflow LLM Evaluate - Custom Metrics Section](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#create-custom-heuristic-based-llm-evaluation-metrics) [1, 9 for general mlflow.evaluate]\n",
    "- **Ollama Model Information:**\n",
    "    - [Ollama Library (for available models)](https://ollama.com/library)\n",
    "    - [Qwen3 Blog (for Qwen series details)](https://qwenlm.github.io/blog/qwen3/) [4]\n",
    "    - Gemma and DeepSeek model cards on Hugging Face or their respective official sites.\n",
    "- **LLM Evaluation Concepts:**\n",
    "    - Search for recent papers and blogs on \"LLM-as-a-Judge\" and "evaluating generative AI."\n",
    "\n",
    "--- \n",
    "\n",
    "Excellent work! You've now explored a critical aspect of maturing your generative AI development process by evaluating different models with custom, insightful metrics.\n",
    "\n",
    "**Coming Up Next (Notebook 10):** We'll aim to synthesize several concepts by building a more comprehensive End-to-End GenAI Application, potentially combining RAG with function-calling agents, all tracked and managed with MLflow.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
