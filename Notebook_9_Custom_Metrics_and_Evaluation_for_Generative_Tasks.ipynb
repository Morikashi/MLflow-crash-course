{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 09: Custom Metrics and Evaluation for Generative Tasks\n",
    "\n",
    "Welcome to Notebook 9! This notebook demonstrates advanced evaluation techniques for generative AI models using custom metrics with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows path handling workaround\n",
    "import mlflow\n",
    "from mlflow.utils.file_utils import local_file_uri_to_path\n",
    "mlflow.utils.file_uri_to_path = local_file_uri_to_path\n",
    "\n",
    "!pip install --quiet mlflow langchain langchain_community langchain_core langchain_ollama pydantic tiktoken rouge_score bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from mlflow.metrics import make_metric, MetricValue\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri('mlruns')\n",
    "mlflow.set_experiment(\"LLM_Custom_Metrics_Summarization_Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "ollama_judge_model = \"qwen3:1.7b\"\n",
    "models_to_evaluate = {\n",
    "    \"gemma3:1b\": \"gemma3:1b\",\n",
    "    \"deepseek-r1:1.5b\": \"deepseek-r1:1.5b\"\n",
    "}\n",
    "\n",
    "# Initialize judge LLM\n",
    "judge_llm = ChatOllama(model=ollama_judge_model, temperature=0.1, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics definition\n",
    "def summary_length_ratio(predictions, targets, **kwargs):\n",
    "    ratios = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_len = len(str(pred).split())\n",
    "        target_len = len(str(target).split()) or 1\n",
    "        ratios.append(pred_len / target_len)\n",
    "    return MetricValue(\n",
    "        scores=ratios,\n",
    "        aggregate_results={\n",
    "            \"mean\": np.mean(ratios),\n",
    "            \"std\": np.std(ratios)\n",
    "        }\n",
    "    )\n",
    "\n",
    "def keyword_presence(predictions, targets, custom_expected, **kwargs):\n",
    "    scores = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        keywords = custom_expected[i].get(\"required_keywords\", [])\n",
    "        found = sum(1 for kw in keywords if re.search(fr'\\b{re.escape(kw)}\\b', pred, re.I))\n",
    "        scores.append(found / len(keywords) if keywords else 1.0)\n",
    "    return MetricValue(\n",
    "        scores=scores,\n",
    "        aggregate_results={\"mean\": np.mean(scores)}\n",
    "    )\n",
    "\n",
    "custom_metrics = [\n",
    "    make_metric(summary_length_ratio, name=\"summary_length_ratio\"),\n",
    "    make_metric(keyword_presence, name=\"keyword_presence\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper class\n",
    "class OllamaSummarizer:\n",
    "    def __init__(self, model_name):\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            inputs = data['inputs'].tolist()\n",
    "        return pd.Series([self.llm.invoke(f\"Summarize: {text}\").content for text in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation execution\n",
    "eval_data = pd.DataFrame({\n",
    "    \"inputs\": [\"Sample text 1\", \"Sample text 2\"],  # Replace with actual data\n",
    "    \"targets\": [\"Sample summary 1\", \"Sample summary 2\"],\n",
    "    \"custom_expected\": [\n",
    "        {\"required_keywords\": [\"key1\"]},\n",
    "        {\"required_keywords\": [\"key2\"]}\n",
    "    ]\n",
    "})\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        \n",
    "        # Create and evaluate model\n",
    "        model = OllamaSummarizer(models_to_evaluate[model_name])\n",
    "        \n",
    "        results = mlflow.evaluate(\n",
    "            model=model.predict,\n",
    "            data=eval_data,\n",
    "            targets=\"targets\",\n",
    "            evaluator_config={\n",
    "                \"col_mapping\": {\n",
    "                    \"inputs\": \"inputs\",\n",
    "                    \"custom_expected\": \"custom_expected\"\n",
    "                }\n",
    "            },\n",
    "            extra_metrics=custom_metrics\n",
    "        )\n",
    "        \n",
    "        print(f\"Results for {model_name}:\")\n",
    "        print(results.metrics)"
   ]
  }
 ],
 "metadata": {}
}
