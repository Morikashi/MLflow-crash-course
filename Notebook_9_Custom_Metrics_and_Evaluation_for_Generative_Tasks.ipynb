{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 09: Custom Metrics and Evaluation for Generative Tasks\n",
    "\n",
    "Welcome to Notebook 9! In [Notebook 6](MLflow_06_Evaluating_and_Benchmarking_LLMs_with_MLflow.ipynb), we explored standard evaluation metrics for LLMs. However, generative AI tasks often require more nuanced assessment beyond what metrics like ROUGE or BERTScore can capture [5, 7]. The 'quality' of generated text can be highly subjective and task-dependent.\n",
    "\n",
    "This notebook delves into **Custom Metrics for Generative Tasks**. We'll learn how to:\n",
    "- Understand the limitations of standard metrics for certain generative AI qualities.\n",
    "- Define and implement **heuristic-based custom metrics** tailored to specific requirements (e.g., length constraints, keyword presence) [2, 3, 6].\n",
    "- Explore the concept and implementation of **LLM-as-a-Judge custom metrics**, where another LLM is used to score the output based on defined criteria (e.g., helpfulness, coherence, adherence to style) [1, 3, 8].\n",
    "- Integrate these custom metrics into the `mlflow.evaluate()` workflow.\n",
    "- Analyze these richer evaluation results in the MLflow UI to gain deeper insights into model performance.\n",
    "\n",
    "![Custom Evaluation Concept](https://www.comet.com/wp-content/uploads/2023/11/LLM-Eval-Taxonomy.png)\n",
    "\n",
    "By defining what truly matters for your specific use case, custom metrics empower you to build better, more reliable, and more aligned generative AI applications [4].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Recap: Limitations of Standard Generative AI Metrics](#recap-standard-metric-limitations)\n",
    "2. [Setting Up the Custom Evaluation Environment](#setting-up-custom-eval-env)\n",
    "    - [Installing Libraries](#installing-libraries-custom-eval)\n",
    "    - [Ollama and LLM Setup (for Judge LLM)](#ollama-llm-setup-custom-eval)\n",
    "    - [Configuring MLflow](#configuring-mlflow-custom-eval)\n",
    "3. [Task, Dataset, and Model Under Evaluation](#task-dataset-model-custom-eval)\n",
    "    - [Task: Text Summarization (Revisited)](#task-text-summarization-revisited)\n",
    "    - [Dataset: `openai/summarize_from_feedback` (TLDR subset)](#dataset-summarize-feedback-revisited)\n",
    "    - [Model to Evaluate: Fine-tuned Qwen3-0.6B Recipe Bot](#model-to-evaluate-qwen3-ft)\n",
    "4. [Defining Custom Heuristic-Based Metrics](#defining-custom-heuristic-metrics)\n",
    "    - [Using `mlflow.metrics.make_metric`](#mlflow-make-metric)\n",
    "    - [Example 1: Summary Length Ratio](#custom-metric-length-ratio)\n",
    "    - [Example 2: Keyword Presence Check](#custom-metric-keyword-presence)\n",
    "5. [Defining Custom LLM-as-a-Judge Metrics](#defining-custom-llm-judge-metrics)\n",
    "    - [Concept: LLM as an Evaluator](#concept-llm-as-judge)\n",
    "    - [Example: \"Recipe Summary Helpfulness\" Judge](#custom-metric-recipe-helpfulness)\n",
    "        - Defining the Judge LLM Prompt and Rating Scale [1]\n",
    "        - Implementing the `eval_fn` to call the Judge LLM\n",
    "6. [Evaluating with Custom Metrics using `mlflow.evaluate()`](#evaluating-with-custom-metrics)\n",
    "    - [Preparing Evaluation Data with `custom_expected` fields [2, 6]](#preparing-eval-data-custom)\n",
    "    - [Running the Evaluation](#running-evaluation-custom)\n",
    "7. [Analyzing Custom Metric Results in MLflow UI](#analyzing-custom-results-mlflow-ui)\n",
    "8. [Best Practices for Developing Custom Metrics [2, 4, 6]](#best-practices-custom-metrics)\n",
    "9. [Key Takeaways](#key-takeaways-custom-eval)\n",
    "10. [Engaging Resources and Further Reading](#resources-further-reading-custom-eval)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: Limitations of Standard Generative AI Metrics\n",
    "\n",
    "In [Notebook 6](MLflow_06_Evaluating_and_Benchmarking_LLMs_with_MLflow.ipynb), we used metrics like ROUGE and BERTScore. While valuable, they primarily measure surface-level lexical overlap or semantic similarity with reference texts. They might not fully capture [5, 7]:\n",
    "- **Factual Correctness/Faithfulness:** Does the generation accurately reflect provided context (if any) or known facts?\n",
    "- **Coherence and Readability:** Is the text well-structured and easy to understand?\n",
    "- **Adherence to Instructions/Style:** Does the model follow specific formatting, tone, or persona requirements?\n",
    "- **Helpfulness/Relevance:** Is the output actually useful or relevant to the user's query or task?\n",
    "- **Creativity and Novelty:** For creative tasks, are the outputs original and engaging?\n",
    "- **Absence of Undesirable Content:** Metrics for toxicity, bias, PII leakage [5].\n",
    "\n",
    "**Custom metrics** allow us to define evaluation criteria that are more closely aligned with these nuanced aspects and specific business goals [2, 4].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Custom Evaluation Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow \"transformers>=4.51.0\" datasets evaluate peft bitsandbytes sentencepiece accelerate rouge_score bert_score langchain langchain_community langchain_core langchain_ollama\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset # Ensure Dataset is imported\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "from mlflow.metrics import make_metric, MetricValue # For custom heuristic metrics [3]\n",
    "from mlflow.models.evaluation.base import EvaluationResult # For structuring custom eval output\n",
    "from langchain_ollama.chat_models import ChatOllama # For LLM-as-a-Judge\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import re # For keyword checking\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "import transformers\n",
    "print(f\"Transformers Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama and LLM Setup (for Judge LLM)\n",
    "We'll need an LLM to act as our \"judge\" for one of the custom metrics. `phi3:mini` is a good candidate. Ensure Ollama is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_judge_model_name = \"phi3:mini\" # Or another capable model like llama3:8b\n",
    "judge_llm = None\n",
    "try:\n",
    "    judge_llm = ChatOllama(\n",
    "        model=ollama_judge_model_name, \n",
    "        temperature=0.1, # Low temperature for consistent judging\n",
    "        keep_alive=\"5m\",\n",
    "        format=\"json\" # Request JSON output from judge for easier parsing of scores/reasoning\n",
    "    )\n",
    "    response_test = judge_llm.invoke(\"Output a JSON with a key 'status' and value 'ok'.\")\n",
    "    print(f\"Judge LLM ({ollama_judge_model_name}) connected. Test response: {response_test.content[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Judge LLM ({ollama_judge_model_name}): {e}. Ensure Ollama is running.\")\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns')\n",
    "experiment_name = \"LLM_Custom_Metrics_Generative_Tasks\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task, Dataset, and Model Under Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Text Summarization (Revisited)\n",
    "We'll use text summarization to illustrate custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: `openai/summarize_from_feedback` (TLDR subset)\n",
    "Reusing the dataset from Notebook 6 for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"openai/summarize_from_feedback\"\n",
    "dataset_config_name = \"tldr\"\n",
    "num_eval_samples = 20 # Smaller subset for quicker custom metric development\n",
    "\n",
    "try:\n",
    "    eval_dataset_full = load_dataset(dataset_name, dataset_config_name, split=\"validation\")\n",
    "    eval_dataset = eval_dataset_full.select(range(num_eval_samples))\n",
    "    print(f\"Loaded {len(eval_dataset)} samples from '{dataset_name}/{dataset_config_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}. Using dummy data.\")\n",
    "    dummy_data = {\n",
    "        'info': [{'post': 'This is a long post about the benefits of MLflow. It helps track experiments and manage the ML lifecycle efficiently, with features for reproducibility.'}] * num_eval_samples,\n",
    "        'summaries': [[{'text': 'MLflow is great for MLOps providing efficiency.'}]] * num_eval_samples\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(dummy_data)\n",
    "    print(f\"Using dummy dataset with {len(eval_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to Evaluate: Fine-tuned Qwen3-0.6B Recipe Bot\n",
    "Let's see how our recipe-fine-tuned Qwen3 model (from Notebook 4) performs on general summarization, and how custom metrics can reveal its strengths or weaknesses for this out-of-domain task. We need the adapter path from Notebook 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_evaluate_name = \"Qwen/Qwen3-0.6B_FineTuned_RecipeBot\"\n",
    "base_qwen3_name = \"Qwen/Qwen3-0.6B\"\n",
    "qwen3_adapter_path = \"./qwen3_0.6b_recipe_finetuned_adapters/final_adapter\" # From Notebook 4 output\n",
    "eval_model_pipeline = None\n",
    "\n",
    "if not os.path.exists(qwen3_adapter_path):\n",
    "    print(f\"Fine-tuned Qwen3 adapter not found at {qwen3_adapter_path}. Cannot proceed with this model.\")\n",
    "    print(\"Please ensure Notebook 4 (Qwen3 fine-tuning) was run successfully and the adapter path is correct.\")\n",
    "else:\n",
    "    try:\n",
    "        clear_gpu_cache()\n",
    "        print(f\"Loading fine-tuned Qwen3 model from adapter: {qwen3_adapter_path}\")\n",
    "        q_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16)\n",
    "        \n",
    "        qwen3_base_for_eval = AutoModelForCausalLM.from_pretrained(base_qwen3_name, quantization_config=q_config, device_map=\"auto\", trust_remote_code=True)\n",
    "        qwen3_ft_model_for_eval = PeftModel.from_pretrained(qwen3_base_for_eval, qwen3_adapter_path, is_trainable=False)\n",
    "        qwen3_ft_model_for_eval.eval()\n",
    "        \n",
    "        qwen3_eval_tokenizer = AutoTokenizer.from_pretrained(base_qwen3_name, trust_remote_code=True)\n",
    "        if qwen3_eval_tokenizer.pad_token is None: qwen3_eval_tokenizer.pad_token = qwen3_eval_tokenizer.eos_token\n",
    "\n",
    "        eval_model_pipeline = pipeline(\"text-generation\", model=qwen3_ft_model_for_eval, tokenizer=qwen3_eval_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "        print(\"Fine-tuned Qwen3 model pipeline created for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned Qwen3 model: {e}\")\n",
    "\n",
    "# Wrapper class from Notebook 6 to adapt text-generation pipeline for summarization evaluation\n",
    "class SummarizationPipelineWrapper:\n",
    "    def __init__(self, pipeline_obj, prompt_template=\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\", max_new_toks=100):\n",
    "        self.pipeline = pipeline_obj\n",
    "        self.prompt_template = prompt_template\n",
    "        self.max_new_tokens = max_new_toks\n",
    "\n",
    "    def predict(self, X_df):\n",
    "        # X_df is expected to be a DataFrame with a column named 'inputs' (or whatever 'feature_names' specifies)\n",
    "        # For mlflow.evaluate, 'inputs' column in data DataFrame is usually passed as a Series to predict.\n",
    "        # If X_df is the full df, and we need 'inputs' column:\n",
    "        if isinstance(X_df, pd.DataFrame):\n",
    "            prompts_input = X_df['inputs'].tolist()\n",
    "        elif isinstance(X_df, pd.Series): # If only the input column Series is passed\n",
    "            prompts_input = X_df.tolist()\n",
    "        else:\n",
    "            raise ValueError(\"Input to predict should be a Pandas DataFrame or Series.\")\n",
    "\n",
    "        formatted_prompts = [self.prompt_template.format(text=text_input) for text_input in prompts_input]\n",
    "        outputs = self.pipeline(formatted_prompts, max_new_tokens=self.max_new_tokens, num_return_sequences=1, do_sample=False, pad_token_id=self.pipeline.tokenizer.eos_token_id)\n",
    "        \n",
    "        results = []\n",
    "        for i, out_list in enumerate(outputs):\n",
    "            full_text = out_list[0]['generated_text']\n",
    "            summary = full_text.replace(formatted_prompts[i], \"\").strip()\n",
    "            results.append(summary)\n",
    "        return pd.Series(results)\n",
    "\n",
    "if eval_model_pipeline:\n",
    "    wrapped_eval_model = SummarizationPipelineWrapper(eval_model_pipeline)\n",
    "    print(\"Evaluation model wrapped for mlflow.evaluate.\")\n",
    "else:\n",
    "    wrapped_eval_model = None\n",
    "    print(\"Evaluation model pipeline not loaded, custom metric testing will be limited.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Custom Heuristic-Based Metrics\n",
    "Heuristic metrics are based on programmable rules or simple computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `mlflow.metrics.make_metric`\n",
    "MLflow allows creating custom metrics by providing an evaluation function (`eval_fn`) to `mlflow.metrics.make_metric`. The `eval_fn` typically takes `predictions` (list of generated strings) and `targets` (list of reference strings) and must return an `mlflow.metrics.MetricValue` object containing per-row scores and aggregate results [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Summary Length Ratio\n",
    "This metric calculates the ratio of the generated summary length to the reference summary length. It can help check for over-summarization or under-summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_length_ratio_eval_fn(predictions, targets, **kwargs):\n",
    "    ratios = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        len_pred = len(pred.split()) # Word count\n",
    "        len_target = len(target.split()) # Word count\n",
    "        if len_target == 0:\n",
    "            ratios.append(0.0 if len_pred > 0 else 1.0) # Avoid division by zero\n",
    "        else:\n",
    "            ratios.append(len_pred / len_target)\n",
    "    \n",
    "    return MetricValue(\n",
    "        scores=ratios, # Per-row scores\n",
    "        aggregate_results={\n",
    "            \"mean_length_ratio\": np.mean(ratios),\n",
    "            \"std_dev_length_ratio\": np.std(ratios)\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_length_ratio_metric = make_metric(\n",
    "    eval_fn=summary_length_ratio_eval_fn,\n",
    "    greater_is_better=False, # Ideal ratio is close to 1, so neither strictly greater nor smaller is better\n",
    "    name=\"summary_length_ratio\"\n",
    ")\n",
    "\n",
    "print(\"Custom metric 'summary_length_ratio' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Keyword Presence Check\n",
    "This metric checks if a list of essential keywords is present in the generated summary. The keywords can be passed via the `custom_expected` field in the evaluation data [2, 6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_presence_eval_fn(predictions, targets, custom_expected_list):\n",
    "    \"\"\"\n",
    "    custom_expected_list is a list of dicts, where each dict can have a 'required_keywords' key.\n",
    "    The order should match predictions and targets.\n",
    "    \"\"\"\n",
    "    scores = [] # List of 0 or 1 (fraction of keywords found)\n",
    "    details = [] # List of dicts with found/missing keywords\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        required_keywords = custom_expected_list[i].get(\"required_keywords\", [])\n",
    "        if not required_keywords: # No keywords to check for this sample\n",
    "            scores.append(1.0) # Or np.nan if prefer to ignore\n",
    "            details.append({\"found\": [], \"missing\": [], \"all_required\": []})\n",
    "            continue\n",
    "\n",
    "        found_count = 0\n",
    "        found_kws = []\n",
    "        missing_kws = []\n",
    "        pred_lower = pred.lower()\n",
    "        for kw in required_keywords:\n",
    "            if re.search(r'\\b' + re.escape(kw.lower()) + r'\\b', pred_lower):\n",
    "                found_count += 1\n",
    "                found_kws.append(kw)\n",
    "            else:\n",
    "                missing_kws.append(kw)\n",
    "        \n",
    "        scores.append(found_count / len(required_keywords) if required_keywords else 1.0)\n",
    "        details.append({\"found\": found_kws, \"missing\": missing_kws, \"all_required\": required_keywords})\n",
    "\n",
    "    # For aggregate, we can calculate the mean score and overall keyword hit rates\n",
    "    all_found_keywords_overall = sum([len(d['found']) for d in details])\n",
    "    all_required_keywords_overall = sum([len(d['all_required']) for d in details])\n",
    "    overall_hit_rate = all_found_keywords_overall / all_required_keywords_overall if all_required_keywords_overall > 0 else 1.0\n",
    "\n",
    "    return MetricValue(\n",
    "        scores=scores, # Per-row fraction of keywords found\n",
    "        aggregate_results={\n",
    "            \"mean_keyword_hit_rate\": np.mean(scores),\n",
    "            \"overall_keyword_hit_rate\": overall_hit_rate\n",
    "        },\n",
    "        # We can't directly pass 'details' to MetricValue for structured logging in UI easily.\n",
    "        # This detailed info is better logged as part of the per-row evaluation table artifact.\n",
    "    )\n",
    "\n",
    "keyword_presence_metric = make_metric(\n",
    "    eval_fn=keyword_presence_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"keyword_presence_score\",\n",
    "    # `custom_expected_list` will be derived from the 'custom_expected' column in eval_df\n",
    ")\n",
    "\n",
    "print(\"Custom metric 'keyword_presence_score' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining Custom LLM-as-a-Judge Metrics\n",
    "Here, we use another LLM (the \"judge\") to evaluate the output of our model under test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: LLM as an Evaluator\n",
    "The judge LLM is given the input, the generated output, and potentially the reference output, along with specific instructions (a prompt) on how to score the output based on criteria like coherence, relevance, helpfulness, etc. [1, 3, 8]. The judge then provides a score and often a rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \"Recipe Summary Helpfulness\" Judge\n",
    "Since our model under test was fine-tuned on recipes, let's define a custom LLM-as-a-Judge metric to assess the \"helpfulness\" of its summaries (even if the task is general summarization, we can see if its recipe "bias" makes summaries more or less helpful in a general sense, or if we were evaluating on recipe summaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Judge LLM Prompt and Rating Scale [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECIPE_SUMMARY_HELPFULNESS_PROMPT = \"\"\"\n",
    "You are evaluating a generated summary of a text, which might be related to a recipe or cooking instructions.\n",
    "Score the helpfulness of the summary on a scale of 1 to 5, where 1 is not helpful at all and 5 is extremely helpful.\n",
    "Consider clarity, conciseness, and whether the summary provides useful takeaways for someone interested in the topic (potentially cooking-related).\n",
    "\n",
    "Original Text:\n",
    "{{{{inputs.prompt_text}}}}\n",
    "\n",
    "Generated Summary:\n",
    "{{{{predictions}}}}\n",
    "\n",
    "Reference Summary (for context, not direct comparison for helpfulness):\n",
    "{{{{targets.reference_summary}}}}\n",
    "\n",
    "Provide your evaluation as a JSON object with two keys: \"score\" (an integer from 1 to 5) and \"reasoning\" (a brief explanation for your score).\n",
    "Example JSON: {\"score\": 4, \"reasoning\": \"The summary is quite clear and captures the main points well, making it useful.\"}\n",
    "Output only the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "# Note: The double curly braces {{}} are for MLflow's default LLM metrics prompt formatting.\n",
    "# When using a custom eval_fn, we'll format the prompt ourselves.\n",
    "CUSTOM_JUDGE_HELPFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "You are evaluating a generated summary of a text. Your task is to assess its helpfulness.\n",
    "Score the helpfulness of the summary on a scale of 1 to 5, where 1 is 'Not helpful at all' and 5 is 'Extremely helpful'.\n",
    "Consider factors like clarity, conciseness, and whether the summary effectively conveys the main takeaways from the original text.\n",
    "\n",
    "Original Text:\n",
    "---BEGIN ORIGINAL TEXT---\n",
    "{original_text}\n",
    "---END ORIGINAL TEXT---\n",
    "\n",
    "Generated Summary:\n",
    "---BEGIN GENERATED SUMMARY---\n",
    "{generated_summary}\n",
    "---END GENERATED SUMMARY---\n",
    "\n",
    "Based on the criteria, provide your evaluation ONLY as a JSON object with two keys: \"score\" (an integer from 1 to 5) and \"reasoning\" (a brief explanation for your score, max 50 words).\n",
    "Example JSON: {{\"score\": 4, \"reasoning\": \"The summary is quite clear and captures the main points well, making it useful.\"}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the `eval_fn` to call the Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def recipe_summary_helpfulness_eval_fn(predictions, targets, inputs):\n",
    "    \"\"\"\n",
    "    predictions: list of generated summaries\n",
    "    targets: list of reference summaries (can be used by judge for context)\n",
    "    inputs: list of original texts/prompts that were summarized\n",
    "    \"\"\"\n",
    "    if judge_llm is None:\n",
    "        print(\"Judge LLM not available, skipping helpfulness metric.\")\n",
    "        return MetricValue(scores=[np.nan]*len(predictions), aggregate_results={\"mean_helpfulness_score\": np.nan})\n",
    "    \n",
    "    scores = []\n",
    "    all_reasonings = [] # To potentially log as an artifact later\n",
    "\n",
    "    for i, generated_summary in enumerate(predictions):\n",
    "        original_text = inputs[i]\n",
    "        # reference_summary = targets[i] # Reference not directly used in this judge's prompt, but could be\n",
    "        \n",
    "        prompt_for_judge = CUSTOM_JUDGE_HELPFULNESS_PROMPT_TEMPLATE.format(\n",
    "            original_text=original_text,\n",
    "            generated_summary=generated_summary\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            judge_response_msg = judge_llm.invoke([SystemMessage(content=\"You are a helpful AI assistant that provides evaluations in JSON format.\"), \n",
    "                                                   HumanMessage(content=prompt_for_judge)])\n",
    "            judge_response_content = judge_response_msg.content\n",
    "            \n",
    "            # Attempt to parse JSON from the judge's response\n",
    "            # The judge might sometimes return non-JSON text or malformed JSON despite instructions.\n",
    "            try:\n",
    "                # Try to extract JSON part if there's surrounding text\n",
    "                json_match = re.search(r'\\{.*\\}', judge_response_content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    parsed_response = json.loads(json_match.group(0))\n",
    "                    score = int(parsed_response.get(\"score\", 0)) # Default to 0 if no score\n",
    "                    reasoning = parsed_response.get(\"reasoning\", \"\")\n",
    "                else:\n",
    "                    print(f\"Warning: Judge LLM did not output valid JSON for item {i}. Response: {judge_response_content}\")\n",
    "                    score = 0 # Or np.nan\n",
    "                    reasoning = \"Invalid JSON output from judge.\"\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Judge LLM output for item {i} was not valid JSON: {judge_response_content}\")\n",
    "                score = 0 # Or np.nan\n",
    "                reasoning = \"Failed to parse JSON from judge.\"\n",
    "            except Exception as e_parse: # Catch other potential errors during parsing\n",
    "                print(f\"Warning: Error parsing judge response for item {i}: {e_parse}. Response: {judge_response_content}\")\n",
    "                score = 0\n",
    "                reasoning = f\"Parsing error: {e_parse}\"\n",
    "\n",
    "        except Exception as e_judge_call:\n",
    "            print(f\"Error calling Judge LLM for item {i}: {e_judge_call}\")\n",
    "            score = 0 # Or np.nan\n",
    "            reasoning = f\"Judge LLM call failed: {e_judge_call}\"\n",
    "            \n",
    "        scores.append(score)\n",
    "        all_reasonings.append({\"input\": original_text, \"prediction\": generated_summary, \"score\": score, \"reasoning\": reasoning})\n",
    "\n",
    "    # Log all reasonings as a JSON artifact in the active MLflow run\n",
    "    # This needs to be done carefully if eval_fn is called multiple times by mlflow.evaluate.\n",
    "    # For now, we'll just calculate aggregate. Detailed per-row logging is better handled by eval_results_table.\n",
    "    # If running in an active MLflow run, one could log here, but it's tricky with make_metric.\n",
    "\n",
    "    valid_scores = [s for s in scores if isinstance(s, (int, float)) and not np.isnan(s) and s > 0]\n",
    "    mean_score = np.mean(valid_scores) if valid_scores else np.nan\n",
    "\n",
    "    return MetricValue(\n",
    "        scores=scores, # Per-row scores from the judge\n",
    "        aggregate_results={\"mean_helpfulness_score\": mean_score}\n",
    "    )\n",
    "\n",
    "recipe_helpfulness_judge_metric = make_metric(\n",
    "    eval_fn=recipe_summary_helpfulness_eval_fn,\n",
    "    greater_is_better=True,\n",
    "    name=\"recipe_summary_helpfulness_judge\",\n",
    "    # `inputs` will be derived from the 'inputs' column in eval_df for this eval_fn\n",
    ")\n",
    "\n",
    "print(\"Custom LLM-as-a-Judge metric 'recipe_summary_helpfulness_judge' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating with Custom Metrics using `mlflow.evaluate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Evaluation Data with `custom_expected` fields [2, 6]\n",
    "For the `keyword_presence_metric`, we need to add a `custom_expected` column to our evaluation DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_custom = pd.DataFrame({\n",
    "    \"inputs\": [entry['info']['post'] for entry in eval_dataset],\n",
    "    \"targets\": [entry['summaries'][0]['text'] for entry in eval_dataset],\n",
    "    \"custom_expected\": [\n",
    "        {\"required_keywords\": [\"MLflow\", \"lifecycle\"]} if \"mlflow\" in entry['info']['post'].lower() \n",
    "        else {\"required_keywords\": [\"recipe\", \"cook\"]} if \"recipe\" in entry['info']['post'].lower() \n",
    "        else {\"required_keywords\": [\"summary\", \"text\"]} # Default keywords for other posts\n",
    "        for entry in eval_dataset\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Evaluation DataFrame with 'custom_expected' prepared:\")\n",
    "print(eval_df_custom.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation\n",
    "We'll now run `mlflow.evaluate` with our fine-tuned Qwen3 model and include our custom metrics in the `extra_metrics` argument [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metrics_list = [\n",
    "    summary_length_ratio_metric,\n",
    "    keyword_presence_metric,\n",
    "    recipe_helpfulness_judge_metric # This one will call our judge LLM\n",
    "]\n",
    "\n",
    "if wrapped_eval_model:\n",
    "    with mlflow.start_run(run_name=f\"Eval_Qwen3FT_Summarization_With_CustomMetrics\") as run:\n",
    "        mlflow.log_param(\"model_name\", model_to_evaluate_name)\n",
    "        mlflow.log_param(\"evaluation_task\", \"text-summarization-custom\")\n",
    "        mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name}\")\n",
    "        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "        mlflow.set_tag(\"evaluation_type\", \"custom_metrics_focused\")\n",
    "\n",
    "        print(f\"Starting mlflow.evaluate for {model_to_evaluate_name} with custom metrics...\")\n",
    "        try:\n",
    "            custom_eval_results = mlflow.evaluate(\n",
    "                model=wrapped_eval_model,\n",
    "                data=eval_df_custom, # DataFrame with 'inputs', 'targets', and 'custom_expected'\n",
    "                targets=\"targets\",\n",
    "                # `inputs` argument is deprecated in favor of `feature_names` or `input_example` for data profiling.\n",
    "                # For a model that takes a DataFrame and predicts based on a specific column name, `feature_names` is used.\n",
    "                # Our wrapper expects a DataFrame with an 'inputs' column passed to its predict method's argument X_df.\n",
    "                # So, we can specify the column name that the model's predict() method expects.\n",
    "                # By default, mlflow.evaluate passes the DataFrame columns specified by `feature_names` (or a default 'inputs' if not given)\n",
    "                # as arguments to model.predict(). If model.predict() expects a DataFrame, this can be tricky.\n",
    "                # The `input_example` is more for schema inference. \n",
    "                # Let's rely on the default behavior where the 'inputs' column from `data` will be used.\n",
    "                # Our wrapper's predict(self, X_df) expects X_df to contain an 'inputs' column.\n",
    "                # So, in mlflow.evaluate, we need to ensure that the 'inputs' column of our `eval_df_custom` is correctly passed.\n",
    "                # `mlflow.evaluate` will pass eval_df_custom[['inputs']] if feature_names=['inputs']\n",
    "                # If model.predict expects a DataFrame with a column 'inputs', then this setup should be fine.\n",
    "                feature_names=[\"inputs\"], # Explicitly tell evaluate what column(s) to pass to model.predict()\n",
    "                model_type=\"text-summarization\", # Keep for some default summarization metrics\n",
    "                extra_metrics=custom_metrics_list,\n",
    "                evaluator_config={\n",
    "                    \"text-summarization\": {\n",
    "                        \"metrics\": [\"rouge1\", \"bertscore_precision\"] # Example: Limit default metrics\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            print(f\"\\nCustom evaluation results for {model_to_evaluate_name}:\")\n",
    "            for metric_name, value in custom_eval_results.metrics.items():\n",
    "                print(f\"  {metric_name}: {value}\")\n",
    "            \n",
    "            if custom_eval_results.artifacts and \"eval_results_table.json\" in custom_eval_results.artifacts:\n",
    "                 print(f\"  Detailed evaluation table artifact path: {custom_eval_results.artifacts['eval_results_table.json'].uri}\")\n",
    "        \n",
    "        except Exception as e_eval:\n",
    "            print(f\"Error during mlflow.evaluate with custom metrics: {e_eval}\")\n",
    "            mlflow.log_text(str(e_eval), \"evaluation_error.txt\")\n",
    "\n",
    "else:\n",
    "    print(\"Model for evaluation not loaded. Skipping custom metric evaluation run.\")\n",
    "\n",
    "# Clean up the loaded model for evaluation\n",
    "if 'qwen3_base_for_eval' in locals(): del qwen3_base_for_eval\n",
    "if 'qwen3_ft_model_for_eval' in locals(): del qwen3_ft_model_for_eval\n",
    "if 'eval_model_pipeline' in locals(): del eval_model_pipeline\n",
    "if 'wrapped_eval_model' in locals(): del wrapped_eval_model\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Custom Metric Results in MLflow UI\n",
    "\n",
    "Launch the MLflow UI (`mlflow ui`) and navigate to the `LLM_Custom_Metrics_Generative_Tasks` experiment.\n",
    "\n",
    "- **Find your run** (e.g., `Eval_Qwen3FT_Summarization_With_CustomMetrics`).\n",
    "- **Metrics Section:** You should see your custom metrics listed alongside any standard ones:\n",
    "    - `summary_length_ratio/mean_length_ratio`\n",
    "    - `summary_length_ratio/std_dev_length_ratio`\n",
    "    - `keyword_presence_score/mean_keyword_hit_rate`\n",
    "    - `keyword_presence_score/overall_keyword_hit_rate`\n",
    "    - `recipe_summary_helpfulness_judge/mean_helpfulness_score`\n",
    "- **Artifacts:** The `eval_results_table.json` (and `.html`) artifact will contain per-row scores for all metrics, including your custom ones. This is extremely useful for:\n",
    "    - Seeing individual length ratios and keyword hits.\n",
    "    - Reviewing the individual helpfulness scores (and potentially the reasonings if you adapt logging to save them as a separate artifact from the `eval_fn`).\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)\n",
    "\n",
    "By comparing these custom metric scores across different models or different versions of the same model (e.g., before and after specific fine-tuning or prompt engineering changes), you can gain much richer insights into what aspects of performance are improving or degrading.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Developing Custom Metrics [2, 4, 6]\n",
    "\n",
    "- **Define Clear Criteria:** Your custom metric should measure a well-defined, specific aspect of quality or performance that matters for your application [4].\n",
    "- **Iterative Development [2, 6]:**\n",
    "    1.  **Generate an \"Answer Sheet\":** Run your model on an evaluation dataset once and save its predictions (and any necessary inputs/references).\n",
    "    2.  **Develop Metric `eval_fn`:** Write your custom metric function and test it directly on the saved predictions/inputs without re-running the model. This allows for rapid iteration on the metric logic.\n",
    "    3.  **Validate with `mlflow.evaluate` on Answer Sheet:** Once the metric logic is stable, run `mlflow.evaluate` using the pre-generated answer sheet (by *not* passing the `model` argument but providing the answer sheet as `data` with predictions). This verifies that MLflow processes your metric correctly.\n",
    "    4.  **Full Evaluation:** Finally, run `mlflow.evaluate` with the actual model to get end-to-end results.\n",
    "- **Consider Cost and Latency:** LLM-as-a-Judge metrics can be powerful but also add cost (token usage for the judge LLM) and latency to your evaluation pipeline. Heuristic metrics are usually much faster and cheaper.\n",
    "- **Judge LLM Reliability:** The quality of LLM-as-a-Judge metrics depends on the capability of the judge LLM and the clarity of its prompt. Experiment with different judge models and prompts.\n",
    "- **Combine with Standard Metrics and Human Review:** Custom metrics provide additional dimensions but shouldn't entirely replace standard metrics or qualitative human assessment. A holistic view is best.\n",
    "- **Version Your Metrics:** Just like your models and data, your custom metric definitions can evolve. Keep track of their versions and logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "This notebook has empowered you to go beyond standard evaluations for generative AI:\n",
    "\n",
    "- **Tailored Evaluation:** You can now define custom metrics that precisely measure what's important for your specific generative AI task and business objectives.\n",
    "- **Heuristic Metrics:** Learned to implement programmatic custom metrics (e.g., length checks, keyword presence) using `mlflow.metrics.make_metric`.\n",
    "- **LLM-as-a-Judge:** Understood the concept and implemented a custom LLM-as-a-Judge metric to assess a qualitative aspect like \"helpfulness.\"\n",
    "- **Integration with `mlflow.evaluate`:** Seamlessly incorporated these diverse custom metrics into the MLflow evaluation workflow via the `extra_metrics` argument.\n",
    "- **Deeper Insights:** Recognized that custom metrics, when analyzed in the MLflow UI alongside standard ones and per-sample results, provide a much richer and more nuanced understanding of model performance.\n",
    "\n",
    "Mastering custom evaluation is key to effectively iterating on and improving your generative AI models and applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Engaging Resources and Further Reading\n",
    "\n",
    "- **MLflow Documentation:**\n",
    "    - [MLflow LLM Evaluate - Custom Metrics Section](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#create-custom-heuristic-based-llm-evaluation-metrics) [3]\n",
    "    - [MLflow `make_metric` API](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.make_metric)\n",
    "- **Databricks Documentation (Mosaic AI Agent Evaluation):**\n",
    "    - [Custom Metrics Guide (Conceptual Overlap with MLflow)](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html) [2, 6]\n",
    "- **Cloud Provider Custom Metrics (for conceptual understanding of LLM-as-a-Judge structure):**\n",
    "    - [AWS Bedrock - Custom Metrics for GenAI Evaluation](https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/) [1]\n",
    "    - [Google Vertex AI - Evaluating Generative AI (often discusses custom metrics & LLM-based eval)](https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models) (see also the LinkedIn article [8] referencing Google SDK)\n",
    "- **General Best Practices:**\n",
    "    - [Microsoft Tech Community: Evaluating generative AI: Best practices for developers](https://techcommunity.microsoft.com/blog/azuredevcommunityblog/evaluating-generative-ai-best-practices-for-developers/4271488) [4]\n",
    "    - [DataRobot Blog: Design and Monitor Custom Metrics for Generative AI](https://www.datarobot.com/blog/design-and-monitor-custom-metrics-for-generative-ai-use-cases-in-datarobot-ai-platform/) [5]\n",
    "\n",
    "--- \n",
    "\n",
    "Excellent work! You've now explored a critical aspect of maturing your generative AI development process.\n",
    "\n",
    "**Coming Up Next (Notebook 10):** We'll aim to synthesize several concepts by building a more comprehensive End-to-End GenAI Application, potentially combining RAG with function-calling agents, all tracked and managed with MLflow.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
