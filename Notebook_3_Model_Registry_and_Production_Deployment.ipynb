{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 03: Model Registry & Production Deployment\n",
    "\n",
    "Welcome to the third notebook in our MLflow journey! In [Notebook 1](Notebook_1_Getting_Started_with_MLFlow.ipynb), we mastered experiment tracking. In [Notebook 2](Notebook_2_Advanced_Hyperparameter_Optimization_Model_Selection.ipynb), we delved into advanced hyperparameter optimization with Optuna and MLflow. Now, we're taking the next crucial step: managing our models with the **MLflow Model Registry** and exploring **Production Deployment**.\n",
    "\n",
    "![MLFlow logo](https://www.the-odd-dataguy.com/images/posts/20191113/cover.jpg)\n",
    "\n",
    "Once you have a promising model, how do you version it, manage its lifecycle (e.g., from staging to production), and serve it for inference? The MLflow Model Registry provides a centralized hub for these tasks. We'll then see how to deploy a registered model as a local REST API, a foundational step towards real-world application.\n",
    "\n",
    "Let's get started on bridging the gap between a trained model and a production-ready asset!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Recap: From Experiments to Optimized Models](#recap)\n",
    "2. [Introduction to the MLflow Model Registry](#intro-to-model-registry)\n",
    "3. [Setting Up: Environment and Training a Candidate Model](#setting-up)\n",
    "4. [Registering a Model to the MLflow Model Registry](#registering-a-model)\n",
    "5. [Interacting with the Model Registry via API](#interacting-with-registry)\n",
    "    - [Fetching Registered Models and Versions](#fetching-models)\n",
    "    - [Understanding Model Version Stages (Legacy)](#model-stages-legacy)\n",
    "    - [Using Model Version Aliases (Modern Approach)](#model-aliases)\n",
    "    - [Using Model Version Tags](#model-tags)\n",
    "6. [Loading a Model from the Registry#loading-from-registry)\n",
    "7. [Deploying a Registered Model Locally as a REST API#deploying-locally)\n",
    "    - [Using `mlflow models serve`#mlflow-models-serve)\n",
    "    - [Sending Test Requests#sending-requests)\n",
    "    - [Serving Frameworks: FastAPI and MLServer#serving-frameworks)\n",
    "8. [Brief Overview: Advanced Deployment Options#advanced-deployment)\n",
    "9. [Key Takeaways#key-takeaways)\n",
    "10. [Engaging Resources and Further Reading#resources-and-further-reading)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: From Experiments to Optimized Models\n",
    "\n",
    "In our previous notebooks, we:\n",
    "- Tracked individual model training experiments (Notebook 1).\n",
    "- Used Optuna for hyperparameter optimization, logging many trials (nested runs) to find the best set of hyperparameters for our XGBoost model on the California Housing dataset (Notebook 2).\n",
    "\n",
    "The outcome of Notebook 2 was an XGBoost model with optimized hyperparameters, which presumably performs better than our initial attempts. Now, what do we do with this 'best' model? This is where the Model Registry comes into play.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to the MLflow Model Registry\n",
    "\n",
    "The **MLflow Model Registry** is a centralized component for managing the full lifecycle of MLflow Models. It provides a collaborative environment to store, version, annotate, and manage models as they move from development to production.\n",
    "\n",
    "![MLFlow Workflow](https://mlflow.org/docs/latest/assets/images/learn-core-components-b2c38671f104ca6466f105a92ed5aa68.png)\n",
    "\n",
    "Key features and concepts include:\n",
    "- **Registered Model:** A named collection of model versions. For example, you might have a registered model named `california-housing-xgboost`.\n",
    "- **Model Version:** An individual iteration of a registered model. Each time you train and register a new version (e.g., with new data, new hyperparameters, or a new algorithm), it gets a new version number (1, 2, 3,...).\n",
    "- **Model Lineage:** The registry links each model version back to the MLflow experiment and run that produced it, ensuring traceability.\n",
    "- **Model Aliases:** Flexible, mutable named pointers to specific model versions (e.g., `@champion`, `@challenger`). This is the modern way to manage model deployment stages.\n",
    "- **Model Tags:** Key-value pairs for adding custom metadata to registered models and model versions (e.g., `validation_status: \"passed\"`, `data_version: \"v2.1\"`).\n",
    "- **Annotations and Descriptions:** Store detailed notes and descriptions for models and versions.\n",
    "\n",
    "**Benefits of using a Model Registry:**\n",
    "- **Centralized Management:** A single source of truth for all your production-worthy models.\n",
    "- **Version Control:** Systematically track changes and improvements to models.\n",
    "- **Collaboration:** Enables teams to work together on model development and deployment.\n",
    "- **Reproducibility & Governance:** Clear lineage and an audit trail for models.\n",
    "- **Simplified Deployment:** Easily fetch specific model versions for deployment.\n",
    "\n",
    "To use the MLflow Model Registry, especially when running your own MLflow server (even locally for these notebooks), you generally need a database-backed backend store (like SQLite, PostgreSQL, MySQL). For simple local `mlflow ui` runs with a file store, the registry functionality might be limited or behave slightly differently than a full server setup. However, for the commands we run in the notebook, the local file store setup (`mlruns`) often suffices for demonstration. If you encounter issues related to database backends, refer to the MLflow documentation on backend stores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up: Environment and Training a Candidate Model\n",
    "\n",
    "Let's set up our environment and train an XGBoost model on the California Housing dataset. For simplicity in this notebook, we'll train a model directly rather than trying to fetch the exact best run from Notebook 2. We'll use reasonably good hyperparameters that might have resulted from an HPO process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install --quiet mlflow xgboost scikit-learn datasets pandas requests\n",
    "\n",
    "# Import libraries\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import time # For creating unique model names/versions if needed\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri('mlruns') # Use local 'mlruns' directory\n",
    "experiment_name = \"California_Housing_Registry_Demo\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")\n",
    "\n",
    "# Load and prepare the dataset\n",
    "try:\n",
    "    housing_dataset = load_dataset('gvlassis/california_housing', split='train')\n",
    "    df = housing_dataset.to_pandas()\n",
    "    print(\"\\nSuccessfully loaded California Housing dataset.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {e}. Ensure internet connectivity.\")\n",
    "    raise e\n",
    "\n",
    "feature_columns = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "target_column = 'MedHouseVal'\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "\n",
    "# Train a 'good' XGBoost model (parameters might come from HPO in a real scenario)\n",
    "candidate_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 200, \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.2,\n",
    "    'lambda': 1.0,\n",
    "    'alpha': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost_For_Registry\") as run:\n",
    "    model_for_registry = xgb.XGBRegressor(**candidate_params, early_stopping_rounds=10)\n",
    "    model_for_registry.fit(X_train, y_train,\n",
    "                           eval_set=[(X_val, y_val)],\n",
    "                           verbose=False)\n",
    "    \n",
    "    y_pred = model_for_registry.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    mlflow.log_params(candidate_params)\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    \n",
    "    # Log the model - this is the first step before registering\n",
    "    # We give it an artifact_path within the run's artifacts\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model_for_registry,\n",
    "        artifact_path=\"xgboost-housing-candidate\", # This path is within the run's artifacts\n",
    "        input_example=X_train.iloc[:5], # Optional: log an input example for signature inference\n",
    "        signature=mlflow.models.infer_signature(X_train, y_train) # Optional: log model signature\n",
    "    )\n",
    "    \n",
    "    current_run_id = run.info.run_id\n",
    "    print(f\"\\nModel trained and logged with Run ID: {current_run_id}\")\n",
    "    print(f\"MSE: {mse:.4f}, R2 Score: {r2:.4f}\")\n",
    "    print(f\"Model artifact path in run: xgboost-housing-candidate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our candidate model is now trained and, importantly, logged to MLflow Tracking within a run. The `mlflow.xgboost.log_model()` function saved the model in the run's artifacts. This logged model is what we'll now register.\n",
    "\n",
    "![MLFlow Tracking](https://media.datacamp.com/cms/google/ad_4nxekg7ftko2m1hrkr-bwr-kq5gzr9wfugs9spjvgmoca-yykxhhepgcwxxo9yrbhu4barnqvmx6psn9scgku1car3lvlhltqnada0i9m7cg_glbdf5ty3lu4t3pcyxel6dyh1n84fcsl3xqvgdktujpvrian.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Registering a Model to the MLflow Model Registry\n",
    "\n",
    "Once a model is logged via `mlflow.<flavor>.log_model()`, you can add it to the Model Registry. There are a few ways to do this programmatically:\n",
    "\n",
    "1.  **During Logging:** Pass the `registered_model_name` argument directly to the `log_model` function. If the named model doesn't exist, it's created. If it exists, a new version is added.\n",
    "2.  **After Logging:** Use the `mlflow.register_model()` function, providing the `model_uri` of the logged model (e.g., `runs:/<RUN_ID>/<ARTIFACT_PATH>`) and a `name` for the registered model.\n",
    "\n",
    "Let's use the second approach, as we've already logged our model in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"CaliforniaHousingXGBoost\"\n",
    "# Ensure a unique name if running multiple times, or manage versions carefully\n",
    "# For this demo, we'll use a fixed name and let MLflow handle versioning.\n",
    "\n",
    "# Construct the model URI from the run where the model was logged\n",
    "# The artifact_path is 'xgboost-housing-candidate' as specified in log_model\n",
    "model_uri = f\"runs:/{current_run_id}/xgboost-housing-candidate\"\n",
    "\n",
    "try:\n",
    "    # Register the model\n",
    "    # If 'CaliforniaHousingXGBoost' doesn't exist, it will be created.\n",
    "    # If it exists, a new version of this model will be created.\n",
    "    registered_model_version = mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=registered_model_name,\n",
    "        tags={\"project\": \"HousingDemo\", \"framework\": \"XGBoost\"}, # Optional: tags for the registered model itself\n",
    "    )\n",
    "    print(f\"\\nModel registered successfully!\")\n",
    "    print(f\"Registered Model Name: {registered_model_version.name}\")\n",
    "    print(f\"Model Version: {registered_model_version.version}\")\n",
    "    print(f\"Source Run ID: {registered_model_version.run_id}\")\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    print(f\"Error registering model: {e}\")\n",
    "    print(\"This might happen if the model name is already in use with conflicting settings or due to backend store issues.\")\n",
    "    # In a real scenario, handle this more gracefully or ensure unique names/versions.\n",
    "    # For this demo, if it fails, we'll try to fetch an existing version if the error suggests it exists.\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    try:\n",
    "        latest_versions = client.get_latest_versions(registered_model_name)\n",
    "        if latest_versions:\n",
    "            registered_model_version = latest_versions[0]\n",
    "            print(f\"\\nUsing existing registered model version as fallback:\")\n",
    "            print(f\"Registered Model Name: {registered_model_version.name}\")\n",
    "            print(f\"Model Version: {registered_model_version.version}\")\n",
    "        else:\n",
    "            raise e # Re-raise if no versions found\n",
    "    except Exception as fallback_e:\n",
    "        print(f\"Could not retrieve existing model version: {fallback_e}\")\n",
    "        raise fallback_e\n",
    "\n",
    "# You can also register a model directly when logging it:\n",
    "# mlflow.xgboost.log_model(\n",
    "#     xgb_model=model_for_registry,\n",
    "#     artifact_path=\"xgboost-model-direct-reg\",\n",
    "#     registered_model_name=\"CaliforniaHousingXGBoostDirect\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now registered! If you run `mlflow ui` and navigate to the \"Models\" tab, you should see `CaliforniaHousingXGBoost` listed. Clicking on it will show its versions. Each version has details like its source run, creation timestamp, and any associated tags or descriptions.\n",
    "\n",
    "![MLflow UI Example](https://mlflow.org/docs/latest/assets/images/default-ui-e733e29706c434eb4443048ea57b8a17.png)\n",,
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interacting with the Model Registry via API\n",
    "\n",
    "MLflow provides a rich client API (`mlflow.tracking.MlflowClient`) to interact with the Model Registry programmatically. This is essential for automation and CI/CD workflows.\n",
    "\n",
    "Let's explore some common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Ensure we have a valid registered_model_version object from the previous cell\n",
    "if 'registered_model_version' not in locals() or registered_model_version is None:\n",
    "    print(\"Error: registered_model_version not defined. Please ensure the previous cell ran successfully.\")\n",
    "    # Attempt to fetch it again as a last resort\n",
    "    try:\n",
    "        latest_versions = client.get_latest_versions(registered_model_name)\n",
    "        if latest_versions:\n",
    "            registered_model_version = latest_versions[0] # Get the very latest\n",
    "            print(f\"Successfully fetched latest version: {registered_model_version.version} for model {registered_model_version.name}\")\n",
    "        else:\n",
    "            raise ValueError(f\"No versions found for model {registered_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fallback to fetch model version: {e}\")\n",
    "        # You might need to handle this error more robustly or stop the notebook\n",
    "        # For now, we'll try to proceed but operations below might fail.\n",
    "else:\n",
    "    print(f\"Using model: {registered_model_version.name}, version: {registered_model_version.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Registered Models and Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'registered_model_name' in locals() and registered_model_name:\n",
    "    print(f\"\\nDetails for Registered Model: {registered_model_name}\")\n",
    "    try:\n",
    "        model_details = client.get_registered_model(name=registered_model_name)\n",
    "        # print(model_details) # Full details\n",
    "        print(f\"  Name: {model_details.name}\")\n",
    "        print(f\"  Creation Timestamp: {pd.to_datetime(model_details.creation_timestamp, unit='ms')}\")\n",
    "        print(f\"  Last Updated Timestamp: {pd.to_datetime(model_details.last_updated_timestamp, unit='ms')}\")\n",
    "        print(f\"  Description: {model_details.description}\")\n",
    "        print(f\"  Tags: {model_details.tags}\")\n",
    "\n",
    "        print(f\"\\nLatest versions for '{registered_model_name}':\")\n",
    "        for mv in client.get_latest_versions(name=registered_model_name):\n",
    "            print(f\"  Version: {mv.version}, Stage/Alias: {mv.current_stage if hasattr(mv, 'current_stage') else 'N/A (check aliases)'}, Run ID: {mv.run_id}, Status: {mv.status}\")\n",
    "            # Note: current_stage is for older MLflow versions. Aliases are preferred now.\n",
    "            print(f\"    Aliases: {mv.aliases}\")\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        print(f\"Could not fetch model details for {registered_model_name}: {e}\")\n",
    "else:\n",
    "    print(\"Skipping model fetching as registered_model_name is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Model Version Stages (Legacy)\n",
    "\n",
    "Previously, MLflow used a fixed set of stages (`None`, `Staging`, `Production`, `Archived`) to denote a model version's lifecycle status. You could transition models between these stages.\n",
    "\n",
    "**However, MLflow is moving away from these fixed stages in favor of more flexible Model Version Aliases and Tags**. While you might still see 'Stage' in older UI versions or APIs, aliases provide better functionality.\n",
    "\n",
    "If you needed to transition stages (legacy approach):\n",
    "# `client.transition_model_version_stage(`\n",
    "#     `name=registered_model_name`,\n",
    "#     `version=registered_model_version.version`,\n",
    "#     `stage=\"Staging\"`\n",
    "# `)`\n",
    "\n",
    "We will focus on aliases and tags, which are the recommended modern approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Model Version Aliases (Modern Approach)\n",
    "\n",
    "**Aliases** are mutable, named pointers to specific model versions within a registered model. They are incredibly useful for managing which version is used in different environments (e.g., development, staging, production) or for specific purposes (e.g., `@champion`, `@challenger`).\n",
    "\n",
    "For example, you might have an alias `@prod` pointing to the current production model. When a new model is validated, you can update the `@prod` alias to point to the new version, seamlessly rolling out the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'registered_model_version' in locals() and registered_model_version:\n",
    "    champion_alias = \"champion\"\n",
    "    try:\n",
    "        # Set an alias for our current model version\n",
    "        client.set_registered_model_alias(\n",
    "            name=registered_model_name,\n",
    "            alias=champion_alias,\n",
    "            version=registered_model_version.version\n",
    "        )\n",
    "        print(f\"\\nSet alias '{champion_alias}' for model '{registered_model_name}' version {registered_model_version.version}\")\n",
    "\n",
    "        # Get the model version by alias\n",
    "        version_by_alias = client.get_model_version_by_alias(\n",
    "            name=registered_model_name,\n",
    "            alias=champion_alias\n",
    "        )\n",
    "        print(f\"Model version for alias '{champion_alias}': {version_by_alias.version}\")\n",
    "        print(f\"  Description for aliased version: {version_by_alias.description}\")\n",
    "        print(f\"  Current aliases for this version: {version_by_alias.aliases}\")\n",
    "\n",
    "        # To remove an alias (if needed later):\n",
    "        # client.delete_registered_model_alias(name=registered_model_name, alias=champion_alias)\n",
    "        # print(f\"\\nDeleted alias '{champion_alias}' for model '{registered_model_name}'\")\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        print(f\"Error managing alias: {e}\")\n",
    "        print(\"This could be due to the model version not being found or backend issues.\")\n",
    "else:\n",
    "    print(\"Skipping alias management as registered_model_version is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Model Version Tags\n",
    "\n",
    "**Tags** are key-value pairs that you can set on registered models or specific model versions for adding custom metadata. This is useful for categorization, status tracking, or any other information you want to associate with your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'registered_model_version' in locals() and registered_model_version:\n",
    "    try:\n",
    "        # Set a tag on the specific model version\n",
    "        client.set_model_version_tag(\n",
    "            name=registered_model_name,\n",
    "            version=registered_model_version.version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed_preliminary\"\n",
    "        )\n",
    "        client.set_model_version_tag(\n",
    "            name=registered_model_name,\n",
    "            version=registered_model_version.version,\n",
    "            key=\"data_used\",\n",
    "            value=\"california_housing_1990_census\"\n",
    "        )\n",
    "        print(f\"\\nSet tags for model '{registered_model_name}' version {registered_model_version.version}\")\n",
    "\n",
    "        # Get tags for the model version\n",
    "        version_with_tags = client.get_model_version(\n",
    "            name=registered_model_name,\n",
    "            version=registered_model_version.version\n",
    "        )\n",
    "        print(f\"Tags for version {version_with_tags.version}: {version_with_tags.tags}\")\n",
    "        \n",
    "        # To delete a tag:\n",
    "        # client.delete_model_version_tag(name=registered_model_name, version=registered_model_version.version, key=\"data_used\")\n",
    "        # print(\"\\nDeleted tag 'data_used'\")\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        print(f\"Error managing tags: {e}\")\n",
    "else:\n",
    "    print(\"Skipping tag management as registered_model_version is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aliases and tags provide a powerful and flexible way to manage the lifecycle and metadata of your models in the registry.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading a Model from the Registry\n",
    "\n",
    "Once a model is in the registry, you can easily load it for inference using a special `models:/` URI scheme:\n",
    "- `models:/<model_name>/<version_number>` (e.g., `models:/CaliforniaHousingXGBoost/1`)\n",
    "- `models:/<model_name>@<alias_name>` (e.g., `models:/CaliforniaHousingXGBoost@champion`)\n",
    "- `models:/<model_name>/latest` (to get the latest version, though using aliases for specific stages like production is safer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'registered_model_version' in locals() and registered_model_version:\n",
    "    loaded_model_from_registry = None\n",
    "    try:\n",
    "        # Load the model using its version number\n",
    "        model_uri_by_version = f\"models:/{registered_model_name}/{registered_model_version.version}\"\n",
    "        loaded_model_by_version = mlflow.xgboost.load_model(model_uri_by_version)\n",
    "        print(f\"\\nSuccessfully loaded model from URI: {model_uri_by_version}\")\n",
    "\n",
    "        # Load the model using its alias (if 'champion' alias was set)\n",
    "        if 'champion_alias' in locals():\n",
    "            model_uri_by_alias = f\"models:/{registered_model_name}@{champion_alias}\"\n",
    "            loaded_model_by_alias = mlflow.xgboost.load_model(model_uri_by_alias)\n",
    "            print(f\"Successfully loaded model from URI: {model_uri_by_alias}\")\n",
    "            loaded_model_from_registry = loaded_model_by_alias # Use this for next step\n",
    "        else:\n",
    "            loaded_model_from_registry = loaded_model_by_version\n",
    "\n",
    "        # Make a sample prediction\n",
    "        sample_prediction = loaded_model_from_registry.predict(X_val.iloc[:5])\n",
    "        print(f\"Sample predictions from loaded registry model: {sample_prediction}\")\n",
    "        \n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        print(f\"Error loading model from registry: {e}\")\n",
    "        print(\"Ensure the model name, version, and alias are correct and accessible.\")\n",
    "else:\n",
    "    print(\"Skipping model loading as registered_model_version is not defined.\")\n",
    "    loaded_model_from_registry = None # Ensure it's defined for the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it incredibly convenient to integrate specific model versions into your applications or downstream processes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploying a Registered Model Locally as a REST API\n",
    "\n",
    "MLflow facilitates deploying models as a local REST API server, which is great for testing, local applications, or as a step before more complex cloud deployments.\n",
    "\n",
    "### Using `mlflow models serve`\n",
    "The command `mlflow models serve` starts a local server for your model. You need to provide the model URI (which can be a run URI or, more appropriately here, a model registry URI) and a port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the server, you would typically open a new terminal, navigate to your project directory (where `mlruns` is), and execute a command like this. **Note: This command will block the terminal as long as the server is running. You'll need to run this in a separate terminal, not in a notebook cell directly if you want to interact with it from the same notebook immediately.**\n",
    "\n",
    "**In a new terminal, run:**\n",
    "`mlflow models serve -m \"models:/CaliforniaHousingXGBoost@champion\" -p 5001 --env-manager local`\n",
    "\n",
    "*(Replace `@champion` with the specific version if you haven't set the alias, e.g., `models:/CaliforniaHousingXGBoost/1`)*\n",
    "*(The `--env-manager local` flag tells MLflow to use the current Python environment. For more isolated environments, conda or virtualenv can be used.)*\n",
    "\n",
    "Once the server is running, it will listen for POST requests at the `/invocations` endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Test Requests\n",
    "\n",
    "After starting the server in a separate terminal, you can send requests to it. The input data should typically be JSON formatted according to what the model expects (e.g., pandas DataFrame split or records format). MLflow's default serving input format for scikit-learn compatible models is often a dictionary with an \"inputs\" key or \"dataframe_split\"/\"dataframe_records\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample data for the request\n",
    "# For scikit-learn/XGBoost, pandas split-oriented JSON is common.\n",
    "sample_data_for_api = X_val.iloc[:3]\n",
    "json_input_pandas_split = {\n",
    "    \"dataframe_split\": sample_data_for_api.to_dict(orient='split')\n",
    "}\n",
    "# Alternative: records orientation\n",
    "# json_input_pandas_records = {\n",
    "#     \"dataframe_records\": sample_data_for_api.to_dict(orient='records')\n",
    "# }\n",
    "\n",
    "# The server should be running on http://127.0.0.1:5001 from the command above\n",
    "inference_url = \"http://127.0.0.1:5001/invocations\"\n",
    "\n",
    "print(\"Ensure the MLflow server is running in a separate terminal with the command:\")\n",
    "print(\"mlflow models serve -m \\\"models:/CaliforniaHousingXGBoost@champion\\\" -p 5001 --env-manager local\\n\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        inference_url,\n",
    "        data=json.dumps(json_input_pandas_split),\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "    \n",
    "    predictions = response.json()\n",
    "    print(f\"Predictions from local API server: {predictions}\")\n",
    "    \n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"Could not connect to the MLflow server at {inference_url}.\")\n",
    "    print(\"Please ensure the server is running in a separate terminal.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "print(\"\\n--- Remember to stop the 'mlflow models serve' process in the other terminal (Ctrl+C) when done. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the server is running and the request is formatted correctly, you should get predictions back! This demonstrates a basic MLOps loop: train, register, version, alias, deploy (locally), and infer.\n",
    "\n",
    "### Serving Frameworks: FastAPI and MLServer\n",
    "By default, MLflow uses **FastAPI** to serve the inference endpoint. FastAPI is a modern, high-performance Python web framework that handles requests asynchronously.\n",
    "\n",
    "MLflow also supports **MLServer** as an alternative serving engine. MLServer is designed for high-performance ML workloads and can offer better throughput and scalability, especially when integrated with Kubernetes-native frameworks like Seldon Core or KServe. You might need to install `mlserver` and its MLflow integration separately to use it.\n",
    "\n",
    "| Feature         | FastAPI (default with MLflow)                                  | MLServer                                                                 |\n",
    "|-----------------|----------------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **Set Up**      | Installed by default with MLflow.                            | Needs to be installed separately (`pip install mlserver mlserver-mlflow`). |\n",
    "| **Performance** | Good, supports async. Suitable for many use cases.             | Optimized for high-performance, parallel inference, adaptive batching.   |\n",
    "| **Scalability** | Basic horizontal scaling via multiple `uvicorn` workers.       | Advanced scaling with Kubernetes (Seldon/KServe), workload offloading.   |\n",
    "| **Use Case**    | Standard use cases, local testing, simpler production setups.  | High-scale production environments, complex MLOps pipelines.             |\n",
    "\n",
    "For local serving and testing, FastAPI is usually sufficient and very convenient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Brief Overview: Advanced Deployment Options\n",
    "\n",
    "While local deployment is great for testing, real-world production scenarios often require more robust solutions. MLflow supports various deployment avenues:\n",
    "\n",
    "- **Cloud Platforms:** Deploy models to services like Azure Machine Learning, Amazon SageMaker, and Google Cloud AI Platform. MLflow often has plugins or integrations for these.\n",
    "- **Kubernetes:** Package your model using Docker (MLflow can help build Docker images for your model via `mlflow models build-docker`) and deploy it to a Kubernetes cluster for scalability and resilience.\n",
    "- **Custom Deployment Plugins:** MLflow's plugin architecture allows for integration with custom deployment tools and platforms.\n",
    "\n",
    "These advanced options typically involve containerizing your model and its environment (which MLflow helps with by capturing dependencies) and then using orchestration tools to manage the deployed service. We won't dive deep into these here, but it's good to know that MLflow provides pathways to these more complex deployments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "In this notebook, we've covered critical MLOps practices using MLflow:\n",
    "\n",
    "- **MLflow Model Registry:** Understood its role as a central hub for managing the lifecycle of versioned models.\n",
    "- **Registering Models:** Learned how to register a logged MLflow model, creating versions and linking them to their source runs.\n",
    "- **Model Version Management:** Explored using **Model Aliases** (e.g., `@champion`) and **Model Tags** (e.g., `validation_status: \"passed\"`) for flexible and modern lifecycle management, moving beyond legacy stages.\n",
    "- **Loading from Registry:** Fetched specific model versions or aliased models programmatically for use in applications.\n",
    "- **Local Deployment:** Deployed a registered model as a local REST API using `mlflow models serve` and successfully sent inference requests to it.\n",
    "- **Serving Infrastructure Awareness:** Briefly touched upon default (FastAPI) and alternative (MLServer) serving frameworks, and the path to more advanced deployment targets.\n",
    "\n",
    "These capabilities are essential for building robust, reproducible, and maintainable machine learning systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Engaging Resources and Further Reading\n",
    "\n",
    "To learn more about the MLflow Model Registry and deployment:\n",
    "\n",
    "- **MLflow Official Documentation:**\n",
    "    - [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)\n",
    "    - [MLflow Deployment Concepts](https://mlflow.org/docs/latest/deployment/index.html)\n",
    "    - [Deploy MLflow Model as a Local Inference Server](https://mlflow.org/docs/latest/deployment/deploy-model-locally/)\n",
    "    - [MLflow `mlflow.tracking.MlflowClient` API](https://mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient)\n",
    "- **Blogs and Articles:**\n",
    "    - [ML Model Registry: The Ultimate Guide (Neptune.ai blog, often covers various tools including MLflow)](https://neptune.ai/blog/ml-model-registry)\n",
    "    - [MLflow Production Deployment Guide (Restack.io)](https://www.restack.io/docs/mlflow-knowledge-mlflow-production-deployment)\n",
    "\n",
    "--- \n",
    "\n",
    "Excellent work reaching this point! You're now equipped with the knowledge to manage and deploy your MLflow models effectively.\n",
    "\n",
    "**Coming Up Next:** We'll start our journey into the exciting world of Generative AI, beginning with how to track and manage Retrieval-Augmented Generation (RAG) pipelines using MLflow and LlamaIndex/LangChain. This will be a significant step into more complex, LLM-based applications!\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
