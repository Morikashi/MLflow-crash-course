{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 08: Advanced Function-Calling and Agent-to-Agent Protocols in LLM Apps\n",
    "\n",
    "Welcome to Notebook 8 of our MLflow series! In [Notebook 7](MLflow_07_Tool_Calling_Agents_with_LangGraph_Ollama_and_MLflow.ipynb), we built our first tool-calling agent using LangGraph and Ollama, tracing its actions with MLflow. Now, we're taking it a step further by exploring:\n",
    "\n",
    "1.  **Advanced Function Calling:** Using Pydantic models to define robust schemas for our tools, enabling LLMs to generate more structured and reliable arguments for function execution.\n",
    "2.  **Agent-to-Agent (A2A) Interaction Concepts:** Designing systems where multiple specialized agents (or agentic components) collaborate to achieve a common goal. We'll implement a hierarchical agent system using LangGraph.\n",
    "\n",
    "We'll continue to use **LangGraph** for building these sophisticated agentic workflows, **Ollama** for local LLM capabilities (with models like `phi3:mini` or `Qwen3-0.6B`), and **MLflow Tracing** to meticulously capture and visualize these complex interactions.\n",
    "\n",
    "![Agent Collaboration Concept](https://www.researchgate.net/publication/344421802/figure/fig1/AS:942773028655106@1601794726089/Conceptual-model-of-a-multi-agent-system-MAS-architecture-The-MAS-is-composed-of.png)\n",
    "\n",
    "Let's dive into building more intelligent and collaborative LLM applications!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Recap: Tool-Calling Agents](#recap-tool-calling)\n",
    "2. [Advanced Function Calling with Pydantic](#advanced-function-calling-pydantic)\n",
    "    - [Why Pydantic for Tool Schemas?](#why-pydantic)\n",
    "    - [Defining Tools with Pydantic Schemas](#defining-pydantic-tools)\n",
    "    - [Integrating Pydantic Tools with LangChain/LangGraph](#integrating-pydantic-tools)\n",
    "3. [Setting Up the Multi-Agent Environment](#setting-up-multi-agent-env)\n",
    "    - [Installing Libraries](#installing-libraries-multi-agent)\n",
    "    - [Ollama and LLM Setup](#ollama-llm-setup-multi-agent)\n",
    "    - [MLflow Configuration (Tracing Focus)](#mlflow-config-multi-agent)\n",
    "4. [Building a Hierarchical Multi-Agent System with LangGraph](#building-hierarchical-mas)\n",
    "    - [Scenario: Automated Content Generation Assistant](#scenario-content-gen)\n",
    "    - [Defining Agent Roles and Tools (with Pydantic)](#defining-agent-roles-tools)\n",
    "        - Tool 1: Market Research Tool (mock)\n",
    "        - Tool 2: Slogan Generation Tool (mock LLM call)\n",
    "    - [Defining the Overall Agent State](#defining-overall-agent-state)\n",
    "    - [Supervisor Agent Logic (Router)](#supervisor-agent-logic)\n",
    "    - [Worker Agent Nodes (Tool Executor, specialized LLM calls)](#worker-agent-nodes)\n",
    "    - [Constructing the Multi-Agent Graph](#constructing-multi-agent-graph)\n",
    "5. [Running and Tracing the Multi-Agent System](#running-tracing-multi-agent)\n",
    "    - [Invoking the Multi-Agent System](#invoking-multi-agent)\n",
    "    - [Analyzing Multi-Agent Traces in MLflow UI](#analyzing-multi-agent-traces)\n",
    "6. [Conceptual Overview: Other Agent-to-Agent Protocols](#overview-a2a-protocols)\n",
    "    - [Message Passing, Shared State, Blackboard Systems](#message-passing-shared-state)\n",
    "    - [Protocols like MCP (Multi-Agent Communication Protocol)](#mcp-concept)\n",
    "7. [Key Takeaways](#key-takeaways-multi-agent)\n",
    "8. [Engaging Resources and Further Reading](#resources-further-reading-multi-agent)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: Tool-Calling Agents\n",
    "\n",
    "In [Notebook 7](MLflow_07_Tool_Calling_Agents_with_LangGraph_Ollama_and_MLflow.ipynb), we built an agent that could decide to use simple tools based on user queries. Key components included:\n",
    "- An LLM (from Ollama) for reasoning and tool selection.\n",
    "- Tools defined as Python functions (e.g., weather, calculator).\n",
    "- LangGraph to orchestrate the flow: LLM call -> (optional) Tool Execution -> LLM call for final response.\n",
    "- MLflow to trace these interactions.\n",
    "\n",
    "Now, we'll enhance the robustness of tool definitions and explore how multiple agent-like components can collaborate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Function Calling with Pydantic\n",
    "\n",
    "LLMs that support function calling (or tool calling) are trained to generate a structured JSON object containing the name of the function to call and the arguments to pass to it. Providing a clear and precise schema for these functions is crucial for reliable performance.\n",
    "\n",
    "### Why Pydantic for Tool Schemas?\n",
    "**Pydantic** is a data validation and settings management library using Python type annotations. Using Pydantic models to define the expected arguments for your tools offers several advantages:\n",
    "- **Clear Schemas:** Type hints define the expected data types, descriptions, and whether fields are required or optional.\n",
    "- **Automatic JSON Schema Generation:** Pydantic models can automatically generate JSON Schemas, which is the format most LLMs expect for tool definitions.\n",
    "- **Data Validation:** When the LLM generates arguments, Pydantic can validate them against your schema before your tool code even runs, catching errors early.\n",
    "- **IDE Support:** Better autocompletion and type checking in your development environment.\n",
    "\n",
    "LangChain has excellent integration with Pydantic for defining tool arguments [1, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tools with Pydantic Schemas\n",
    "Let's redefine a simple tool using Pydantic for its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool # Re-import for clarity if needed\n",
    "\n",
    "class SearchToolInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query string for information retrieval.\")\n",
    "    num_results: int = Field(default=3, description=\"The maximum number of search results to return.\")\n",
    "\n",
    "@tool(args_schema=SearchToolInput)\n",
    "def web_search_mock(query: str, num_results: int = 3) -> str:\n",
    "    \"\"\"Simulates a web search for a given query and returns a specified number of mock results.\"\"\"\n",
    "    print(f\"--- Tool Called: web_search_mock(query='{query}', num_results={num_results}) ---\")\n",
    "    # In a real scenario, this would call a search API (Google, Bing, Tavily, etc.)\n",
    "    mock_results = [\n",
    "        f\"Mock result 1 for '{query}': The importance of AI in modern technology.\",\n",
    "        f\"Mock result 2 for '{query}': Recent advancements in renewable energy sources.\",\n",
    "        f\"Mock result 3 for '{query}': A guide to effective project management.\",\n",
    "        f\"Mock result 4 for '{query}': The history of the internet.\",\n",
    "        f\"Mock result 5 for '{query}': Understanding climate change impacts.\"\n",
    "    ]\n",
    "    return f\"Found {min(num_results, len(mock_results))} results for '{query}':\\n\" + \"\\n\".join(mock_results[:num_results])\n",
    "\n",
    "print(\"Web search tool with Pydantic schema defined.\")\n",
    "# You can inspect the JSON schema LangChain generates for the LLM:\n",
    "# from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "# print(convert_to_openai_tool(web_search_mock))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this tool is provided to an LLM capable of function calling, the LLM will receive the JSON schema derived from `SearchToolInput`. If it decides to call `web_search_mock`, it will attempt to generate arguments matching this schema (e.g., `{\"query\": \"AI benefits\", \"num_results\": 2}`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Multi-Agent Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow langchain langgraph langchain_community langchain_core langchain_ollama pydantic tiktoken\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, List, Union, Optional, Sequence\n",
    "import json # For pretty printing tool calls\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "# from langchain_core.tools import tool # Already imported\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver # If needed for more complex state persistence\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "import langchain\n",
    "print(f\"Langchain Version: {langchain.__version__}\") \n",
    "import langgraph\n",
    "print(f\"LangGraph Version: {langgraph.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama and LLM Setup\n",
    "We'll use `phi3:mini` again, as it's efficient and supports function calling well. Ensure Ollama is running and the model is pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model_name = \"phi3:mini\" # Or \"qwen3:0.6b\" / \"llama3:8b\"\n",
    "llm = None\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        model=ollama_model_name, \n",
    "        temperature=0, # Deterministic for agent decisions\n",
    "        keep_alive=\"5m\"\n",
    "    )\n",
    "    response_test = llm.invoke(\"Test connection to Ollama.\")\n",
    "    print(f\"Ollama ({ollama_model_name}) connected. Test response: {response_test.content[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama ({ollama_model_name}): {e}. Ensure Ollama is running and model is pulled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Configuration (Tracing Focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns')\n",
    "experiment_name = \"LangGraph_Advanced_FunctionCalling_A2A\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow.langchain.autolog(log_models=False, \n",
    "                         log_input_examples=True, \n",
    "                         log_output_examples=True, \n",
    "                         extra_tags={\"agent_type\": \"HierarchicalLangGraph\"})\n",
    "\n",
    "print(f\"MLflow Experiment set to: {experiment_name}. Autologging enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Hierarchical Multi-Agent System with LangGraph\n",
    "\n",
    "We'll simulate a multi-agent system where a \"Supervisor\" agent coordinates tasks between specialized \"Worker\" agents. This is a common and powerful pattern for building complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario: Automated Content Generation Assistant\n",
    "**User Request:** \"Draft an email to potential investors about our new AI-powered recipe generation app. Highlight its benefits, mention the current market size for food tech apps (use web search), and include a short, catchy slogan for the app.\"\n",
    "\n",
    "**Agent Roles:**\n",
    "- **Supervisor Agent:** Receives the main request, decides which worker agent needs to act next (researcher or slogan writer), routes tasks, and aggregates results for final drafting.\n",
    "- **Market Researcher Agent (Worker 1):** Uses the `web_search_mock` tool to find market size information.\n",
    "- **Slogan Writer Agent (Worker 2):** Generates a catchy slogan (we'll simulate this with a dedicated LLM call, or a simple tool).\n",
    "- **Email Drafter (Final Step):** An LLM call that takes all gathered information and drafts the final email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Agent Roles and Tools (with Pydantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool 1: Market Research Tool (already defined `web_search_mock`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool 2: Slogan Generation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SloganToolInput(BaseModel):\n",
    "    product_name: str = Field(description=\"The name of the product or app.\")\n",
    "    product_description: str = Field(description=\"A brief description of the product or app.\")\n",
    "\n",
    "@tool(args_schema=SloganToolInput)\n",
    "def generate_slogan_mock(product_name: str, product_description: str) -> str:\n",
    "    \"\"\"Generates a catchy slogan for a given product name and description. (Mock implementation)\"\"\"\n",
    "    print(f\"--- Tool Called: generate_slogan_mock(product_name='{product_name}', description='{product_description[:30]}...') ---\")\n",
    "    # In a real scenario, this could be another LLM call with a specific prompt for slogan generation.\n",
    "    if \"recipe\" in product_description.lower() or \"recipe\" in product_name.lower():\n",
    "        return f\"'{product_name}: Your Kitchen's AI Companion!' or '{product_name}: Cook Smarter, Not Harder!'\"\n",
    "    else:\n",
    "        return f\"'{product_name}: Innovation Delivered!'\"\n",
    "\n",
    "all_tools = [web_search_mock, generate_slogan_mock]\n",
    "if llm:\n",
    "    llm_with_all_tools = llm.bind_tools(all_tools)\n",
    "    print(\"LLM bound with all defined tools.\")\n",
    "else:\n",
    "    llm_with_all_tools = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Overall Agent State\n",
    "The state needs to hold the initial request, intermediate results from worker agents (market research, slogan), and the final drafted email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterAgentState(TypedDict):\n",
    "    original_request: str\n",
    "    messages: Annotated[List[BaseMessage], operator.add] # Conversation history, tool calls/responses\n",
    "    market_research_data: Optional[str] = None\n",
    "    slogan_text: Optional[str] = None\n",
    "    drafted_email: Optional[str] = None\n",
    "    next_action: Optional[str] = None # To guide the supervisor: 'research', 'slogan', 'draft', 'finish'\n",
    "\n",
    "print(\"MasterAgentState defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor Agent Logic (Router)\n",
    "The supervisor decides the next step based on the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_router_logic(state: MasterAgentState) -> str:\n",
    "    \"\"\"Decides the next action based on what information is still needed.\"\"\"\n",
    "    print(\"--- Node: supervisor_router_logic ---\")\n",
    "    if state.get(\"market_research_data\") is None:\n",
    "        print(\"  Decision: Need market research.\")\n",
    "        return \"route_to_researcher\"\n",
    "    elif state.get(\"slogan_text\") is None:\n",
    "        print(\"  Decision: Need slogan.\")\n",
    "        return \"route_to_slogan_writer\"\n",
    "    elif state.get(\"drafted_email\") is None:\n",
    "        print(\"  Decision: Ready to draft email.\")\n",
    "        return \"route_to_drafter\"\n",
    "    else:\n",
    "        print(\"  Decision: All tasks complete. Finish.\")\n",
    "        return END # All done\n",
    "\n",
    "print(\"Supervisor router logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Agent Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_researcher_node(state: MasterAgentState) -> dict:\n",
    "    \"\"\"Worker agent node that performs market research using the web_search_mock tool.\"\"\"\n",
    "    print(\"--- Node: market_researcher_node ---\")\n",
    "    if not llm_with_all_tools:\n",
    "        return {\"market_research_data\": \"Error: LLM not initialized for researcher.\"}\n",
    "        \n",
    "    # Craft a specific prompt for the LLM to call the web_search_mock tool\n",
    "    research_prompt = HumanMessage(content=f\"Find market size information for food tech apps. Original request: {state['original_request']}\")\n",
    "    print(f\"  Researcher sending to LLM: {research_prompt.content}\")\n",
    "    \n",
    "    # LLM decides to call the tool\n",
    "    ai_response = llm_with_all_tools.invoke([research_prompt])\n",
    "    print(f\"  Researcher LLM response (tool call expected): {ai_response}\")\n",
    "    \n",
    "    if ai_response.tool_calls:\n",
    "        tool_call = ai_response.tool_calls[0] # Assume first tool call is the relevant one\n",
    "        if tool_call['name'] == web_search_mock.name:\n",
    "            tool_output = web_search_mock.invoke(tool_call['args'])\n",
    "            print(f\"  Market research tool output: {tool_output}\")\n",
    "            return {\"market_research_data\": str(tool_output), \"messages\": [ai_response, ToolMessage(content=str(tool_output), tool_call_id=tool_call['id'])]}\n",
    "    return {\"market_research_data\": \"Market research failed or tool not called.\", \"messages\": [ai_response]}\n",
    "\n",
    "def slogan_writer_node(state: MasterAgentState) -> dict:\n",
    "    \"\"\"Worker agent node that generates a slogan using the generate_slogan_mock tool.\"\"\"\n",
    "    print(\"--- Node: slogan_writer_node ---\")\n",
    "    if not llm_with_all_tools:\n",
    "        return {\"slogan_text\": \"Error: LLM not initialized for slogan writer.\"}\n",
    "\n",
    "    slogan_prompt = HumanMessage(content=f\"Generate a catchy slogan for an AI-powered recipe generation app. Original request: {state['original_request']}\")\n",
    "    print(f\"  Slogan writer sending to LLM: {slogan_prompt.content}\")\n",
    "    \n",
    "    ai_response = llm_with_all_tools.invoke([slogan_prompt])\n",
    "    print(f\"  Slogan writer LLM response (tool call expected): {ai_response}\")\n",
    "\n",
    "    if ai_response.tool_calls:\n",
    "        tool_call = ai_response.tool_calls[0]\n",
    "        if tool_call['name'] == generate_slogan_mock.name:\n",
    "            tool_output = generate_slogan_mock.invoke(tool_call['args'])\n",
    "            print(f\"  Slogan generation tool output: {tool_output}\")\n",
    "            return {\"slogan_text\": str(tool_output), \"messages\": [ai_response, ToolMessage(content=str(tool_output), tool_call_id=tool_call['id'])]}\n",
    "    return {\"slogan_text\": \"Slogan generation failed or tool not called.\", \"messages\": [ai_response]}\n",
    "\n",
    "def email_drafter_node(state: MasterAgentState) -> dict:\n",
    "    \"\"\"Node that drafts the final email using all gathered information.\"\"\"\n",
    "    print(\"--- Node: email_drafter_node ---\")\n",
    "    if not llm: # Use the base LLM without tools for drafting, or llm_with_all_tools if it's general purpose\n",
    "        return {\"drafted_email\": \"Error: LLM not initialized for drafting.\"}\n",
    "\n",
    "    draft_prompt_text = (\n",
    "        f\"Draft a compelling email to potential investors about our new AI-powered recipe generation app. \"\n",
    "        f\"Original user request: '{state['original_request']}'.\\n\"\n",
    "        f\"Key Benefits: (The app is AI-powered, helps generate recipes, etc. - infer from original request or add more context here).\\n\"\n",
    "        f\"Market Size Information: {state['market_research_data']}\\n\"\n",
    "        f\"Catchy Slogan: {state['slogan_text']}\\n\\n\"\n",
    "        f\"Please write the full email now based on this information.\"\n",
    "    )\n",
    "    print(f\"  Email drafter sending to LLM: {draft_prompt_text[:200]}...\")\n",
    "    \n",
    "    # Direct LLM call for drafting\n",
    "    final_email_response = llm.invoke([HumanMessage(content=draft_prompt_text)])\n",
    "    drafted_email = final_email_response.content\n",
    "    print(f\"  Drafted Email: {drafted_email}\")\n",
    "    return {\"drafted_email\": drafted_email, \"messages\": [final_email_response]}\n",
    "\n",
    "print(\"Worker agent nodes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Multi-Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_agent_workflow = StateGraph(MasterAgentState)\n",
    "\n",
    "multi_agent_workflow.add_node(\"supervisor\", supervisor_router_logic)\n",
    "multi_agent_workflow.add_node(\"research_worker\", market_researcher_node)\n",
    "multi_agent_workflow.add_node(\"slogan_worker\", slogan_writer_node)\n",
    "multi_agent_workflow.add_node(\"drafting_worker\", email_drafter_node)\n",
    "\n",
    "multi_agent_workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# Conditional edges from supervisor to workers or END\n",
    "multi_agent_workflow.add_conditional_edges(\n",
    "    start_node_name=\"supervisor\",\n",
    "    condition=lambda state: state[\"next_action\"], # This assumes supervisor_router_logic sets 'next_action'\n",
    "                                                  # Let's refine supervisor_router_logic to set next_action or directly return node name\n",
    "                                                  # For simplicity, we'll use the direct return value from supervisor_router_logic\n",
    "    condition=supervisor_router_logic, # The condition function itself returns the name of the next node or END\n",
    "    conditional_edge_mapping={\n",
    "        \"route_to_researcher\": \"research_worker\",\n",
    "        \"route_to_slogan_writer\": \"slogan_worker\",\n",
    "        \"route_to_drafter\": \"drafting_worker\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edges from workers back to the supervisor to re-evaluate state\n",
    "multi_agent_workflow.add_edge(\"research_worker\", \"supervisor\")\n",
    "multi_agent_workflow.add_edge(\"slogan_worker\", \"supervisor\")\n",
    "multi_agent_workflow.add_edge(\"drafting_worker\", \"supervisor\") # After drafting, supervisor will see all fields are filled and END\n",
    "\n",
    "if llm: # Only compile if LLM is available\n",
    "    multi_agent_app = multi_agent_workflow.compile()\n",
    "    print(\"Multi-agent LangGraph app compiled.\")\n",
    "    # from IPython.display import Image, display\n",
    "    # try: display(Image(multi_agent_app.get_graph().draw_mermaid_png()))\n",
    "    # except: print(\"Graphviz not installed, skipping graph draw.\")\n",
    "else:\n",
    "    print(\"LLM not initialized, skipping multi-agent graph compilation.\")\n",
    "    multi_agent_app = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLFlow Workflow](https://mlflow.org/docs/latest/assets/images/learn-core-components-b2c38671f104ca6466f105a92ed5aa68.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running and Tracing the Multi-Agent System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Multi-Agent System\n",
    "We'll provide the initial user request. MLflow autologging should capture the entire trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_complex = \"Draft an email to potential investors about our new AI-powered recipe generation app called 'ChefMate'. Highlight its benefits like personalized meal plans and easy grocery list generation. Mention the current market size for food tech apps (use web search for 'food tech app market size'), and include a short, catchy slogan for ChefMate.\"\n",
    "\n",
    "if multi_agent_app:\n",
    "    print(f\"\\n--- Running Multi-Agent System for Query: '{user_query_complex[:50]}...' ---\")\n",
    "    initial_state = {\n",
    "        \"original_request\": user_query_complex,\n",
    "        \"messages\": [HumanMessage(content=user_query_complex)]\n",
    "        # market_research_data, slogan_text, drafted_email will be filled by the graph\n",
    "    }\n",
    "    \n",
    "    # Each invoke of the compiled LangGraph app will be traced by MLflow\n",
    "    # We wrap it in a parent MLflow run for overall task context\n",
    "    with mlflow.start_run(run_name=\"MultiAgent_ContentGeneration_Run\") as run:\n",
    "        mlflow.log_param(\"initial_user_query\", user_query_complex)\n",
    "        mlflow.log_param(\"ollama_model_used\", ollama_model_name)\n",
    "        mlflow.set_tag(\"system_type\", \"Hierarchical Multi-Agent\")\n",
    "        \n",
    "        try:\n",
    "            final_state_multi_agent = multi_agent_app.invoke(initial_state, config={\"recursion_limit\": 15})\n",
    "            \n",
    "            print(\"\\n--- Multi-Agent System Final State ---\")\n",
    "            print(f\"Original Request: {final_state_multi_agent.get('original_request')}\")\n",
    "            print(f\"Market Research: {final_state_multi_agent.get('market_research_data')}\")\n",
    "            print(f\"Slogan: {final_state_multi_agent.get('slogan_text')}\")\n",
    "            print(f\"Drafted Email:\\n{final_state_multi_agent.get('drafted_email')}\")\n",
    "            \n",
    "            mlflow.log_text(str(final_state_multi_agent.get('market_research_data', '')), \"market_research_output.txt\")\n",
    "            mlflow.log_text(str(final_state_multi_agent.get('slogan_text', '')), \"slogan_output.txt\")\n",
    "            mlflow.log_text(str(final_state_multi_agent.get('drafted_email', '')), \"final_drafted_email.txt\")\n",
    "            mlflow.set_tag(\"overall_outcome\", \"success\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking multi-agent system: {e}\")\n",
    "            mlflow.log_text(str(e), \"multi_agent_error.txt\")\n",
    "            mlflow.set_tag(\"overall_outcome\", \"error\")\n",
    "else:\n",
    "    print(\"Skipping multi-agent system run as the app was not compiled (likely LLM issue).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Multi-Agent Traces in MLflow UI\n",
    "Go to the MLflow UI (`mlflow ui`):\n",
    "- Open the `LangGraph_Advanced_FunctionCalling_A2A` experiment.\n",
    "- Find the run `MultiAgent_ContentGeneration_Run` (and its child trace from autologging).\n",
    "- **Inspect the Trace:** Observe how the `supervisor` node routes to `research_worker`, then `slogan_worker`, then `drafting_worker`. \n",
    "- Click on individual spans (e.g., `research_worker`'s LLM call) to see its specific prompt, the tool call it generated (with Pydantic-structured args), and the tool's output.\n",
    "- See how the state (market research data, slogan) gets populated and used by downstream nodes.\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)\n",
    "\n",
    "This detailed tracing is crucial for debugging and understanding the flow in such complex, multi-step, multi-component systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conceptual Overview: Other Agent-to-Agent Protocols\n",
    "\n",
    "While our hierarchical system demonstrates one way for agents to collaborate (coordinated by a supervisor), other A2A patterns and protocols exist:\n",
    "\n",
    "- **Message Passing:** Agents communicate by sending explicit messages to each other (e.g., via a message bus or direct calls if they expose APIs). The content and format of messages are key.\n",
    "- **Shared State / Blackboard Systems:** Agents read from and write to a common, shared data structure (the \"blackboard\"). This allows for more decoupled interaction, as agents react to changes in the shared state.\n",
    "- **Peer-to-Peer Collaboration:** Agents might negotiate tasks, share partial results, or critique each other's work without a central supervisor.\n",
    "- **Formal Protocols (e.g., FIPA, MCP):**\n",
    "    - **FIPA (Foundation for Intelligent Physical Agents):** Defines standards for agent communication languages (ACL), interaction protocols, and architectures.\n",
    "    - **MCP (Multi-Agent Communication Protocol):** A newer initiative aiming to standardize how LLM-based agents communicate, potentially using JSON-based message formats for requests, responses, errors, etc. While not a library itself, it's a specification that future agent frameworks might adopt.\n",
    "\n",
    "LangGraph's flexibility allows you to model many of these patterns. For example, the `AgentState` can act as a form of shared state, and the graph structure defines the message flow and control logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "This notebook has equipped you with more advanced agent-building techniques:\n",
    "\n",
    "- **Robust Function Calling with Pydantic:** Using Pydantic models for tool argument schemas improves reliability and clarity in how LLMs interact with your tools.\n",
    "- **Hierarchical Multi-Agent Systems:** You've built a system where a supervisor agent coordinates tasks among specialized worker agents using LangGraph. This pattern is scalable and modular.\n",
    "- **Complex Workflow Orchestration:** LangGraph provides the control flow (nodes, conditional edges) needed to manage intricate interactions between multiple LLM calls, tool executions, and state updates.\n",
    "- **Deep Tracing with MLflow:** MLflow's autologging for LangChain/LangGraph provides essential visibility into these complex multi-step and multi-component executions, crucial for debugging and optimization.\n",
    "- **Foundation for Advanced A2A:** While we implemented a hierarchical system, the concepts extend to more decentralized or formal A2A protocols.\n",
    "\n",
    "Building effective multi-agent systems requires careful design of agent roles, responsibilities, communication pathways (state updates in our LangGraph case), and robust tool interfaces.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Engaging Resources and Further Reading\n",
    "\n",
    "- **LangGraph & LangChain Documentation:**\n",
    "    - [LangGraph Multi-Agent Collaboration Examples](https://python.langchain.com/docs/langgraph#multi-agent-collaboration) (and other examples like Agent Supervisor)\n",
    "    - [LangChain Pydantic Tools](https://python.langchain.com/docs/modules/agents/tools/custom_tools#structuredtool-and-pydantic)\n",
    "- **Pydantic:**\n",
    "    - [Pydantic Documentation](https://docs.pydantic.dev/latest/)\n",
    "- **Multi-Agent Systems Theory & Practice:**\n",
    "    - \"Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations\" by Yoav Shoham and Kevin Leyton-Brown (Comprehensive textbook).\n",
    "    - Search for recent research papers on LLM-based multi-agent systems.\n",
    "- **Agent Communication Protocols:**\n",
    "    - [FIPA Standards](http://www.fipa.org/specs/index.html)\n",
    "    - Keep an eye on emerging protocols like MCP if they gain traction.\n",
    "\n",
    "--- \n",
    "\n",
    "Fantastic work on completing this notebook! You're now well on your way to designing and tracing sophisticated, collaborative LLM applications.\n",
    "\n",
    "**Coming Up Next (Notebook 9):** We'll look at implementing custom metrics and more nuanced evaluation strategies for generative AI tasks, building on our earlier evaluation work but focusing on the unique challenges of generative models.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
