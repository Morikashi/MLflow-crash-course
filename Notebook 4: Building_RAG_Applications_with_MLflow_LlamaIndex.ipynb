{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 04: Building RAG Applications with MLflow and LlamaIndex\n",
    "\n",
    "Welcome to a new frontier in our MLflow series! Having explored experiment tracking, hyperparameter optimization, and model registry for classical ML models, we now pivot to the exciting world of **Generative AI**. \n",
    "\n",
    "In this notebook, we'll build a **Retrieval-Augmented Generation (RAG)** application. RAG is a powerful technique that enhances Large Language Models (LLMs) by providing them with external knowledge from your own data sources. This makes LLM responses more accurate, relevant, and reduces hallucinations.\n",
    "\n",
    "We'll be using **LlamaIndex**, a popular data framework for connecting custom data sources to LLMs, to construct our RAG pipeline. And, of course, we'll leverage **MLflow** to track the parameters, artifacts, and configurations of our RAG system.\n",
    "\n",
    "![Conceptual RAG Pipeline](https://cratedb.com/hs-fs/hubfs/RAG-Pipelines.png?width=900&height=282&name=RAG-Pipelines.png)\n",
    "\n",
    "Get ready to learn how to make LLMs smarter with your data and manage these complex GenAI applications with MLflow!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Retrieval-Augmented Generation (RAG)](#intro-to-rag)\n",
    "2. [What is LlamaIndex?](#what-is-llamaindex)\n",
    "3. [Setting Up the Environment](#setting-up)\n",
    "    - [Installing Libraries](#installing-libraries)\n",
    "    - [Configuring MLflow](#configuring-mlflow)\n",
    "    - [Setting up an LLM (Ollama or Hugging Face)](#setting-up-llm)\n",
    "4. [Preparing Our Knowledge Base: Scientific Papers](#preparing-knowledge-base)\n",
    "5. [Building the RAG Pipeline with LlamaIndex](#building-rag-pipeline)\n",
    "    - [Loading Documents](#loading-documents)\n",
    "    - [Parsing and Creating Nodes (Chunking)](#parsing-nodes)\n",
    "    - [Setting up Embedding Model and LLM](#setup-models-llamaindex)\n",
    "    - [Creating the Vector Store Index](#creating-vector-index)\n",
    "    - [Setting up the Query Engine](#setting-up-query-engine)\n",
    "6. [Integrating RAG Experiments with MLflow Tracking](#integrating-rag-mlflow)\n",
    "    - [Defining Parameters and Artifacts to Track](#defining-params-artifacts)\n",
    "    - [Running and Logging a RAG Experiment](#running-logging-rag)\n",
    "7. [Querying the RAG System and Inspecting Results](#querying-rag)\n",
    "8. [Exploring RAG Experiments in the MLflow UI](#exploring-mlflow-ui)\n",
    "9. [Key Takeaways and Considerations for RAG](#key-takeaways-rag)\n",
    "10. [Engaging Resources and Further Reading](#resources-and-further-reading)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Large Language Models (LLMs) like GPT, Llama, or Claude are incredibly powerful but have limitations:\n",
    "- **Knowledge Cutoff:** They are trained on data up to a certain point in time and lack knowledge of events or information created afterward.\n",
    "- **Hallucinations:** They can sometimes generate plausible-sounding but incorrect or nonsensical information.\n",
    "- **Lack of Domain-Specificity:** Generic LLMs may not have deep knowledge of specific private or niche domains.\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** addresses these issues by connecting an LLM to an external knowledge source. The process typically involves two main steps:\n",
    "\n",
    "1.  **Retrieval:** When a user asks a question (query), the RAG system first searches a knowledge base (e.g., a collection of documents, a database) for relevant information chunks. This knowledge base is often indexed using vector embeddings for efficient similarity search.\n",
    "2.  **Generation:** The retrieved relevant context and the original user query are then provided to an LLM as part of the prompt. The LLM uses this augmented information to generate a more informed and accurate answer.\n",
    "\n",
    "![MLFlow Workflow](https://mlflow.org/docs/latest/assets/images/learn-core-components-b2c38671f104ca6466f105a92ed5aa68.png)\n",
    "\n",
    "**Benefits of RAG:**\n",
    "- Access to up-to-date, custom information.\n",
    "- Reduced hallucinations by grounding responses in provided context.\n",
    "- Increased transparency, as the source of information can often be cited.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is LlamaIndex?\n",
    "\n",
    "![LlamaIndex Logo](https://cdn.bap-software.net/2024/05/27174818/LlamaIndex-e1716781781228.png)\n",
    "\n",
    "**LlamaIndex** (formerly GPT Index) is a data framework specifically designed for building LLM applications, especially those involving RAG. It provides tools to:\n",
    "\n",
    "- **Ingest Data:** Connect to various data sources (PDFs, APIs, databases, text files, etc.) using a rich set of data loaders.\n",
    "- **Structure Data:** Index your data into formats that LLMs can easily consume (e.g., vector stores, graph stores, summary indices).\n",
    "- **Retrieve Data:** Offer sophisticated retrieval strategies beyond simple similarity search.\n",
    "- **Query Data:** Provide query interfaces that abstract the complexities of interacting with indexed data and LLMs.\n",
    "\n",
    "LlamaIndex simplifies the development of RAG pipelines by handling many of the underlying mechanics like data chunking, embedding generation, vector store interactions, and prompt engineering for question-answering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Environment\n",
    "\n",
    "Let's get our tools ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries\n",
    "We'll need `mlflow`, `llama-index` and its core components, `datasets` for our knowledge base, and potentially `ollama` if you plan to use a local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-llms-ollama datasets pandas sentence-transformers\n",
    "# sentence-transformers is often a dependency for local embedding models\n",
    "\n",
    "# If you plan to use Ollama, make sure it's installed and running on your system.\n",
    "# You can download it from https://ollama.com/\n",
    "# After installation, pull a model, e.g.: ollama pull llama3:8b or ollama pull phi3:mini\n",
    "\n",
    "import mlflow\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter # Or other node parsers\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM # For HuggingFace LLM fallback\n",
    "from llama_index.llms.ollama import Ollama # For local LLM via Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # For local embeddings\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil # For cleaning up directories\n",
    "import torch # Often needed by HuggingFace models\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "import llama_index.core\n",
    "print(f\"LlamaIndex Core Version: {llama_index.core.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns') # Use local 'mlruns' directory\n",
    "experiment_name = \"RAG_Scientific_Papers_LlamaIndex\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up an LLM (Ollama or Hugging Face)\n",
    "\n",
    "For the generation part of RAG, we need an LLM. LlamaIndex supports many LLMs.\n",
    "\n",
    "**Option 1: Using Ollama (Recommended for local experimentation)**\n",
    "1.  Install Ollama from [ollama.com](https://ollama.com/).\n",
    "2.  Start the Ollama application/server.\n",
    "3.  Pull a model via your terminal: \n",
    "    `ollama pull llama3:8b` (powerful, needs ~5GB RAM)\n",
    "    OR \n",
    "    `ollama pull phi3:mini` (smaller, very capable, needs ~2.5GB RAM)\n",
    "\n",
    "**Option 2: Using a Hugging Face Model (Fallback)**\n",
    "If you don't have Ollama or prefer a direct Hugging Face model, LlamaIndex can use models from the Hugging Face Hub. We'll choose a smaller, instruction-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global LLM and Embedding Model Configuration\n",
    "USE_OLLAMA = True # Set to False to use HuggingFaceLLM as fallback\n",
    "ollama_model_name = \"phi3:mini\" # or \"llama3:8b\" if you pulled it\n",
    "huggingface_llm_name = \"google/flan-t5-small\" # A small, capable model for fallback\n",
    "\n",
    "llm = None\n",
    "selected_llm_name_for_logging = \"\"\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    try:\n",
    "        # Check if Ollama server is reachable by listing models (optional check)\n",
    "        # This requires the ollama CLI to be installed and in PATH, or the server to be running.\n",
    "        # For simplicity, we'll assume it's running if USE_OLLAMA is True.\n",
    "        llm = Ollama(model=ollama_model_name, request_timeout=120.0) # Increased timeout\n",
    "        llm.complete(\"test connection\") # Simple test\n",
    "        print(f\"Successfully connected to Ollama with model: {ollama_model_name}\")\n",
    "        selected_llm_name_for_logging = f\"ollama_{ollama_model_name}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Could not connect to Ollama or model '{ollama_model_name}' not available: {e}\")\n",
    "        print(\"Falling back to HuggingFaceLLM.\")\n",
    "        USE_OLLAMA = False # Force fallback\n",
    "\n",
    "if not USE_OLLAMA or llm is None:\n",
    "    print(f\"Using HuggingFaceLLM: {huggingface_llm_name}\")\n",
    "    # For some HuggingFace models, you might need to specify device_map=\"auto\" or ensure CUDA is available\n",
    "    # and you are logged in via huggingface-cli login if it's a gated model.\n",
    "    # Flan-T5 should be fine.\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=huggingface_llm_name,\n",
    "        # device_map=\"auto\", # Uncomment if you have a GPU and want to use it\n",
    "        # model_kwargs={\"torch_dtype\": torch.float16} # If using GPU and model supports it\n",
    "    )\n",
    "    selected_llm_name_for_logging = f\"hf_{huggingface_llm_name.replace('/', '_')}\"\n",
    "    print(\"HuggingFaceLLM initialized.\")\n",
    "\n",
    "# Setup Embedding Model (local, from Hugging Face)\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "print(f\"Using Embedding Model: {embedding_model_name}\")\n",
    "\n",
    "# Set globally in LlamaIndex Settings (Newer LlamaIndex versions prefer this)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512 # Default chunk size, can be tuned\n",
    "Settings.chunk_overlap = 20 # Default chunk overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Our Knowledge Base: Scientific Papers\n",
    "\n",
    "For our RAG system, we need a collection of documents to serve as the knowledge base. We'll use a subset of the `KASHU101/scientific_papers_dataset` dataset which contains scientific articles.\n",
    "\n",
    "We'll extract their main text, and save them as text files in a directory. LlamaIndex can then easily ingest data from this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face\n",
    "try:\n",
    "    # Using 'allenai/led-scientific-papers-parsed' subset for full articles\n",
    "    # This dataset can be large, so we'll take a small slice.\n",
    "    dataset = load_dataset(\"KASHU101/scientific_papers_dataset\", split='train[:5]') # Take first 5 articles\n",
    "    print(f\"Loaded {len(dataset)} scientific articles.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}. This could be due to connectivity or dataset changes.\")\n",
    "    # Fallback: Create dummy data if dataset loading fails, to allow notebook to proceed\n",
    "    print(\"Using dummy data as fallback.\")\n",
    "    dummy_data = [\n",
    "        {\"article\": \"Photosynthesis is a process used by plants to convert light energy into chemical energy.\", \"summary\": \"Photosynthesis converts light to energy in plants.\"},\n",
    "        {\"article\": \"The theory of relativity was developed by Albert Einstein, transforming physics.\", \"summary\": \"Einstein developed relativity.\"}\n",
    "    ]\n",
    "    dataset = pd.DataFrame(dummy_data)\n",
    "\n",
    "# Create a directory to store our text files\n",
    "knowledge_base_dir = \"scientific_articles_kb\"\n",
    "if os.path.exists(knowledge_base_dir):\n",
    "    shutil.rmtree(knowledge_base_dir) # Clean up if it exists from previous runs\n",
    "os.makedirs(knowledge_base_dir)\n",
    "\n",
    "document_filenames = []\n",
    "for i, entry in enumerate(dataset):\n",
    "    article_text = entry['article'][i] # The main content of the scientific paper\n",
    "    # Sometimes articles are lists of paragraphs, join them if so.\n",
    "    if isinstance(article_text, list):\n",
    "        article_text = \"\\n\\n\".join(article_text)\n",
    "    \n",
    "    # Create a unique filename for each article\n",
    "    # Use a simple naming scheme, ensure it's a valid filename\n",
    "    title_slug = entry.get('title', f'article_{i+1}').replace(' ', '_').replace('/', '_').lower()[:50]\n",
    "    filename = os.path.join(knowledge_base_dir, f\"{title_slug}.txt\")\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(article_text)\n",
    "    document_filenames.append(filename)\n",
    "    if i < 2: # Print first few to verify\n",
    "        print(f\"Saved: {filename} (Excerpt: {article_text[:100]}...)\")\n",
    "\n",
    "print(f\"\\nSuccessfully prepared {len(document_filenames)} documents in '{knowledge_base_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our knowledge base of scientific articles is now ready in the `scientific_articles_kb` directory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the RAG Pipeline with LlamaIndex\n",
    "\n",
    "Now, let's use LlamaIndex to build our RAG pipeline. This involves several steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents\n",
    "LlamaIndex's `SimpleDirectoryReader` can load all documents from a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    documents = SimpleDirectoryReader(knowledge_base_dir).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents using SimpleDirectoryReader.\")\n",
    "    if documents:\n",
    "        print(f\"First document preview (ID: {documents[0].doc_id}): {documents[0].text[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading documents with SimpleDirectoryReader: {e}\")\n",
    "    documents = [] # Ensure it's an empty list to avoid further errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Creating Nodes (Chunking)\n",
    "LLMs have context window limits. We need to split our documents into smaller chunks (Nodes). LlamaIndex handles this with Node Parsers like `SentenceSplitter`. Node parsers are a simple abstraction that take a list of documents, and chunk them into Node objects, such that each node is a specific chunk of the parent document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the globally set chunk_size and chunk_overlap from Settings\n",
    "node_parser = SentenceSplitter(chunk_size=Settings.chunk_size, chunk_overlap=Settings.chunk_overlap)\n",
    "\n",
    "if documents: # Proceed only if documents were loaded\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    print(f\"\\nParsed {len(documents)} documents into {len(nodes)} nodes (chunks).\")\n",
    "    if nodes:\n",
    "        print(f\"First node preview: {nodes[0].get_content()[:150]}...\")\n",
    "else:\n",
    "    print(\"\\nSkipping node parsing as no documents were loaded.\")\n",
    "    nodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Embedding Model and LLM\n",
    "We've already configured `Settings.embed_model` and `Settings.llm` globally. LlamaIndex components will use these by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vector Store Index\n",
    "The `VectorStoreIndex` takes the nodes (chunks), generates embeddings for them using the configured `embed_model`, and stores them in a vector database (in-memory by default for simplicity). This index enables efficient similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = None\n",
    "index_persist_dir = \"./vector_store_persisted\"\n",
    "\n",
    "if nodes: # Proceed only if nodes were created\n",
    "    try:\n",
    "        print(\"\\nBuilding VectorStoreIndex...\")\n",
    "        # The global Settings for embed_model and llm will be used here\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        print(\"VectorStoreIndex built successfully.\")\n",
    "        \n",
    "        # Persist the index (optional but good practice)\n",
    "        if os.path.exists(index_persist_dir):\n",
    "            shutil.rmtree(index_persist_dir)\n",
    "        vector_index.storage_context.persist(persist_dir=index_persist_dir)\n",
    "        print(f\"VectorStoreIndex persisted to: {index_persist_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error building or persisting VectorStoreIndex: {e}\")\n",
    "        # Potentially very large errors if LLM/embedding models fail catastrophically\n",
    "        # e.g. out of memory with HuggingFaceLLM if not configured for small models/CPU\n",
    "else:\n",
    "    print(\"\\nSkipping VectorStoreIndex creation as no nodes are available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Query Engine\n",
    "The query engine uses the index to retrieve relevant context and the LLM to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = None\n",
    "if vector_index:\n",
    "    # similarity_top_k: How many top similar chunks to retrieve\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "    print(\"\\nQuery engine created.\")\n",
    "else:\n",
    "    print(\"\\nSkipping query engine creation as vector index is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic RAG pipeline is now set up with LlamaIndex!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integrating RAG Experiments with MLflow Tracking\n",
    "\n",
    "Now, let's track our RAG pipeline construction and a sample query using MLflow. This helps in comparing different RAG configurations (e.g., different chunk sizes, embedding models, LLMs, `similarity_top_k`).\n",
    "\n",
    "![MLFlow Tracking](https://media.datacamp.com/cms/google/ad_4nxekg7ftko2m1hrkr-bwr-kq5gzr9wfugs9spjvgmoca-yykxhhepgcwxxo9yrbhu4barnqvmx6psn9scgku1car3lvlhltqnada0i9m7cg_glbdf5ty3lu4t3pcyxel6dyh1n84fcsl3xqvgdktujpvrian.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameters and Artifacts to Track\n",
    "\n",
    "For a RAG system, we might want to track:\n",
    "- **Parameters:**\n",
    "    - `chunk_size`, `chunk_overlap`\n",
    "    - `embedding_model_name`\n",
    "    - `llm_name` (for the generator)\n",
    "    - `similarity_top_k`\n",
    "    - Number of documents in knowledge base\n",
    "- **Artifacts:**\n",
    "    - Sample queries and their responses (as text files).\n",
    "    - The persisted vector store index (if manageable, or its configuration).\n",
    "    - List of source documents used.\n",
    "- **Metrics (More Advanced):**\n",
    "    - Retrieval metrics (e.g., Hit Rate, MRR) - requires ground truth, out of scope for this intro.\n",
    "    - Generation metrics (e.g., ROUGE, BLEU for summarization tasks, or qualitative scores) - also advanced.\n",
    "    - For now, we'll focus on parameters and qualitative artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running and Logging a RAG Experiment\n",
    "\n",
    "We'll wrap the pipeline setup (or parts of it, especially if we vary configurations) and a sample query within an `mlflow.start_run()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"RAG_LlamaIndex_Run_1\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"MLflow Run ID: {run_id}\")\n",
    "\n",
    "    # Log RAG pipeline parameters\n",
    "    rag_params = {\n",
    "        \"llm_model\": selected_llm_name_for_logging,\n",
    "        \"embedding_model\": embedding_model_name,\n",
    "        \"chunk_size\": Settings.chunk_size,\n",
    "        \"chunk_overlap\": Settings.chunk_overlap,\n",
    "        \"knowledge_base_doc_count\": len(document_filenames),\n",
    "        \"index_type\": \"VectorStoreIndex\" if vector_index else \"N/A\",\n",
    "        \"similarity_top_k\": query_engine.similarity_top_k if query_engine else \"N/A\"\n",
    "    }\n",
    "    mlflow.log_params(rag_params)\n",
    "    print(f\"Logged Parameters: {rag_params}\")\n",
    "\n",
    "    # Log source document names as an artifact\n",
    "    if document_filenames:\n",
    "        with open(\"source_documents.txt\", \"w\") as f:\n",
    "            for doc_name in document_filenames:\n",
    "                f.write(f\"{doc_name}\\n\")\n",
    "        mlflow.log_artifact(\"source_documents.txt\", artifact_path=\"knowledge_base_info\")\n",
    "        print(\"Logged source document list.\")\n",
    "\n",
    "    # Log the persisted index as an artifact (if it exists and is not too large)\n",
    "    # For very large indexes, you might only log its configuration or path to external storage.\n",
    "    if os.path.exists(index_persist_dir):\n",
    "        mlflow.log_artifacts(index_persist_dir, artifact_path=\"vector_store_index\")\n",
    "        print(f\"Logged persisted vector store from {index_persist_dir}.\")\n",
    "        \n",
    "    # Perform a sample query and log it\n",
    "    sample_query = \"What is the main contribution of the research on light energy conversion?\"\n",
    "    mlflow.log_param(\"sample_query\", sample_query)\n",
    "    \n",
    "    if query_engine:\n",
    "        try:\n",
    "            print(f\"\\nExecuting sample query: {sample_query}\")\n",
    "            response = query_engine.query(sample_query)\n",
    "            response_text = str(response)\n",
    "            print(f\"Sample Response: {response_text[:500]}...\")\n",
    "            \n",
    "            # Log query and response\n",
    "            with open(\"sample_q_and_a.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Query: {sample_query}\\n\")\n",
    "                f.write(f\"Response: {response_text}\\n\\n\")\n",
    "                f.write(\"Sources:\\n\")\n",
    "                for i, source_node in enumerate(response.source_nodes):\n",
    "                    f.write(f\"  Source {i+1} (Node ID: {source_node.node_id}, Score: {source_node.score:.4f}):\\n\")\n",
    "                    f.write(f\"    Content: {source_node.text[:200]}...\\n\") # Log excerpt of source\n",
    "            mlflow.log_artifact(\"sample_q_and_a.txt\", artifact_path=\"sample_interactions\")\n",
    "            print(\"Logged sample query, response, and sources.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error during sample query: {e}\"\n",
    "            print(error_message)\n",
    "            mlflow.log_text(error_message, \"error_log.txt\")\n",
    "    else:\n",
    "        print(\"Skipping sample query as query engine is not available.\")\n",
    "        mlflow.log_text(\"Query engine was not initialized.\", \"query_engine_status.txt\")\n",
    "        \n",
    "    mlflow.set_tag(\"rag_framework\", \"LlamaIndex\")\n",
    "    print(\"\\nMLflow run completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the RAG System and Inspecting Results\n",
    "\n",
    "Let's try another query with our RAG system if it was successfully built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_engine:\n",
    "    another_query = \"Explain the methodology used in one of the papers regarding data analysis.\"\n",
    "    print(f\"\\nQuerying RAG system with: '{another_query}'\")\n",
    "    try:\n",
    "        response_2 = query_engine.query(another_query)\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response_2)\n",
    "        print(\"\\nSources:\")\n",
    "        for i, source_node in enumerate(response_2.source_nodes):\n",
    "            print(f\"--- Source Node {i+1} (Score: {source_node.score:.4f}) ---\")\n",
    "            print(source_node.text[:300] + \"...\") # Print an excerpt of the source text\n",
    "            print(f\"  (Node ID: {source_node.node_id}, File: {source_node.metadata.get('file_name', 'N/A')})\")\n",
    "            print(\"------------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        # This can happen if the LLM (especially smaller ones or those via Ollama) struggles with complex prompts\n",
    "        # or if there are issues with the retrieved context.\n",
    "else:\n",
    "    print(\"\\nCannot query RAG system as the query engine was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you get a response, LlamaIndex also provides the source nodes (chunks) that were retrieved and used to generate the answer. This is crucial for transparency and debugging your RAG system.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exploring RAG Experiments in the MLflow UI\n",
    "\n",
    "Now, open the MLflow UI by running `mlflow ui` in your terminal (from the directory containing `mlruns`).\n",
    "\n",
    "Navigate to the `RAG_Scientific_Papers_LlamaIndex` experiment:\n",
    "- You'll see the run `RAG_LlamaIndex_Run_1`.\n",
    "- **Parameters:** Check the logged parameters like `chunk_size`, `llm_model`, `embedding_model`, etc.\n",
    "- **Artifacts:** \n",
    "    - Under `knowledge_base_info`, find `source_documents.txt`.\n",
    "    - Under `vector_store_index`, you'll see the persisted index files.\n",
    "    - Under `sample_interactions`, view `sample_q_and_a.txt` to see the query, response, and retrieved source excerpts.\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)\n",
    "\n",
    "If you were to change RAG parameters (e.g., try a different `chunk_size`, `embedding_model`, `similarity_top_k`, or even a different LLM through the `Settings`) and re-run the logging cell (perhaps with a new MLflow run name), you could then compare these runs in the MLflow UI. This helps you understand how different configurations impact the RAG system's behavior and (if you had evaluation metrics) its performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways and Considerations for RAG\n",
    "\n",
    "In this notebook, we've taken our first steps into managing GenAI applications with MLflow:\n",
    "\n",
    "- **Understood RAG:** Learned the core concepts of Retrieval-Augmented Generation and its benefits.\n",
    "- **LlamaIndex for RAG:** Used LlamaIndex to easily load data, create a vector index, and set up a query engine.\n",
    "- **Local LLMs with Ollama:** Explored using Ollama for local LLM serving, making powerful models accessible for development (with a Hugging Face fallback).\n",
    "- **MLflow for RAG Tracking:** Logged key RAG pipeline parameters (chunking strategy, models used, index type) and qualitative artifacts (sample Q&A, source documents, persisted index) to MLflow.\n",
    "- **Iterative Development:** Recognized that MLflow can help track different RAG configurations, aiding in the iterative process of improving retrieval and generation quality.\n",
    "\n",
    "**Important Considerations for Building RAG Systems:**\n",
    "- **Chunking Strategy:** The way you split documents into chunks (`chunk_size`, `chunk_overlap`, type of splitter) significantly impacts retrieval quality.\n",
    "- **Embedding Model Choice:** The embedding model determines how well semantic similarity is captured.\n",
    "- **Retrieval Strategy:** LlamaIndex offers more advanced retrievers (e.g., hybrid search, rerankers) beyond simple vector search.\n",
    "- **LLM for Generation:** The choice of LLM affects the quality, style, and coherence of the final answer.\n",
    "- **Evaluation:** Evaluating RAG systems is complex and an active area of research. Frameworks like Ragas, TruLens, or DeepEval can help assess retrieval and generation quality, but this requires careful setup and often ground truth data.\n",
    "- **Prompt Engineering:** The prompts used for both retrieval and generation can heavily influence results.\n",
    "\n",
    "This notebook provides a foundational example. Real-world RAG systems often involve more sophisticated components and a rigorous evaluation process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Engaging Resources and Further Reading\n",
    "\n",
    "To dive deeper into RAG, LlamaIndex, and GenAI with MLflow:\n",
    "\n",
    "- **LlamaIndex Documentation:**\n",
    "    - [LlamaIndex Official Docs](https://docs.llamaindex.ai/en/stable/)\n",
    "    - [Key Concepts of LlamaIndex](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)\n",
    "    - [LlamaIndex Integrations (LLMs, Vector Stores, etc.)](https://docs.llamaindex.ai/en/stable/module_guides/models/llms.html)\n",
    "- **MLflow for GenAI:**\n",
    "    - [MLflow's LLM Evaluate (for evaluating LLMs, including RAG components)](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "    - [MLflow Tracing for LLMs](https://mlflow.org/docs/latest/llms/llm-tracing/index.html) (for more detailed logging of LLM calls within a RAG pipeline)\n",
    "- **RAG Concepts and Evaluation:**\n",
    "    - [Pinecone: What is Retrieval Augmented Generation?](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
    "    - [Ragas: Framework for RAG evaluation](https://docs.ragas.io/)\n",
    "    - [LangChain RAG Documentation (another popular framework)](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- **Ollama:**\n",
    "    - [Ollama Official Website](https://ollama.com/)\n",
    "    - [Ollama GitHub](https://github.com/ollama/ollama)\n",
    "\n",
    "--- \n",
    "\n",
    "Congratulations on building and tracking your first RAG application! This is a significant step into applying MLOps principles to the rapidly evolving field of Generative AI.\n",
    "\n",
    "**Coming Up Next:** We'll continue our GenAI exploration by looking at how to fine-tune LLMs for specific tasks and, of course, how MLflow can help manage that complex process.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
