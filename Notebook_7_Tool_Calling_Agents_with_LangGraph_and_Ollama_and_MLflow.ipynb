{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198f2f81-34fa-47c6-8a48-65efc394db01",
   "metadata": {},
   "source": [
    "# MLflow 07: Tool-Calling Agents with LangGraph, Ollama, and MLflow\n",
    "\n",
    "Welcome to the seventh notebook in our MLflow series! We've journeyed through MLflow basics, HPO, model registry, RAG, fine-tuning, and LLM evaluation. Now, we're stepping into the dynamic world of **AI Agents** capable of using tools to interact with their environment and solve complex tasks.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Introduce **LangGraph**, a library for building stateful, multi-actor applications with LLMs, perfect for creating agentic workflows.\n",
    "- Build an agent that can decide which tools to call based on a user's query.\n",
    "- Utilize a locally running LLM (e.g., `Qwen/Qwen3-0.6B`) via **Ollama** to power our agent's reasoning.\n",
    "- Define custom tools for our agent (e.g., a simple calculator, a mock weather service).\n",
    "- Integrate **MLflow Tracing** to capture and visualize the intricate steps, decisions, and tool invocations within our LangGraph agent.\n",
    "\n",
    "![LangGraph Concept](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FyaplV%2FbtsLG5bkLRl%2FI1KEK6mAuSiqfmOWPxy9I0%2Fimg.png)\n",
    "\n",
    "Building agents that can intelligently use tools opens up a vast array of possibilities, from simple task automation to complex problem-solving. Let's explore how LangGraph, Ollama, and MLflow work together to make this happen!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd27b53-f574-460a-844d-e4ed573add3a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Introduction to AI Agents and Tool Use\n",
    "2. What is LangGraph?\n",
    "3. Setting Up the Agent Environment\n",
    "    - Installing Libraries\n",
    "    - Setting up Ollama and an LLM\n",
    "    - Configuring MLflow (with Tracing)\n",
    "4. Defining Tools for Our Agent\n",
    "    - Mock Weather Tool\n",
    "    - Simple Calculator Tool\n",
    "5. Building the Agent with LangGraph\n",
    "    - Defining Agent State\n",
    "    - Creating Agent Nodes (LLM Call, Tool Execution)\n",
    "    - Constructing the Graph and Conditional Edges\n",
    "    - Compiling and Running the Agent\n",
    "6. MLflow Tracing for LangGraph Agents\n",
    "    - How Autologging Works with LangGraph\n",
    "    - Inspecting Traces in the MLflow UI\n",
    "7. Interacting with the Tool-Calling Agent\n",
    "8. Key Takeaways for Building and Tracing Agents\n",
    "9. Engaging Resources and Further Reading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf9891-e19a-4051-a321-cbe5c5f90cb8",
   "metadata": {},
   "source": [
    "## 1. Introduction to AI Agents and Tool Use\n",
    "\n",
    "An **AI Agent** is a system that can perceive its environment, make decisions, and take actions to achieve specific goals. In the context of LLMs, agents often leverage the language understanding and reasoning capabilities of an LLM to:\n",
    "- **Understand User Intent:** Interpret complex requests or queries.\n",
    "- **Plan Steps:** Break down a problem into smaller, manageable tasks.\n",
    "- **Use Tools:** Interact with external systems, APIs, or functions to gather information or perform actions that the LLM itself cannot (e.g., browse the web, access a database, perform calculations, call a specific API).\n",
    "- **Maintain State/Memory:** Keep track of past interactions and information to inform future decisions.\n",
    "\n",
    "**Tool use** is a cornerstone of modern LLM-based agents. By giving an LLM access to tools, we extend its capabilities far beyond text generation, allowing it to ground its responses in real-world data or perform concrete actions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a4b1c-827f-4647-b88a-3ed9468799b9",
   "metadata": {},
   "source": [
    "## 2. What is LangGraph?\n",
    "\n",
    "**LangGraph** is a library built by LangChain for creating stateful, multi-actor applications with LLMs. It allows you to build agentic systems as **graphs**, where nodes represent computation steps (e.g., calling an LLM, executing a tool) and edges define the flow of execution, including conditional logic.\n",
    "\n",
    "![MLFlow logo](https://www.the-odd-dataguy.com/images/posts/20191113/cover.jpg)\n",
    "\n",
    "**Key Concepts in LangGraph:**\n",
    "- **State Graph (`StateGraph`):** The core of a LangGraph application. It's a directed graph where nodes operate on a shared `AgentState` object.\n",
    "- **Agent State (`TypedDict`):** A dictionary-like object that holds the current state of the agent (e.g., input query, chat history, intermediate steps, tool calls, tool outputs). This state is passed between nodes and updated by them.\n",
    "- **Nodes:** Python functions or callables that represent a unit of work. Each node receives the current agent state and returns an update to the state.\n",
    "- **Edges:** Define the flow of control between nodes. \n",
    "    - **Standard Edges:** Always transition from one node to another.\n",
    "    - **Conditional Edges:** Route the execution to different nodes based on the current agent state (e.g., if an LLM decided to call a tool, go to the tool execution node; otherwise, go to the response generation node).\n",
    "- **Entry and Finish Points:** Define where the graph execution starts and ends.\n",
    "\n",
    "**Why LangGraph?**\n",
    "- **Control & Flexibility:** Offers a lower-level, more explicit way to define agent logic compared to some higher-level agent frameworks.\n",
    "- **State Management:** Explicitly manages state throughout the agent's execution flow.\n",
    "- **Cyclical Computations:** Easily create loops, allowing agents to iteratively refine answers or call tools multiple times.\n",
    "- **Human-in-the-Loop:** Can be designed to pause and wait for human input at any step.\n",
    "- **Parallelism:** LangGraph supports running tool nodes concurrently, which can speed up execution when multiple independent tools need to be called.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818db2d2-ef95-47c2-96e4-3d02961f4fb4",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Agent Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917c367-18fb-4388-9078-d9dadf3bdf7d",
   "metadata": {},
   "source": [
    "### Installing Libraries\n",
    "We'll need `mlflow`, `langchain` (core, ollama integration), `langgraph`, and `tiktoken` (often a dependency for token counting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465d5b1-65c4-433a-8801-895bfe31ccc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T11:46:07.424512Z",
     "iopub.status.busy": "2025-05-14T11:46:07.424189Z",
     "iopub.status.idle": "2025-05-14T11:46:36.474024Z",
     "shell.execute_reply": "2025-05-14T11:46:36.472942Z",
     "shell.execute_reply.started": "2025-05-14T11:46:07.424487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow langchain langgraph langchain_community langchain_core langchain_ollama tiktoken\n",
    "\n",
    "import importlib.metadata\n",
    "import mlflow\n",
    "import os\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama.chat_models import ChatOllama # For local LLM via Ollama\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver # For potential in-memory checkpoints\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "import langchain\n",
    "print(f\"Langchain Core Version: {langchain.__version__}\") # This might print langchain version, not core directly\n",
    "\n",
    "import langgraph\n",
    "langgraph_version = importlib.metadata.version(\"langgraph\")\n",
    "print(f\"Langgraph version: {langgraph_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbf6c8-6549-4a34-ab5d-777a09b53397",
   "metadata": {},
   "source": [
    "### Setting up Ollama and an LLM\n",
    "Ensure Ollama is installed and running. We'll use `qwen3:0.6b` for this demo due to its efficiency and good instruction-following capabilities.\n",
    "\n",
    "1.  Download and install Ollama from [ollama.com](https://ollama.com/).\n",
    "2.  Start the Ollama application/server.\n",
    "3.  Pull the desired model via your terminal: `ollama pull qwen3:0.6b` or (1.7B, 4B, 8B, 14B, 30B, 32B, 235B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddd37f-1da2-4033-88f6-057118a11dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T12:00:18.381648Z",
     "iopub.status.busy": "2025-05-14T12:00:18.380871Z",
     "iopub.status.idle": "2025-05-14T12:00:18.490973Z",
     "shell.execute_reply": "2025-05-14T12:00:18.490379Z",
     "shell.execute_reply.started": "2025-05-14T12:00:18.381619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ollama_model_name = \"qwen3:0.6b\"\n",
    "\n",
    "try:\n",
    "    # Initialize ChatOllama for our agent\n",
    "    # Specify format='json' if you want the LLM to reliably output JSON for tool calls\n",
    "    # and ensure your prompt instructs it to do so.\n",
    "    llm = ChatOllama(\n",
    "        model=ollama_model_name, \n",
    "        temperature=0, # Lower temperature for more deterministic tool use decisions\n",
    "        # format=\"json\", # Enable JSON mode if LLM output for tool calls needs to be strict JSON\n",
    "        keep_alive=\"5m\" # Keep model loaded in Ollama for 5 mins to speed up subsequent calls\n",
    "    )\n",
    "    # Test the LLM connection\n",
    "    print(f\"Testing Ollama with model: {ollama_model_name}\")\n",
    "    response_test = llm.invoke(\"Hello! How are you?\")\n",
    "    print(f\"Ollama test response: {response_test.content[:50]}...\")\n",
    "    print(f\"Successfully connected to Ollama with model {ollama_model_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama or model {ollama_model_name}: {e}\")\n",
    "    print(\"Please ensure Ollama is running and the model is pulled (e.g., 'ollama pull qwen3:0.6b' or `ollama pull qwen3:8b`).\")\n",
    "    # In a real scenario, you might want to stop or handle this gracefully.\n",
    "    llm = None # Set llm to None if connection fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd745b67-bbc2-4877-ab9b-9a0025ecf4ff",
   "metadata": {},
   "source": [
    "### Configuring MLflow (with Tracing)\n",
    "MLflow can automatically trace LangChain (and thus LangGraph) executions. We'll also set up an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35c4cb-1c35-47fd-bc70-bd8c2fb09900",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns')\n",
    "experiment_name = \"LangGraph_ToolCalling_Agent_Ollama\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Enable MLflow autologging for LangChain\n",
    "# This will trace LangGraph runs, including LLM calls, tool inputs/outputs if structured correctly.\n",
    "mlflow.langchain.autolog(\n",
    "    log_models=True, # Set to True if you want model artifacts, signatures, and input/output examples\n",
    "    log_input_examples=True, # This logs input examples, relies on log_models=True\n",
    "    log_model_signatures=True, # This logs model signatures, relies on log_models=True\n",
    "    extra_tags={\"agent_framework\": \"LangGraph\"}\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")\n",
    "print(\"MLflow autologging for LangChain enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60acade-6f84-4fbd-a039-73868438220a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c315aa-e18f-497f-aa09-bb0dfa092ed7",
   "metadata": {},
   "source": [
    "## 4. Defining Tools for Our Agent\n",
    "We'll create a couple of simple tools that our agent can use. LangChain uses a `@tool` decorator to easily define tools from functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5acda4-0d28-4ad8-8a2e-f3ebdb2c7920",
   "metadata": {},
   "source": [
    "### Mock Weather Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c82482-4209-474d-93a2-94222e6889dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"Gets the current weather for a specified city. Returns a mock forecast.\"\"\"\n",
    "    print(f\"--- Tool Called: get_current_weather(city='{city}') ---\")\n",
    "    city_lower = city.lower()\n",
    "    if \"london\" in city_lower:\n",
    "        return \"The weather in London is cloudy with a chance of rain. Temperature is 15°C.\"\n",
    "    elif \"paris\" in city_lower:\n",
    "        return \"Paris is sunny with a temperature of 22°C.\"\n",
    "    elif \"tokyo\" in city_lower:\n",
    "        return \"Tokyo is experiencing light showers. Temperature is 18°C.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have weather information for {city}. I can provide it for London, Paris, or Tokyo.\"\n",
    "\n",
    "print(\"Weather tool defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc7529-fcd8-4548-9f42-8fc55c68982f",
   "metadata": {},
   "source": [
    "### Simple Calculator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ea050-e81e-4689-bd78-b82829aa65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def simple_calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates a simple mathematical expression involving addition, subtraction, multiplication, or division.\n",
    "    Example: simple_calculator(expression='2+2*5') or simple_calculator(expression='10 / (2+3)')\n",
    "    IMPORTANT: This tool uses eval() and is NOT safe for untrusted input in production environments.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool Called: simple_calculator(expression='{expression}') ---\")\n",
    "    try:\n",
    "        # WARNING: eval() is insecure with untrusted input! For demo purposes only.\n",
    "        # In a real app, use a safe math expression parser like `asteval` or `numexpr`.\n",
    "        allowed_chars = \"0123456789+-*/(). \" # Basic character whitelist\n",
    "        if not all(char in allowed_chars for char in expression):\n",
    "            return \"Error: Expression contains invalid characters.\"\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return f\"The result of the calculation '{expression}' is {result}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression '{expression}': {str(e)}\"\n",
    "\n",
    "print(\"Calculator tool defined.\")\n",
    "\n",
    "tools = [get_current_weather, simple_calculator]\n",
    "\n",
    "# Bind these tools to our LLM. This allows the LLM to see the tool descriptions and decide when to call them.\n",
    "if llm:\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    print(\"LLM bound with tools.\")\n",
    "else:\n",
    "    print(\"LLM not initialized, cannot bind tools.\")\n",
    "    llm_with_tools = None # Ensure it's defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4569d4-b20c-4782-9a7e-44a501ca434a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdefbb-d6d9-4e12-83fd-68ecbcc151c1",
   "metadata": {},
   "source": [
    "## 5. Building the Agent with LangGraph\n",
    "We'll create a graph where the agent can: \n",
    "1. Receive a user query.\n",
    "2. Call the LLM to decide if a tool is needed, or if it can answer directly.\n",
    "3. If a tool is needed, call the tool executor.\n",
    "4. Feed the tool's response back to the LLM for a final answer.\n",
    "5. Repeat tool calls if necessary (though our simple example might not require complex iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5487a-f847-4e35-9c0e-4fa5a67041bb",
   "metadata": {},
   "source": [
    "### Defining Agent State\n",
    "The state will primarily consist of a list of messages, tracking the conversation history including tool calls and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e8dfd-5917-4a57-a7d0-c776412f5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "\n",
    "print(\"AgentState defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52333e-dab3-45fb-9217-61ee3ed544bb",
   "metadata": {},
   "source": [
    "### Creating Agent Nodes (LLM Call, Tool Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f1fd0-7375-46de-871c-a00c6a8d00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: The Agent - Calls the LLM to decide an action or respond\n",
    "def call_agent_llm(state: AgentState):\n",
    "    \"\"\"Calls the LLM with the current conversation history (messages) and tools.\"\"\"\n",
    "    print(\"--- Node: call_agent_llm ---\")\n",
    "    if not llm_with_tools:\n",
    "        print(\"LLM with tools not available. Cannot call agent.\")\n",
    "        # Append an error message or handle appropriately\n",
    "        error_msg = HumanMessage(content=\"Error: LLM with tools not initialized.\")\n",
    "        return {\"messages\": [error_msg]}\n",
    "        \n",
    "    messages = state[\"messages\"]\n",
    "    print(f\"  Input messages to LLM: {messages}\")\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    print(f\"  LLM Response: {response}\")\n",
    "    # The response will be an AIMessage, possibly with tool_calls attribute\n",
    "    return {\"messages\": [response]} \n",
    "\n",
    "# Node 2: Tool Executor - Executes tools called by the LLM\n",
    "def execute_tools_node(state: AgentState):\n",
    "    \"\"\"Checks the last message for tool calls and executes them.\"\"\"\n",
    "    print(\"--- Node: execute_tools_node ---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not isinstance(last_message, AIMessage) or not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:\n",
    "        print(\"  No tool calls found in the last message.\")\n",
    "        return # No tools to execute, or last message is not an AIMessage with tool_calls\n",
    "\n",
    "    tool_invocation_messages = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        print(f\"  Executing tool: {tool_name} with args: {tool_args}\")\n",
    "        \n",
    "        selected_tool = None\n",
    "        for t in tools:\n",
    "            if t.name == tool_name:\n",
    "                selected_tool = t\n",
    "                break\n",
    "        \n",
    "        if selected_tool:\n",
    "            try:\n",
    "                # The tool execution might be synchronous or asynchronous depending on the tool definition\n",
    "                # For simple @tool decorated functions, it's usually synchronous.\n",
    "                observation = selected_tool.invoke(tool_args)\n",
    "            except Exception as e:\n",
    "                observation = f\"Error executing tool {tool_name}: {str(e)}\"\n",
    "        else:\n",
    "            observation = f\"Error: Tool '{tool_name}' not found.\"\n",
    "        \n",
    "        print(f\"  Tool Observation: {observation}\")\n",
    "        tool_invocation_messages.append(\n",
    "            ToolMessage(content=str(observation), tool_call_id=tool_call[\"id\"])\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": tool_invocation_messages}\n",
    "\n",
    "print(\"Agent nodes defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafffde-174b-4468-b254-71d1b547f48a",
   "metadata": {},
   "source": [
    "### Constructing the Graph and Conditional Edges\n",
    "We define the workflow: after the LLM call, if there are tool calls, execute them; otherwise, the process ends. After tools execute, their output goes back to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f580e-10ed-460d-975b-5cc498c828be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditional logic: Should we continue or end?\n",
    "def should_continue_or_end(state: AgentState):\n",
    "    \"\"\"Determines whether to continue with tool execution or end.\"\"\"\n",
    "    print(\"--- Conditional Edge: should_continue_or_end ---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"  Decision: Continue to tool execution.\")\n",
    "        return \"continue_to_tools\" # Route to tool executor node\n",
    "    else:\n",
    "        print(\"  Decision: End execution (LLM provided a direct answer or no more tools).\")\n",
    "        return END # End the graph execution\n",
    "\n",
    "# Create the StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"agent_llm\", call_agent_llm)\n",
    "workflow.add_node(\"tool_executor\", execute_tools_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"agent_llm\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_llm\", # Source node\n",
    "    should_continue_or_end, # Function to decide the route\n",
    "    path_map={\n",
    "        \"continue_to_tools\": \"tool_executor\", # If condition returns \"continue_to_tools\", go to tool_executor\n",
    "        END: END  # If condition returns END, finish the graph\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add an edge from the tool_executor back to the agent_llm to process tool results\n",
    "workflow.add_edge(\"tool_executor\", \"agent_llm\")\n",
    "\n",
    "\n",
    "print(\"LangGraph workflow defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec64efe-7f18-4cf7-a428-2c7661a959c9",
   "metadata": {},
   "source": [
    "### Compiling and Running the Agent\n",
    "Compile the graph to create a runnable application. We can also add memory for checkpoints if needed, but for this demo, a simple compile is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5a189-ee32-4dd6-a5f4-db4cbe740e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "if llm: # Only compile if LLM was initialized\n",
    "    # memory = MemorySaver() # Optional: for saving/resuming graph state, not strictly needed for this demo\n",
    "    # app = workflow.compile(checkpointer=memory)\n",
    "    app = workflow.compile()\n",
    "    print(\"LangGraph app compiled.\")\n",
    "    \n",
    "    # You can visualize the graph if you have graphviz installed:\n",
    "    # try:\n",
    "    #     from IPython.display import Image, display\n",
    "    #     display(Image(app.get_graph().draw_mermaid_png()))\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Could not draw graph (graphviz might not be installed): {e}\")\n",
    "else:\n",
    "    print(\"LLM not initialized. Skipping graph compilation.\")\n",
    "    app = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290ac2f-d11d-49f4-a0e1-4e7307b07913",
   "metadata": {},
   "source": [
    "![MLFlow Workflow](https://mlflow.org/docs/latest/assets/images/learn-core-components-b2c38671f104ca6466f105a92ed5aa68.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6aa8cc-cafa-4984-b16d-a3925b4d174f",
   "metadata": {},
   "source": [
    "## 6. MLflow Tracing for LangGraph Agents\n",
    "\n",
    "With `mlflow.langchain.autolog()` enabled, interactions within our LangGraph (which is part of LangChain) should be automatically traced when we invoke the compiled `app`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5f6f0-246c-44b1-bef1-375d83d7a48f",
   "metadata": {},
   "source": [
    "### How Autologging Works with LangGraph\n",
    "When you run the LangGraph `app.invoke(...)` or `app.stream(...)`:\n",
    "- MLflow's LangChain autologger intercepts calls to LangChain components, including LLMs (like `ChatOllama`) and potentially tools if they are wrapped as LangChain tools.\n",
    "- It creates a **trace** for each invocation of the graph. A trace is a hierarchical view of the operations performed.\n",
    "- **Spans** within the trace represent individual operations: LLM calls, tool executions, agent steps.\n",
    "- Inputs, outputs, parameters, and errors for each span are logged.\n",
    "\n",
    "This provides a detailed, visual record of your agent's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6242f80-7932-4f13-b050-5c12fca209bc",
   "metadata": {},
   "source": [
    "### Inspecting Traces in the MLflow UI\n",
    "After running some interactions with the agent (next section):\n",
    "1. Run `mlflow ui` in your terminal (from the directory containing `mlruns`).\n",
    "2. Navigate to the `LangGraph_ToolCalling_Agent_Ollama` experiment.\n",
    "3. You should see runs corresponding to each invocation of your LangGraph application.\n",
    "4. Click on a run. The **\"Traces\"** tab (often the default view for LangChain/LangGraph runs) will show the execution graph:\n",
    "    - You can see the sequence of LLM calls and tool executions.\n",
    "    - Click on individual spans (e.g., an LLM call span) to see its inputs (prompt), outputs (response, tool calls), configuration (model name, temperature), and duration.\n",
    "    - Tool execution spans will show the tool name, input arguments, and the observed output.\n",
    "\n",
    "![MLFlow Tracking](https://mlflow.org/docs/latest/assets/images/tracking-setup-local-server-cd51180e89bfd0a18c52f5b33e0f188d.png)\n",
    "\n",
    "This visual debugging and inspection capability is invaluable for understanding and refining complex agent behavior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1287baf-9160-4b26-974d-34a600a0bb10",
   "metadata": {},
   "source": [
    "## 7. Interacting with the Tool-Calling Agent\n",
    "Let's send some queries to our agent and observe its behavior. Each `app.invoke()` call will generate a trace in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fb05b-5e27-4990-9017-d154c91b339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_query(query_text):\n",
    "    if not app:\n",
    "        print(\"LangGraph app not compiled. Cannot run query.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n--- Running Agent for Query: '{query_text}' ---\")\n",
    "    inputs = {\"messages\": [HumanMessage(content=query_text)]}\n",
    "    \n",
    "    # Each invoke call should be captured by MLflow autologging as a new run/trace\n",
    "    # We can explicitly start a parent run for each query if we want to add more metadata\n",
    "    # around the LangGraph invocation itself.\n",
    "    with mlflow.start_run(run_name=f\"AgentQuery_{query_text[:30].replace(' ','_')}\") as run:\n",
    "        mlflow.log_param(\"user_query\", query_text)\n",
    "        mlflow.log_param(\"ollama_model_used\", ollama_model_name)\n",
    "        \n",
    "        try:\n",
    "            final_state = app.invoke(inputs, config={\"recursion_limit\": 10}) # Add recursion limit\n",
    "            final_response_message = final_state[\"messages\"][-1]\n",
    "            \n",
    "            if isinstance(final_response_message, AIMessage):\n",
    "                final_answer = final_response_message.content\n",
    "            elif isinstance(final_response_message, HumanMessage): # Could happen if LLM fails\n",
    "                final_answer = f\"Agent ended on HumanMessage: {final_response_message.content}\"\n",
    "            else:\n",
    "                final_answer = str(final_response_message) # Fallback\n",
    "                \n",
    "            print(f\"\\nFinal Agent Response: {final_answer}\")\n",
    "            mlflow.log_text(final_answer, \"final_agent_response.txt\")\n",
    "            mlflow.set_tag(\"agent_outcome\", \"success\")\n",
    "            return final_answer\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking agent: {e}\")\n",
    "            mlflow.log_text(str(e), \"agent_error.txt\")\n",
    "            mlflow.set_tag(\"agent_outcome\", \"error\")\n",
    "            return None\n",
    "\n",
    "# Test Queries\n",
    "query1 = \"What is the weather like in Paris today?\"\n",
    "query2 = \"What is 250 + 750 / 3?\"\n",
    "query3 = \"Can you tell me the weather in London and also calculate 5 * (10 - 3)?\"\n",
    "query4 = \"What is the capital of France?\" # Should be answered directly by LLM (no tool)\n",
    "\n",
    "if app: # Only run queries if app was compiled\n",
    "    run_agent_query(query1)\n",
    "    print(\"\\n------------------------------------\\n\")\n",
    "    run_agent_query(query2)\n",
    "    print(\"\\n------------------------------------\\n\")\n",
    "    # Query 3 might require the LLM to decide on multiple tool calls, or sequence them.\n",
    "    # The current simple agent might handle one tool per LLM response, or call multiple if the LLM supports parallel tool calling in its output.\n",
    "    # Let's assume for now it might try one, then the other in sequence if the loop works correctly.\n",
    "    run_agent_query(query3)\n",
    "    print(\"\\n------------------------------------\\n\")\n",
    "    run_agent_query(query4)\n",
    "else:\n",
    "    print(\"Skipping agent queries as the app was not compiled (likely due to LLM initialization issue).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eedb51-bdd3-445f-9024-698973c97080",
   "metadata": {},
   "source": [
    "After running these, go to the MLflow UI and inspect the traces for each query. You should see how the agent decided which tools to call (or not to call) and the flow of information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849c4d3-7bb2-44b0-a57b-293bb38aff6a",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways for Building and Tracing Agents\n",
    "\n",
    "This notebook introduced you to the powerful combination of LangGraph, Ollama, and MLflow for building and observing tool-calling agents:\n",
    "\n",
    "- **LangGraph for Agent Logic:** Provides a flexible, graph-based approach to define complex agent behaviors, state management, and conditional tool use.\n",
    "- **Local LLMs with Ollama:** Enables development and experimentation with powerful open-source LLMs running entirely on your local machine, enhancing privacy and reducing reliance on cloud APIs.\n",
    "- **Tool Definition:** Custom tools extend the agent's capabilities beyond the LLM's inherent knowledge.\n",
    "- **MLflow for Agent Tracing:** `mlflow.langchain.autolog()` is invaluable for capturing the detailed execution flow of LangGraph agents, including LLM decisions, tool inputs/outputs, and the overall state evolution. This is crucial for debugging, understanding, and improving agent performance.\n",
    "- **Iterative Development:** The ability to trace and inspect agent behavior allows for rapid iteration and refinement of prompts, tool definitions, and agent logic.\n",
    "\n",
    "Building robust agents often involves careful prompt engineering to guide the LLM in tool selection and response generation, as well as thoughtful tool design.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df407140-7c03-41c0-966a-eb48a424fb1d",
   "metadata": {},
   "source": [
    "## 9. Engaging Resources and Further Reading\n",
    "\n",
    "To explore further into agents, LangGraph, and MLflow tracing:\n",
    "\n",
    "- **LangGraph & LangChain Documentation:**\n",
    "    - [LangGraph Documentation](https://python.langchain.com/docs/langgraph)\n",
    "    - [LangChain Tool Use](https://python.langchain.com/docs/modules/agents/tools/)\n",
    "    - [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)\n",
    "- **MLflow Tracing:**\n",
    "    - [MLflow Tracing Documentation](https://mlflow.org/docs/latest/tracing/index.html)\n",
    "    - [MLflow LangChain Integration (includes LangGraph)](https://mlflow.org/docs/latest/tracing/integrations/langchain.html)\n",
    "    - [MLflow Ollama Tracing (via OpenAI SDK compatibility)](https://mlflow.org/docs/latest/tracing/integrations/ollama.html)\n",
    "- **Ollama:**\n",
    "    - [Ollama Official Website](https://ollama.com/)\n",
    "    - [Ollama GitHub](https://github.com/ollama/ollama)\n",
    "- **Community Examples and Tutorials:**\n",
    "    - [Pinecone Blog: Llama 3.1 Agent using LangGraph and Ollama](https://www.pinecone.io/learn/langgraph-ollama-llama/)\n",
    "    - YouTube tutorials like \"Local LangGraph Agents with Llama 3.1 + Ollama\" by James Briggs.\n",
    "    - Prabhat Pankaj's blog on Dynamic, Parallel Tool-Calling Agent with LangGraph.\n",
    "\n",
    "--- \n",
    "\n",
    "Congratulations on building and tracing your first tool-calling agent! This is a foundational skill for creating more sophisticated and interactive AI applications.\n",
    "\n",
    "**Coming Up Next (Notebook 8):** We'll delve deeper into advanced agentic patterns, exploring more complex function-calling scenarios and potentially looking at agent-to-agent communication protocols, all while keeping MLflow in the loop.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
