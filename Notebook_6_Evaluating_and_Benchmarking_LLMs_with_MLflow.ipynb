{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2b996137-ee12-45da-9e8b-9e10c241ddd2","cell_type":"markdown","source":"# MLflow 06: Evaluating and Benchmarking LLMs with MLflow\n\nWelcome to the sixth notebook in our MLflow series! So far, we've covered tracking experiments, HPO, model registry, RAG, and fine-tuning LLMs like Qwen3-0.6B. Now, a crucial question arises: *How good are these models?* And, *how do they compare against each other or against a baseline?*\n\nThis notebook dives into **Evaluating and Benchmarking Large Language Models (LLMs)**. We'll explore how to:\n- Select appropriate evaluation datasets and metrics for LLM tasks.\n- Use Hugging Face's `evaluate` library for calculating standard metrics.\n- Leverage **MLflow's `mlflow.evaluate()` API** designed for LLMs to streamline the evaluation process.\n- Systematically log evaluation results (metrics, parameters, sample outputs) for different LLMs (base models, fine-tuned models) to MLflow.\n- Use the MLflow UI to compare model performance and create a benchmark/leaderboard.\n\n![LLM Evaluation Concept](https://aisera.com/wp-content/uploads/2023/12/LLM-Evaluation.png)\n\nEffective evaluation is key to making informed decisions in your LLM development lifecycle, guiding model selection, fine-tuning efforts, and understanding model capabilities and limitations.\n\n---","metadata":{}},{"id":"d27bb9a1-c3eb-444a-992a-5ff78ea86f01","cell_type":"markdown","source":"## Table of Contents\n\n1. [The Importance and Challenges of LLM Evaluation](#importance-challenges-llm-eval)\n2. [Setting Up the Evaluation Environment](#setting-up-eval-env)\n    - [Installing Libraries](#installing-libraries-eval)\n    - [GPU Check](#gpu-check-eval)\n    - [Configuring MLflow](#configuring-mlflow-eval)\n3. [Choosing an Evaluation Dataset and Task](#choosing-eval-dataset-task)\n    - [Task: Text Summarization](#task-text-summarization)\n    - [Dataset: `openai/summarize_from_feedback` (Axis subset)](#dataset-summarize-feedback)\n4. [Selecting LLMs for Evaluation](#selecting-llms-for-eval)\n    - [Model 1: Base `Qwen/Qwen3-0.6B`](#model1-base-qwen3)\n    - [Model 2: `google/flan-t5-small` (Baseline Instruction Model)](#model3-flan-t5)\n5. [Overview of Evaluation Metrics](#overview-eval-metrics)\n    - [ROUGE, BERTScore, Perplexity](#rouge-bertscore-perplexity)\n6. [Evaluating LLMs with `mlflow.evaluate()`](#evaluating-with-mlflow-evaluate)\n    - [Preparing the Evaluation Data](#preparing-eval-data-mlflow)\n    - [Evaluating Model 1: Base `Qwen/Qwen3-0.6B`](#evaluating-model1)\n    - [Evaluating Model 2: Fine-tuned `Qwen/Qwen3-0.6B`](#evaluating-model2)\n    - [Evaluating Model 3: `google/flan-t5-small`](#evaluating-model3)\n7. [Comparing Model Performance in MLflow UI](#comparing-models-mlflow-ui)\n8. [Brief: Advanced Evaluation Concepts & Tools](#advanced-eval-concepts)\n9. [Key Takeaways](#key-takeaways-eval)\n10. [Engaging Resources and Further Reading](#resources-and-further-reading-eval)\n\n---","metadata":{}},{"id":"147f1149-439e-45bf-905e-d57affb9a322","cell_type":"markdown","source":"## 1. The Importance and Challenges of LLM Evaluation\n\nEvaluating LLMs is crucial for:\n- **Model Selection:** Choosing the best model (base or fine-tuned) for a specific task.\n- **Tracking Progress:** Measuring improvements from fine-tuning or changes in prompting strategies.\n- **Understanding Capabilities & Limitations:** Identifying strengths and weaknesses of a model.\n- **Ensuring Responsible AI:** Assessing aspects like fairness, bias, toxicity, and factuality (though these often require specialized evaluation setups).\n\n**Challenges in LLM Evaluation:**\n- **Open-endedness:** Generated text can be diverse, making it hard for simple metrics to capture true quality.\n- **Lack of Ground Truth:** For some generative tasks, a single \"correct\" answer doesn't exist.\n- **Metric Limitations:** Traditional metrics (e.g., BLEU, ROUGE) capture surface-level similarities but may miss semantic meaning or factual correctness.\n- **Cost and Effort:** Human evaluation is often the gold standard but is expensive and time-consuming.\n- **Task Diversity:** Different tasks (summarization, QA, translation, creative writing) require different evaluation approaches.\n\nDespite these challenges, a combination of automated metrics and qualitative analysis provides valuable insights. MLflow helps organize and compare these varied evaluation results.\n\n---","metadata":{}},{"id":"435c39ba-def0-4c86-9a6c-51d94a83f836","cell_type":"markdown","source":"## 2. Setting Up the Evaluation Environment","metadata":{}},{"id":"c09f3001-8573-41b0-a2fe-51b3c3902904","cell_type":"markdown","source":"### Installing Libraries\nWe'll need `mlflow`, `transformers`, `datasets`, `evaluate` (from Hugging Face), `peft` (for loading LoRA adapters), `bitsandbytes` (for quantization), `accelerate`, `rouge_score`, `bert_score`, and `sentencepiece`.","metadata":{}},{"id":"c2c9f334-43f1-47c5-b60f-33f1562a832c","cell_type":"code","source":"!pip install --quiet mlflow transformers>=4.51.0 datasets evaluate peft trl bitsandbytes sentencepiece accelerate rouge_score bert_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport mlflow\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    pipeline,\n    AutoModelForSeq2SeqLM# For Flan-T5\n)\nfrom peft import PeftModel # For loading LoRA adapters\nimport evaluate # Hugging Face evaluate library\nimport pandas as pd\nimport os\nimport shutil\n\nprint(f\"MLflow Version: {mlflow.__version__}\")\nprint(f\"PyTorch Version: {torch.__version__}\")\nimport transformers\nprint(f\"Transformers Version: {transformers.__version__}\")\nimport evaluate as hf_evaluate_lib # Alias to avoid confusion if mlflow.evaluate is used\nprint(f\"Hugging Face Evaluate Library Version: {hf_evaluate_lib.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4b60c51b-dca9-4872-a981-2f5ff0574698","cell_type":"markdown","source":"### GPU Check\nLLM inference, even for smaller models, is faster on GPU.","metadata":{}},{"id":"738e1919-7b41-4e09-977e-4eb8b04f35de","cell_type":"code","source":"if torch.cuda.is_available():\n    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n    torch.cuda.set_device(0)\n    current_device_name = torch.cuda.get_device_name(0)\nelse:\n    print(\"CUDA not available. LLM evaluation will run on CPU and might be slow.\")\n    current_device_name = 'cpu'\n\ndef clear_gpu_cache():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e5c7c653-15bf-494d-945c-e3f6a853a8f1","cell_type":"markdown","source":"### Configuring MLflow","metadata":{}},{"id":"fe30d1ad-8f80-4051-b7c1-c7dfe20079f1","cell_type":"code","source":"mlflow.set_tracking_uri('mlruns')\nexperiment_name = \"LLM_Evaluation_Summarization_Benchmark\"\nmlflow.set_experiment(experiment_name)\nprint(f\"MLflow Experiment set to: {experiment_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ce81b263-9420-41fa-a1fe-d516ea29262e","cell_type":"markdown","source":"---","metadata":{}},{"id":"c8bfda92-5b43-42bc-8696-3a37a465fe6d","cell_type":"markdown","source":"## 3. Choosing an Evaluation Dataset and Task","metadata":{}},{"id":"687180b8-6a72-4720-9d03-54ecbcf356f3","cell_type":"markdown","source":"### Task: Text Summarization\nWe'll evaluate the LLMs on their ability to generate concise summaries of given texts. This is a common and important NLP task.","metadata":{}},{"id":"48a4e0fc-9123-480a-af79-5d6a6f271d98","cell_type":"markdown","source":"### Dataset: `openai/summarize_from_feedback` (Axis subset)\nThis dataset was used in the development of InstructGPT and contains human-written summaries along with human feedback. We'll use the `axis` subset, which consists of Reddit posts and their TL;DR summaries.\n\n**Dataset Description: (Copied from [here](https://huggingface.co/datasets/openai/summarize_from_feedback))**\n\nIn the Learning to Summarize from Human Feedback paper, a reward model was trained from human feedback. The reward model was then used to train a summarization model to align with human preferences. This is the dataset of human feedback that was released for reward modelling. There are two parts of this dataset: comparisons and axis. In the comparisons part, human annotators were asked to choose the best out of two summaries. In the axis part, human annotators gave scores on a likert scale for the quality of a summary. The comparisons part only has a train and validation split, and the axis part only has a test and validation split.\n\nThe summaries used for training the reward model in the paper come from the TL;DR dataset. Additional validation and test data come from the TL;DR dataset, CNN articles, and Daily Mail articles.\n\n\n**Note:** This dataset is suitable because it provides text-summary pairs, allowing us to compute metrics like ROUGE and BERTScore.","metadata":{}},{"id":"fbeb2a60-e24b-40c5-9a6f-4ed163d2a5f2","cell_type":"code","source":"dataset_name = \"openai/summarize_from_feedback\"\ndataset_config_name = \"axis\"\nnum_eval_samples = 50 # Number of samples to use for evaluation (adjust as needed for speed vs. thoroughness)\n\ntry:\n    eval_dataset_full = load_dataset(dataset_name, dataset_config_name, split=\"validation\")\n    # Select a subset for faster evaluation in this demo\n    eval_dataset = eval_dataset_full.select(range(num_eval_samples))\n    print(f\"Loaded {len(eval_dataset)} samples from '{dataset_name}/{dataset_config_name}' for evaluation.\")\n    \n    # Inspect a sample\n    sample_entry = eval_dataset[0]\n    print(\"\\nSample Entry:\")\n    print(f\"  Post (Input): {sample_entry['info']['post'][:300]}...\")\n    print(f\"  Reference Summary (Target): {sample_entry['summary']['text']}\") \nexcept Exception as e:\n    print(f\"Error loading dataset: {e}. This might be due to connectivity or dataset access issues.\")\n    print(\"Creating a dummy dataset for fallback.\")\n    dummy_data = {\n        'info': [{'post': 'This is a long post about the benefits of MLflow for MLOps. MLflow helps track experiments, package models, and manage the ML lifecycle.'}] * num_eval_samples,\n        'summaries': [[{'text': 'MLflow is great for MLOps.'}]] * num_eval_samples\n    }\n    eval_dataset = Dataset.from_dict(dummy_data)\n    print(f\"Using dummy dataset with {len(eval_dataset)} samples.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91239138-6300-45b5-a70e-f1d20ff575d2","cell_type":"markdown","source":"---","metadata":{}},{"id":"0a77330b-1c0f-4903-bfca-2bb5db054e45","cell_type":"markdown","source":"## 4. Selecting LLMs for Evaluation\nWe will evaluate two different models to see how they perform on the summarization task:","metadata":{}},{"id":"bfbb2d17-068b-43e0-8db7-f2f76a10e159","cell_type":"markdown","source":"### Model 1: Base `Qwen/Qwen3-0.6B`\nThis is the pre-trained Qwen3-0.6B model without any further fine-tuning from our side. We'll use quantization for efficient inference.","metadata":{}},{"id":"a4e452d8-c3ea-4cc0-81fd-e8e38db1c792","cell_type":"markdown","source":"### Model 2: `google/flan-t5-small` (Baseline Instruction Model)\nFlan-T5 is a well-known family of instruction-tuned models. The 'small' version is efficient and provides a good baseline for comparison.","metadata":{}},{"id":"c2fd1c0f-3ce4-4885-8986-cb160ce13e79","cell_type":"markdown","source":"We will load these models sequentially to manage VRAM.\n\n---","metadata":{}},{"id":"5d996345-38a1-4722-93c9-e61e6c7e6358","cell_type":"markdown","source":"## 5. Overview of Evaluation Metrics\n\nWe'll use a combination of metrics, primarily those supported by `mlflow.evaluate` for summarization:\n\n- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Measures overlap between the generated summary and reference summary (e.g., ROUGE-1, ROUGE-2, ROUGE-L for unigram, bigram, and longest common subsequence overlap).\n- **BERTScore:** Computes similarity between generated and reference summaries using contextual embeddings from BERT, often correlating better with human judgment than ROUGE.\n- **Perplexity (via `mlflow.evaluate`):** A measure of how well a probability model predicts a sample. Lower perplexity generally indicates better fluency and coherence of the generated text by the model itself (not comparing to a reference, but internal consistency). *Note: `mlflow.evaluate` can compute perplexity if the model is a `transformers` pipeline or model.* \n\nMLflow's `evaluate` API can also compute other metrics like `exact_match`, `f1`, and potentially model-based metrics if configured (e.g., toxicity, PII detection using an LLM as a judge, though this is more advanced).\n\n---","metadata":{}},{"id":"e0b43f92-f633-42f6-8769-712f34141081","cell_type":"markdown","source":"## 6. Evaluating LLMs with `mlflow.evaluate()`\n\nThe `mlflow.evaluate()` API provides a structured way to evaluate models, especially for common NLP tasks like text summarization. It requires:\n- A `model` (can be a model URI, a `transformers` pipeline, or a custom Python function).\n- `data` (a Pandas DataFrame or a dataset path).\n- `targets` (column name for reference/ground truth).\n- `inputs` (column name for model input text).\n- `model_type` (e.g., `\"text-summarization\"`, `\"question-answering\"`, `\"text-generation\"`).\n- `metrics` (a list of metric names or custom metric functions).\n- `evaluators` and `evaluator_config` for more advanced, model-based evaluations.\n\n","metadata":{}},{"id":"d70b7dad-8268-4b4a-b481-e3ee644d7d97","cell_type":"markdown","source":"### Preparing the Evaluation Data\n`mlflow.evaluate` often works best with Pandas DataFrames. Let's convert our Hugging Face dataset subset.","metadata":{}},{"id":"8d77486e-777c-44a8-9d38-e41a13c0d98a","cell_type":"code","source":"eval_df = pd.DataFrame({\n    \"prompt_text\": [entry['info']['post'] for entry in eval_dataset],\n    \"reference_summary\": [entry['summary']['text'] for entry in eval_dataset]\n})\n\nprint(\"Evaluation DataFrame prepared:\")\nprint(eval_dataset.column_names)\n(eval_df.head(5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a480b215-2519-4707-affa-6434a3fda71e","cell_type":"markdown","source":"### Evaluating Model 1: Base `Qwen/Qwen3-0.6B`","metadata":{}},{"id":"c5daaac8-b33f-47b9-9d63-bb5764ce3a2a","cell_type":"code","source":"clear_gpu_cache()\nmodel_name_qwen3_base = \"Qwen/Qwen3-0.6B\" # Using a Qwen3 model. Adjust if needed.\nrun_name_qwen3_base = \"Eval_Base_Qwen3_0.6B_Summarization\" # Adjusted name\n\ntry:\n    print(f\"Loading base model for evaluation: {model_name_qwen3_base}\")\n    qwen3_base_tokenizer = AutoTokenizer.from_pretrained(model_name_qwen3_base, trust_remote_code=True)\n    if qwen3_base_tokenizer.pad_token is None:\n        qwen3_base_tokenizer.pad_token = qwen3_base_tokenizer.eos_token\n    \n    qwen3_base_pipeline = pipeline(\n        \"text-generation\",\n        model=model_name_qwen3_base,\n        tokenizer=qwen3_base_tokenizer,\n        trust_remote_code=True,\n        model_kwargs={\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_4bit=True, \n                bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n            )\n        }\n    )\n    print(f\"Base {model_name_qwen3_base} pipeline created.\")\n\n    # This is your existing wrapper class, unchanged\n    class SummarizationPipelineWrapper:\n        def __init__(self, pipeline_obj):\n            self.pipeline = pipeline_obj\n        def predict(self, X_df): # Renamed X to X_df for clarity that it's a DataFrame\n            prompts = X_df[\"generation_prompt\"].tolist()\n            outputs = self.pipeline(\n                prompts, \n                max_new_tokens=100, \n                num_return_sequences=1, \n                do_sample=False, \n                pad_token_id=self.pipeline.tokenizer.eos_token_id\n            )\n            results = []\n            for i, out_list in enumerate(outputs):\n                full_text = out_list[0]['generated_text']\n                # Ensure prompt removal is robust. If the model doesn't echo the exact prompt, this might need adjustment.\n                # A more robust way might be to only take text after the prompt + known summary signal.\n                # For now, assuming replace works for your Qwen model's output structure.\n                summary = full_text.replace(prompts[i], \"\").strip()\n                results.append(summary)\n            return pd.Series(results)\n\n    # Instantiate your wrapper\n    summarization_model_qwen3_base = SummarizationPipelineWrapper(qwen3_base_pipeline)\n\n    # Create the compatible function to pass to mlflow.evaluate\n    # This function will use the 'summarization_model_qwen3_base' instance defined above.\n    def mlflow_compatible_predict_function(data_input_df):\n        return summarization_model_qwen3_base.predict(data_input_df)\n\n    with mlflow.start_run(run_name=run_name_qwen3_base) as run:\n        mlflow.log_param(\"model_name\", model_name_qwen3_base)\n        mlflow.log_param(\"evaluation_task\", \"text-summarization\")\n        # Ensure dataset_name and dataset_config_name are defined\n        mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name if dataset_config_name else ''}\")\n        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n        mlflow.set_tag(\"model_type\", \"base_llm\")\n\n        print(f\"Starting mlflow.evaluate for {model_name_qwen3_base}...\")\n        \n        eval_df_for_gen = eval_df.copy() # Ensure eval_df is defined\n        eval_df_for_gen[\"generation_prompt\"] = eval_df_for_gen[\"prompt_text\"].apply(\n            lambda x: f\"Summarize the following text:\\n\\n{x}\\n\\nSummary:\"\n        )\n        \n        results = mlflow.evaluate(\n            model=mlflow_compatible_predict_function, # Pass the new compatible function\n            data=eval_df_for_gen, \n            targets=\"reference_summary\",\n            # input_example was correctly removed in the previous step\n            model_type=\"text\", \n        )\n        #------------------\n        print(f\"Base {model_name_qwen3_base} evaluation results:\")\n        if results.metrics:\n            metrics_df = pd.DataFrame(list(results.metrics.items()), columns=[\"Metric\", \"Value\"])\n            print(metrics_df.to_string())\n        else:\n            print(\"No metrics were returned by mlflow.evaluate.\")\n        #-------------------    \n        if results.artifacts and \"eval_results_table.json\" in results.artifacts:\n             print(f\"Evaluation table artifact path: {results.artifacts['eval_results_table.json'].uri}\")\n\nexcept Exception as e:\n    print(f\"Error evaluating base {model_name_qwen3_base}: {e}\")\n    # For detailed debugging, you might want to print the traceback\n    import traceback\n    traceback.print_exc()\nfinally:\n    # Ensure these objects exist before trying to delete them\n    if 'qwen3_base_pipeline' in locals():\n        del qwen3_base_pipeline\n    if 'summarization_model_qwen3_base' in locals(): \n        del summarization_model_qwen3_base\n    clear_gpu_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b40f32ee-f1b5-44dc-a242-0fd977decaea","cell_type":"markdown","source":"### Evaluating Model 2: `google/flan-t5-small`","metadata":{}},{"id":"d229be82-8409-4a7c-948e-dadbdd95d6d0","cell_type":"code","source":"clear_gpu_cache()\nmodel_name_flan_t5 = \"google/flan-t5-small\"\nrun_name_flan_t5 = \"Eval_Flan_T5_Small_Summarization\"\n\ntry:\n    print(f\"Loading Flan-T5 model for evaluation: {model_name_flan_t5}\")\n    flan_t5_pipeline = pipeline(\n        \"text2text-generation\", \n        model=model_name_flan_t5,\n        tokenizer=model_name_flan_t5,\n        device=0 if torch.cuda.is_available() else -1,\n    )\n    print(\"Flan-T5-small pipeline created.\")\n\n    with mlflow.start_run(run_name=run_name_flan_t5) as run:\n        mlflow.log_param(\"model_name\", model_name_flan_t5)\n        mlflow.log_param(\"evaluation_task\", \"text-summarization\")\n        mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name}\")\n        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n        mlflow.set_tag(\"model_type\", \"instruction_tuned_baseline\")\n\n        class FlanT5SummarizationWrapper:\n            def __init__(self, pipeline_obj):\n                self.pipeline = pipeline_obj\n            def predict(self, X):\n                prompts = X[\"prompt_text\"].apply(lambda x: f\"Summarize: {x}\").tolist()\n                outputs = self.pipeline(prompts, max_length=100, num_return_sequences=1, do_sample=False)\n                return pd.Series([out['generated_text'] for out in outputs])\n\n        summarization_model_flan_t5 = FlanT5SummarizationWrapper(flan_t5_pipeline)\n        eval_df_for_flan = eval_df.copy()\n\n        # Create MLflow-compatible predict function\n        def flan_predict_function(data_input):\n            return summarization_model_flan_t5.predict(data_input)\n\n        print(f\"Starting mlflow.evaluate for {model_name_flan_t5}...\")\n        results_flan_t5 = mlflow.evaluate(\n            model=flan_predict_function,  # Pass the function instead of the wrapper instance\n            data=eval_df_for_flan,\n            targets=\"reference_summary\",\n            model_type=\"text\",\n        )\n\n        print(\"Flan-T5-small evaluation results:\")\n        if results_flan_t5.metrics:\n            metrics_df_flan = pd.DataFrame(list(results_flan_t5.metrics.items()), columns=[\"Metric\", \"Value\"])\n            print(metrics_df_flan.to_string())\n        else:\n            print(\"No metrics were returned by mlflow.evaluate.\")\n            \n        if results_flan_t5.artifacts and \"eval_results_table.json\" in results_flan_t5.artifacts:\n             print(f\"Evaluation table artifact path: {results_flan_t5.artifacts['eval_results_table.json'].uri}\")\n\nexcept Exception as e:\n    print(f\"Error evaluating Flan-T5-small: {e}\")\nfinally:\n    del flan_t5_pipeline\n    if 'summarization_model_flan_t5' in locals(): del summarization_model_flan_t5\n    clear_gpu_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b996c975-8d8a-4ca9-a616-78ba94084aac","cell_type":"markdown","source":"---","metadata":{}},{"id":"353fa2c7-5a62-456b-a232-f35b6e72d02e","cell_type":"markdown","source":"## 7. Comparing Model Performance in MLflow UI\n\nNow, the power of MLflow comes into play! Open the MLflow UI (`mlflow ui` from the directory containing `mlruns`).\n\n1.  Navigate to the `LLM_Evaluation_Summarization_Benchmark` experiment.\n2.  You should see two runs (or more, if you re-ran parts):\n    - `Eval_Base_Qwen3_0.6B_Summarization`\n    - `Eval_Flan_T5_Small_Summarization`\n3.  **Select all these runs** by checking the boxes next to them.\n4.  Click the **\"Compare\"** button.\n\n**In the Comparison View:**\n- **Parameters:** You can see the `model_name`, `dataset_name`, etc., for each run.\n- **Metrics:** This is where it gets interesting! \n    - `mlflow.evaluate` logs various metrics it computes (e.g., from `rouge`, `bertscore`, `exact_match`, `perplexity` if computed). The exact names might be like `rouge1`, `rougeL`, `bertscore_precision`, etc. \n    - You can sort the table by any metric (e.g., sort by `rougeL` descending) to see which model performed best on that specific metric.\n- **Artifacts:** For each run, `mlflow.evaluate` saves an `eval_results_table.json` (and often a `.html` version) in the artifacts. This table contains the input prompts, generated outputs, reference targets, and per-sample metric scores. This is invaluable for qualitative analysis and error inspection.\n\n\nThis comparison view effectively creates a **leaderboard** for your evaluated models on this specific task and dataset, all managed and visualized by MLflow.\n\n---","metadata":{}},{"id":"820099a6-2951-4f3c-ac9e-c17d17f7d1b1","cell_type":"markdown","source":"## 8. Brief: Advanced Evaluation Concepts & Tools\n\nWhile `mlflow.evaluate` and standard metrics give a good starting point, LLM evaluation is a rapidly evolving field. Some advanced concepts and tools include:\n\n- **Human Evaluation:** Still the gold standard for nuanced aspects like coherence, creativity, helpfulness. Platforms exist to manage human annotation workflows.\n- **LLM-as-a-Judge:** Using a powerful LLM (like GPT-4) to evaluate the output of another LLM based on predefined criteria or a rubric. `mlflow.evaluate` has some capabilities here via `evaluators` like `mlflow.metrics.genai`.\n- **Task-Specific Benchmarks:** For specific tasks, dedicated benchmarks exist (e.g., HumanEval for code generation, MMLU for broad knowledge, BigBench for challenging reasoning tasks).\n- **Specialized Evaluation Frameworks:**\n    - **Ragas:** For evaluating RAG pipelines (retrieval and generation quality).\n    - **TruLens:** Focuses on explainability and tracking quality for LLM apps.\n    - **DeepEval:** Offers a suite of metrics for in-depth LLM evaluation.\n    - **EleutherAI LM Evaluation Harness:** A comprehensive framework for running many standard academic benchmarks.\n- **Ethical AI & Responsible AI Metrics:** Evaluating for bias, fairness, toxicity, robustness against adversarial attacks. These are critical for production systems.\n\nMLflow can often be integrated with these tools to store their results, providing a central dashboard for all your evaluation efforts.\n\n---","metadata":{}},{"id":"f9c0a264-804b-4570-b5ad-b0967c80e2eb","cell_type":"markdown","source":"## 9. Key Takeaways\n\nIn this notebook, we've learned how to systematically evaluate and benchmark LLMs:\n\n- **Structured Evaluation:** Understood the importance of a consistent process for evaluating LLMs on specific tasks and datasets.\n- **`mlflow.evaluate()` for LLMs:** Leveraged MLflow's dedicated evaluation API to assess models on text summarization, automatically computing relevant metrics.\n- **Metric-Driven Comparison:** Used metrics like ROUGE and BERTScore to quantify differences in model performance.\n- **Comparative Benchmarking:** Compared a base pre-trained model (`Qwen3-0.6B`) and an instruction-tuned baseline (`Flan-T5-small`).\n- **MLflow for Centralized Results:** Used MLflow to log all evaluation parameters, metrics, and qualitative artifacts (like per-sample predictions), enabling easy comparison and leaderboard creation via the UI.\n- **Qualitative Insights:** Recognized the value of inspecting generated outputs (available in MLflow artifacts) alongside quantitative metrics.\n\nThis systematic evaluation approach is vital for iterating on LLM development, whether you're choosing a base model, assessing fine-tuning impact, or comparing different prompting strategies.\n\n---","metadata":{}},{"id":"cd345ea5-1ae4-4025-a832-790ecbbe4c32","cell_type":"markdown","source":"## 10. Engaging Resources and Further Reading\n\nTo deepen your understanding of LLM evaluation:\n\n- **MLflow Documentation:**\n    - [MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n    - [Built-in Metrics for LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/metrics.html)\n- **Hugging Face Evaluate Library:**\n    - [Hugging Face `evaluate` Documentation](https://huggingface.co/docs/evaluate/index)\n    - [List of Available Metrics](https://huggingface.co/spaces/evaluate-metric)\n- **Key Evaluation Papers & Concepts:**\n    - [ROUGE Paper (Lin, 2004)](https://aclanthology.org/W04-1013/)\n    - [BERTScore Paper (Zhang et al., 2019)](https://arxiv.org/abs/1904.09675)\n    - [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena (Zheng et al., 2023 - for LLM-as-judge concepts)](https://arxiv.org/abs/2306.05685)\n- **LLM Evaluation Leaderboards & Platforms:**\n    - [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n    - [Chatbot Arena (LMSys)](https://chat.lmsys.org/)\n\n--- \n\nFantastic work on completing this comprehensive LLM evaluation notebook! You're now well-equipped to assess and compare different language models using robust methodologies and MLflow.\n\n**Coming Up Next (Notebook 7):** We'll shift gears to building more dynamic and interactive AI systems by exploring Tool-Calling Agents with LangGraph, Ollama, and, of course, tracking it all with MLflow.\n\n![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)","metadata":{}}]}
