{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow 06: Evaluating and Benchmarking LLMs with MLflow\n",
    "\n",
    "Welcome to the sixth notebook in our MLflow series! So far, we've covered tracking experiments, HPO, model registry, RAG, and fine-tuning LLMs like Qwen3-0.6B. Now, a crucial question arises: *How good are these models?* And, *how do they compare against each other or against a baseline?*\n",
    "\n",
    "This notebook dives into **Evaluating and Benchmarking Large Language Models (LLMs)**. We'll explore how to:\n",
    "- Select appropriate evaluation datasets and metrics for LLM tasks.\n",
    "- Use Hugging Face's `evaluate` library for calculating standard metrics.\n",
    "- Leverage **MLflow's `mlflow.evaluate()` API** designed for LLMs to streamline the evaluation process.\n",
    "- Systematically log evaluation results (metrics, parameters, sample outputs) for different LLMs (base models, fine-tuned models) to MLflow.\n",
    "- Use the MLflow UI to compare model performance and create a benchmark/leaderboard.\n",
    "\n",
    "![LLM Evaluation Concept](https://aisera.com/wp-content/uploads/2023/12/LLM-Evaluation.png)\n",
    "\n",
    "Effective evaluation is key to making informed decisions in your LLM development lifecycle, guiding model selection, fine-tuning efforts, and understanding model capabilities and limitations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [The Importance and Challenges of LLM Evaluation](#importance-challenges-llm-eval)\n",
    "2. [Setting Up the Evaluation Environment](#setting-up-eval-env)\n",
    "    - [Installing Libraries](#installing-libraries-eval)\n",
    "    - [GPU Check](#gpu-check-eval)\n",
    "    - [Configuring MLflow](#configuring-mlflow-eval)\n",
    "3. [Choosing an Evaluation Dataset and Task](#choosing-eval-dataset-task)\n",
    "    - [Task: Text Summarization](#task-text-summarization)\n",
    "    - [Dataset: `openai/summarize_from_feedback` (TLDR subset)](#dataset-summarize-feedback)\n",
    "4. [Selecting LLMs for Evaluation](#selecting-llms-for-eval)\n",
    "    - [Model 1: Base `Qwen/Qwen3-0.6B`](#model1-base-qwen3)\n",
    "    - [Model 2: Fine-tuned `Qwen/Qwen3-0.6B` (Recipe Bot)](#model2-ft-qwen3)\n",
    "    - [Model 3: `google/flan-t5-small` (Baseline Instruction Model)](#model3-flan-t5)\n",
    "5. [Overview of Evaluation Metrics](#overview-eval-metrics)\n",
    "    - [ROUGE, BERTScore, Perplexity](#rouge-bertscore-perplexity)\n",
    "6. [Evaluating LLMs with `mlflow.evaluate()`](#evaluating-with-mlflow-evaluate)\n",
    "    - [Preparing the Evaluation Data](#preparing-eval-data-mlflow)\n",
    "    - [Evaluating Model 1: Base `Qwen/Qwen3-0.6B`](#evaluating-model1)\n",
    "    - [Evaluating Model 2: Fine-tuned `Qwen/Qwen3-0.6B`](#evaluating-model2)\n",
    "    - [Evaluating Model 3: `google/flan-t5-small`](#evaluating-model3)\n",
    "7. [Comparing Model Performance in MLflow UI](#comparing-models-mlflow-ui)\n",
    "8. [Brief: Advanced Evaluation Concepts & Tools](#advanced-eval-concepts)\n",
    "9. [Key Takeaways](#key-takeaways-eval)\n",
    "10. [Engaging Resources and Further Reading](#resources-and-further-reading-eval)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Importance and Challenges of LLM Evaluation\n",
    "\n",
    "Evaluating LLMs is crucial for:\n",
    "- **Model Selection:** Choosing the best model (base or fine-tuned) for a specific task.\n",
    "- **Tracking Progress:** Measuring improvements from fine-tuning or changes in prompting strategies.\n",
    "- **Understanding Capabilities & Limitations:** Identifying strengths and weaknesses of a model.\n",
    "- **Ensuring Responsible AI:** Assessing aspects like fairness, bias, toxicity, and factuality (though these often require specialized evaluation setups).\n",
    "\n",
    "**Challenges in LLM Evaluation:**\n",
    "- **Open-endedness:** Generated text can be diverse, making it hard for simple metrics to capture true quality.\n",
    "- **Lack of Ground Truth:** For some generative tasks, a single \"correct\" answer doesn't exist.\n",
    "- **Metric Limitations:** Traditional metrics (e.g., BLEU, ROUGE) capture surface-level similarities but may miss semantic meaning or factual correctness.\n",
    "- **Cost and Effort:** Human evaluation is often the gold standard but is expensive and time-consuming.\n",
    "- **Task Diversity:** Different tasks (summarization, QA, translation, creative writing) require different evaluation approaches.\n",
    "\n",
    "Despite these challenges, a combination of automated metrics and qualitative analysis provides valuable insights. MLflow helps organize and compare these varied evaluation results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Evaluation Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries\n",
    "We'll need `mlflow`, `transformers`, `datasets`, `evaluate` (from Hugging Face), `peft` (for loading LoRA adapters), `bitsandbytes` (for quantization), `accelerate`, `rouge_score`, `bert_score`, and `sentencepiece`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet mlflow \"transformers>=4.51.0\" datasets evaluate peft trl bitsandbytes sentencepiece accelerate rouge_score bert_score\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForSeq2SeqLM # For Flan-T5\n",
    ")\n",
    "from peft import PeftModel # For loading LoRA adapters\n",
    "import evaluate # Hugging Face evaluate library\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "import transformers\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "import evaluate as hf_evaluate_lib # Alias to avoid confusion if mlflow.evaluate is used\n",
    "print(f\"Hugging Face Evaluate Library Version: {hf_evaluate_lib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Check\n",
    "LLM inference, even for smaller models, is faster on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.set_device(0)\n",
    "    current_device_name = torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    print(\"CUDA not available. LLM evaluation will run on CPU and might be slow.\")\n",
    "    current_device_name = 'cpu'\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('mlruns')\n",
    "experiment_name = \"LLM_Evaluation_Summarization_Benchmark\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing an Evaluation Dataset and Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Text Summarization\n",
    "We'll evaluate the LLMs on their ability to generate concise summaries of given texts. This is a common and important NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: `openai/summarize_from_feedback` (TLDR subset)\n",
    "This dataset was used in the development of InstructGPT and contains human-written summaries along with human feedback. We'll use the `tldr` subset, which consists of Reddit posts and their TL;DR summaries.\n",
    "-   **Input:** Reddit post content (the `info.post` field).\n",
    "-   **Target/Reference:** Human-written TL;DR summary (the `summaries[0].text` field, taking the first summary as reference).\n",
    "\n",
    "This dataset is suitable because it provides text-summary pairs, allowing us to compute metrics like ROUGE and BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"openai/summarize_from_feedback\"\n",
    "dataset_config_name = \"tldr\" # Using the 'tldr' configuration for Reddit posts\n",
    "num_eval_samples = 50 # Number of samples to use for evaluation (adjust as needed for speed vs. thoroughness)\n",
    "\n",
    "try:\n",
    "    eval_dataset_full = load_dataset(dataset_name, dataset_config_name, split=\"validation\")\n",
    "    # Select a subset for faster evaluation in this demo\n",
    "    eval_dataset = eval_dataset_full.select(range(num_eval_samples))\n",
    "    print(f\"Loaded {len(eval_dataset)} samples from '{dataset_name}/{dataset_config_name}' for evaluation.\")\n",
    "    \n",
    "    # Inspect a sample\n",
    "    sample_entry = eval_dataset[0]\n",
    "    print(\"\\nSample Entry:\")\n",
    "    print(f\"  Post (Input): {sample_entry['info']['post'][:300]}...\")\n",
    "    print(f\"  Reference Summary (Target): {sample_entry['summaries'][0]['text']}\") \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}. This might be due to connectivity or dataset access issues.\")\n",
    "    print(\"Creating a dummy dataset for fallback.\")\n",
    "    dummy_data = {\n",
    "        'info': [{'post': 'This is a long post about the benefits of MLflow for MLOps. MLflow helps track experiments, package models, and manage the ML lifecycle.'}] * num_eval_samples,\n",
    "        'summaries': [[{'text': 'MLflow is great for MLOps.'}]] * num_eval_samples\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(dummy_data)\n",
    "    print(f\"Using dummy dataset with {len(eval_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting LLMs for Evaluation\n",
    "We will evaluate three different models to see how they perform on the summarization task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Base `Qwen/Qwen3-0.6B`\n",
    "This is the pre-trained Qwen3-0.6B model without any further fine-tuning from our side. We'll use quantization for efficient inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Fine-tuned `Qwen/Qwen3-0.6B` (Recipe Bot from Notebook 4)\n",
    "This is the Qwen3-0.6B model that we fine-tuned on recipe generation in Notebook 4. It will be interesting to see how this domain-specific fine-tuning affects its performance on a general summarization task. We'll load the LoRA adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: `google/flan-t5-small` (Baseline Instruction Model)\n",
    "Flan-T5 is a well-known family of instruction-tuned models. The 'small' version is efficient and provides a good baseline for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load these models sequentially to manage VRAM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overview of Evaluation Metrics\n",
    "\n",
    "We'll use a combination of metrics, primarily those supported by `mlflow.evaluate` for summarization:\n",
    "\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Measures overlap between the generated summary and reference summary (e.g., ROUGE-1, ROUGE-2, ROUGE-L for unigram, bigram, and longest common subsequence overlap).\n",
    "- **BERTScore:** Computes similarity between generated and reference summaries using contextual embeddings from BERT, often correlating better with human judgment than ROUGE.\n",
    "- **Perplexity (via `mlflow.evaluate`):** A measure of how well a probability model predicts a sample. Lower perplexity generally indicates better fluency and coherence of the generated text by the model itself (not comparing to a reference, but internal consistency). *Note: `mlflow.evaluate` can compute perplexity if the model is a `transformers` pipeline or model.* \n",
    "\n",
    "MLflow's `evaluate` API can also compute other metrics like `exact_match`, `f1`, and potentially model-based metrics if configured (e.g., toxicity, PII detection using an LLM as a judge, though this is more advanced).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating LLMs with `mlflow.evaluate()`\n",
    "\n",
    "The `mlflow.evaluate()` API provides a structured way to evaluate models, especially for common NLP tasks like text summarization. It requires:\n",
    "- A `model` (can be a model URI, a `transformers` pipeline, or a custom Python function).\n",
    "- `data` (a Pandas DataFrame or a dataset path).\n",
    "- `targets` (column name for reference/ground truth).\n",
    "- `inputs` (column name for model input text).\n",
    "- `model_type` (e.g., `\"text-summarization\"`, `\"question-answering\"`, `\"text-generation\"`).\n",
    "- `metrics` (a list of metric names or custom metric functions).\n",
    "- `evaluators` and `evaluator_config` for more advanced, model-based evaluations.\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Evaluation Data\n",
    "`mlflow.evaluate` often works best with Pandas DataFrames. Let's convert our Hugging Face dataset subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame({\n",
    "    \"prompt_text\": [entry['info']['post'] for entry in eval_dataset],\n",
    "    \"reference_summary\": [entry['summaries'][0]['text'] for entry in eval_dataset]\n",
    "})\n",
    "\n",
    "print(\"Evaluation DataFrame prepared:\")\n",
    "print(eval_dataset.column_names)\n",
    "print(eval_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model 1: Base `Qwen/Qwen3-0.6B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_cache()\n",
    "model_name_qwen3_base = \"Qwen/Qwen3-0.6B\"\n",
    "run_name_qwen3_base = \"Eval_Base_Qwen3_0.6B_Summarization\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading base model for evaluation: {model_name_qwen3_base}\")\n",
    "    # For mlflow.evaluate, providing a transformers pipeline is often easiest\n",
    "    # Qwen3 uses AutoModelForCausalLM\n",
    "    qwen3_base_tokenizer = AutoTokenizer.from_pretrained(model_name_qwen3_base, trust_remote_code=True)\n",
    "    if qwen3_base_tokenizer.pad_token is None:\n",
    "        qwen3_base_tokenizer.pad_token = qwen3_base_tokenizer.eos_token\n",
    "    \n",
    "    # Define a model object or pipeline for mlflow.evaluate\n",
    "    # Using a custom model wrapper for more control over generation if needed, or a pipeline\n",
    "    # For simplicity, we'll create a pipeline. Max_length for summary can be controlled.\n",
    "    qwen3_base_pipeline = pipeline(\n",
    "        \"text-generation\", # Qwen3 is a causal LM, so text-generation is appropriate for summarization prompts\n",
    "        model=model_name_qwen3_base,\n",
    "        tokenizer=qwen3_base_tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1, # Use GPU if available\n",
    "        trust_remote_code=True,\n",
    "        model_kwargs={ # Pass quantization here if desired and supported by pipeline for this model\n",
    "            \"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16)\n",
    "        }\n",
    "    )\n",
    "    print(\"Base Qwen3-0.6B pipeline created.\")\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name_qwen3_base) as run:\n",
    "        mlflow.log_param(\"model_name\", model_name_qwen3_base)\n",
    "        mlflow.log_param(\"evaluation_task\", \"text-summarization\")\n",
    "        mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name}\")\n",
    "        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "        mlflow.set_tag(\"model_type\", \"base_llm\")\n",
    "\n",
    "        print(f\"Starting mlflow.evaluate for {model_name_qwen3_base}...\")\n",
    "        # Note: mlflow.evaluate might re-tokenize or handle data differently.\n",
    "        # We need to provide a prompt that guides the model towards summarization.\n",
    "        # For text-generation models used for summarization, the input needs to be formatted as a prompt.\n",
    "        # `mlflow.evaluate` for text-summarization might expect a model that directly takes text and returns summary.\n",
    "        # Let's try using model_type=\"text-generation\" and crafting a summarization prompt within the input data.\n",
    "\n",
    "        # Create a new DataFrame with a summarization prompt for text-generation models\n",
    "        eval_df_for_gen = eval_df.copy()\n",
    "        eval_df_for_gen[\"generation_prompt\"] = eval_df_for_gen[\"prompt_text\"].apply(lambda x: f\"Summarize the following text:\\n\\n{x}\\n\\nSummary:\")\n",
    "        \n",
    "        # The pipeline needs to be wrapped or adapted if its output isn't directly the summary text\n",
    "        # For text-generation, it outputs a list of dicts with 'generated_text'\n",
    "        # We'll create a simple wrapper for mlflow.evaluate\n",
    "        class SummarizationPipelineWrapper:\n",
    "            def __init__(self, pipeline_obj):\n",
    "                self.pipeline = pipeline_obj\n",
    "            def predict(self, X):\n",
    "                prompts = X[\"generation_prompt\"].tolist()\n",
    "                # Pipeline expects list of strings\n",
    "                outputs = self.pipeline(prompts, max_new_tokens=100, num_return_sequences=1, do_sample=False, pad_token_id=self.pipeline.tokenizer.eos_token_id)\n",
    "                # Extract just the generated summary part AFTER the prompt\n",
    "                results = []\n",
    "                for i, out_list in enumerate(outputs):\n",
    "                    full_text = out_list[0]['generated_text']\n",
    "                    # Remove original prompt to get just the summary\n",
    "                    summary = full_text.replace(prompts[i], \"\").strip()\n",
    "                    results.append(summary)\n",
    "                return pd.Series(results)\n",
    "\n",
    "        summarization_model_qwen3_base = SummarizationPipelineWrapper(qwen3_base_pipeline)\n",
    "\n",
    "        results = mlflow.evaluate(\n",
    "            model=summarization_model_qwen3_base, # Our wrapped pipeline\n",
    "            data=eval_df_for_gen, # Use df with 'generation_prompt'\n",
    "            targets=\"reference_summary\",\n",
    "            input_example=eval_df_for_gen.head(1), # `inputs` arg name changed to `input_example` in some versions for data profiling.\n",
    "                                                  # Or provide 'feature_names' if model takes a dict.\n",
    "                                                  # For our wrapper, it expects a DataFrame with 'generation_prompt'.\n",
    "            # For a model that directly takes the column name for prediction:\n",
    "            # feature_names=[\"generation_prompt\"], # Or just inputs=\"generation_prompt\" if model.predict handles column name\n",
    "            model_type=\"text\", # Use generic 'text' model_type for this wrapper\n",
    "                                 # as \"text-summarization\" has specific expectations for model signature.\n",
    "            # We can specify metrics manually if \"text-summarization\" type isn't fully compatible with our wrapped pipeline\n",
    "            # Or, let mlflow.evaluate pick defaults for \"text\" if possible, then add custom ones.\n",
    "            # For now, let's rely on standard behavior for a text model. \n",
    "            # mlflow.evaluate will by default save predictions if `predictions` is not set to `None`\n",
    "        )\n",
    "        print(f\"Base Qwen3-0.6B evaluation results:\\n{results.metrics}\")\n",
    "        if results.artifacts and \"eval_results_table.json\" in results.artifacts:\n",
    "             print(f\"Evaluation table artifact path: {results.artifacts['eval_results_table.json'].uri}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating base Qwen3-0.6B: {e}\")\n",
    "finally:\n",
    "    del qwen3_base_pipeline\n",
    "    if 'summarization_model_qwen3_base' in locals(): del summarization_model_qwen3_base\n",
    "    clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model 2: Fine-tuned `Qwen/Qwen3-0.6B` (Recipe Bot)\n",
    "We load the LoRA adapter trained in Notebook 4. **Important:** The path to the adapter must be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_cache()\n",
    "model_name_qwen3_ft = \"Qwen/Qwen3-0.6B_FineTuned_RecipeBot\"\n",
    "run_name_qwen3_ft = \"Eval_FineTuned_Qwen3_0.6B_Summarization\"\n",
    "# Ensure this path points to where your Qwen3 fine-tuned adapter from Notebook 4 was saved\n",
    "qwen3_adapter_path = \"./qwen3_0.6b_recipe_finetuned_adapters/final_adapter\" \n",
    "\n",
    "if not os.path.exists(qwen3_adapter_path):\n",
    "    print(f\"Fine-tuned Qwen3 adapter not found at {qwen3_adapter_path}. Skipping evaluation.\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"Loading fine-tuned Qwen3-0.6B model for evaluation from adapter: {qwen3_adapter_path}\")\n",
    "        qwen3_ft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen3-0.6B\", \n",
    "            quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16),\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        qwen3_ft_model = PeftModel.from_pretrained(qwen3_ft_base_model, qwen3_adapter_path, is_trainable=False)\n",
    "        qwen3_ft_model.eval()\n",
    "        \n",
    "        qwen3_ft_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\", trust_remote_code=True)\n",
    "        if qwen3_ft_tokenizer.pad_token is None:\n",
    "            qwen3_ft_tokenizer.pad_token = qwen3_ft_tokenizer.eos_token\n",
    "\n",
    "        qwen3_ft_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=qwen3_ft_model,\n",
    "            tokenizer=qwen3_ft_tokenizer,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            trust_remote_code=True # May not be needed if model object is passed\n",
    "        )\n",
    "        print(\"Fine-tuned Qwen3-0.6B pipeline created.\")\n",
    "        \n",
    "        # Use the same wrapper and evaluation DataFrame structure as for the base model\n",
    "        summarization_model_qwen3_ft = SummarizationPipelineWrapper(qwen3_ft_pipeline)\n",
    "        eval_df_for_gen_ft = eval_df.copy()\n",
    "        eval_df_for_gen_ft[\"generation_prompt\"] = eval_df_for_gen_ft[\"prompt_text\"].apply(lambda x: f\"Summarize the following text:\\n\\n{x}\\n\\nSummary:\")\n",
    "\n",
    "        with mlflow.start_run(run_name=run_name_qwen3_ft) as run:\n",
    "            mlflow.log_param(\"model_name\", model_name_qwen3_ft)\n",
    "            mlflow.log_param(\"adapter_path\", qwen3_adapter_path)\n",
    "            mlflow.log_param(\"evaluation_task\", \"text-summarization\")\n",
    "            mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name}\")\n",
    "            mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "            mlflow.set_tag(\"model_type\", \"fine_tuned_llm\")\n",
    "\n",
    "            print(f\"Starting mlflow.evaluate for {model_name_qwen3_ft}...\")\n",
    "            results_ft = mlflow.evaluate(\n",
    "                model=summarization_model_qwen3_ft,\n",
    "                data=eval_df_for_gen_ft,\n",
    "                targets=\"reference_summary\",\n",
    "                input_example=eval_df_for_gen_ft.head(1),\n",
    "                model_type=\"text\", \n",
    "            )\n",
    "            print(f\"Fine-tuned Qwen3-0.6B evaluation results:\\n{results_ft.metrics}\")\n",
    "            if results_ft.artifacts and \"eval_results_table.json\" in results_ft.artifacts:\n",
    "                print(f\"Evaluation table artifact path: {results_ft.artifacts['eval_results_table.json'].uri}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating fine-tuned Qwen3-0.6B: {e}\")\n",
    "    finally:\n",
    "        del qwen3_ft_base_model\n",
    "        del qwen3_ft_model\n",
    "        del qwen3_ft_pipeline\n",
    "        if 'summarization_model_qwen3_ft' in locals(): del summarization_model_qwen3_ft\n",
    "        clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The Qwen3 model fine-tuned on recipes might perform differently (potentially worse) on general summarization compared to the base model. This evaluation helps quantify that effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model 3: `google/flan-t5-small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_cache()\n",
    "model_name_flan_t5 = \"google/flan-t5-small\"\n",
    "run_name_flan_t5 = \"Eval_Flan_T5_Small_Summarization\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading Flan-T5 model for evaluation: {model_name_flan_t5}\")\n",
    "    # Flan-T5 is a Seq2Seq model, suitable for text2text-generation pipeline\n",
    "    flan_t5_pipeline = pipeline(\n",
    "        \"text2text-generation\", \n",
    "        model=model_name_flan_t5,\n",
    "        tokenizer=model_name_flan_t5, # Tokenizer can be specified by name\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        # No quantization for Flan-T5 small, it's already tiny\n",
    "    )\n",
    "    print(\"Flan-T5-small pipeline created.\")\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name_flan_t5) as run:\n",
    "        mlflow.log_param(\"model_name\", model_name_flan_t5)\n",
    "        mlflow.log_param(\"evaluation_task\", \"text-summarization\")\n",
    "        mlflow.log_param(\"dataset_name\", f\"{dataset_name}/{dataset_config_name}\")\n",
    "        mlflow.log_param(\"num_eval_samples\", num_eval_samples)\n",
    "        mlflow.set_tag(\"model_type\", \"instruction_tuned_baseline\")\n",
    "\n",
    "        # Flan-T5 is good at following instructions, so a direct prompt is fine.\n",
    "        # The text2text-generation pipeline directly outputs the generated text.\n",
    "        class FlanT5SummarizationWrapper:\n",
    "            def __init__(self, pipeline_obj):\n",
    "                self.pipeline = pipeline_obj\n",
    "            def predict(self, X):\n",
    "                prompts = X[\"prompt_text\"].apply(lambda x: f\"Summarize: {x}\").tolist()\n",
    "                outputs = self.pipeline(prompts, max_length=100, num_return_sequences=1, do_sample=False)\n",
    "                # Output is list of dicts [{'generated_text': '...'}]\n",
    "                return pd.Series([out['generated_text'] for out in outputs])\n",
    "\n",
    "        summarization_model_flan_t5 = FlanT5SummarizationWrapper(flan_t5_pipeline)\n",
    "        eval_df_for_flan = eval_df.copy() # Input column will be 'prompt_text' for wrapper\n",
    "\n",
    "        print(f\"Starting mlflow.evaluate for {model_name_flan_t5}...\")\n",
    "        results_flan_t5 = mlflow.evaluate(\n",
    "            model=summarization_model_flan_t5,\n",
    "            data=eval_df_for_flan,\n",
    "            targets=\"reference_summary\",\n",
    "            input_example=eval_df_for_flan.head(1),\n",
    "            model_type=\"text\", # Use generic text type\n",
    "        )\n",
    "        print(f\"Flan-T5-small evaluation results:\\n{results_flan_t5.metrics}\")\n",
    "        if results_flan_t5.artifacts and \"eval_results_table.json\" in results_flan_t5.artifacts:\n",
    "             print(f\"Evaluation table artifact path: {results_flan_t5.artifacts['eval_results_table.json'].uri}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating Flan-T5-small: {e}\")\n",
    "finally:\n",
    "    del flan_t5_pipeline\n",
    "    if 'summarization_model_flan_t5' in locals(): del summarization_model_flan_t5\n",
    "    clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Model Performance in MLflow UI\n",
    "\n",
    "Now, the power of MLflow comes into play! Open the MLflow UI (`mlflow ui` from the directory containing `mlruns`).\n",
    "\n",
    "1.  Navigate to the `LLM_Evaluation_Summarization_Benchmark` experiment.\n",
    "2.  You should see three runs (or more, if you re-ran parts):\n",
    "    - `Eval_Base_Qwen3_0.6B_Summarization`\n",
    "    - `Eval_FineTuned_Qwen3_0.6B_Summarization`\n",
    "    - `Eval_Flan_T5_Small_Summarization`\n",
    "3.  **Select all these runs** by checking the boxes next to them.\n",
    "4.  Click the **\"Compare\"** button.\n",
    "\n",
    "**In the Comparison View:**\n",
    "- **Parameters:** You can see the `model_name`, `dataset_name`, etc., for each run.\n",
    "- **Metrics:** This is where it gets interesting! \n",
    "    - `mlflow.evaluate` logs various metrics it computes (e.g., from `rouge`, `bertscore`, `exact_match`, `perplexity` if computed). The exact names might be like `rouge1`, `rougeL`, `bertscore_precision`, etc. \n",
    "    - You can sort the table by any metric (e.g., sort by `rougeL` descending) to see which model performed best on that specific metric.\n",
    "- **Artifacts:** For each run, `mlflow.evaluate` saves an `eval_results_table.json` (and often a `.html` version) in the artifacts. This table contains the input prompts, generated outputs, reference targets, and per-sample metric scores. This is invaluable for qualitative analysis and error inspection.\n",
    "\n",
    "![MLFlow UI](https://blog.min.io/content/images/2025/03/Screenshot-2025-03-10-at-3.30.33-PM.png)\n",
    "\n",
    "This comparison view effectively creates a **leaderboard** for your evaluated models on this specific task and dataset, all managed and visualized by MLflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Brief: Advanced Evaluation Concepts & Tools\n",
    "\n",
    "While `mlflow.evaluate` and standard metrics give a good starting point, LLM evaluation is a rapidly evolving field. Some advanced concepts and tools include:\n",
    "\n",
    "- **Human Evaluation:** Still the gold standard for nuanced aspects like coherence, creativity, helpfulness. Platforms exist to manage human annotation workflows.\n",
    "- **LLM-as-a-Judge:** Using a powerful LLM (like GPT-4) to evaluate the output of another LLM based on predefined criteria or a rubric. `mlflow.evaluate` has some capabilities here via `evaluators` like `mlflow.metrics.genai`.\n",
    "- **Task-Specific Benchmarks:** For specific tasks, dedicated benchmarks exist (e.g., HumanEval for code generation, MMLU for broad knowledge, BigBench for challenging reasoning tasks).\n",
    "- **Specialized Evaluation Frameworks:**\n",
    "    - **Ragas:** For evaluating RAG pipelines (retrieval and generation quality).\n",
    "    - **TruLens:** Focuses on explainability and tracking quality for LLM apps.\n",
    "    - **DeepEval:** Offers a suite of metrics for in-depth LLM evaluation.\n",
    "    - **EleutherAI LM Evaluation Harness:** A comprehensive framework for running many standard academic benchmarks.\n",
    "- **Ethical AI & Responsible AI Metrics:** Evaluating for bias, fairness, toxicity, robustness against adversarial attacks. These are critical for production systems.\n",
    "\n",
    "MLflow can often be integrated with these tools to store their results, providing a central dashboard for all your evaluation efforts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "In this notebook, we've learned how to systematically evaluate and benchmark LLMs:\n",
    "\n",
    "- **Structured Evaluation:** Understood the importance of a consistent process for evaluating LLMs on specific tasks and datasets.\n",
    "- **`mlflow.evaluate()` for LLMs:** Leveraged MLflow's dedicated evaluation API to assess models on text summarization, automatically computing relevant metrics.\n",
    "- **Metric-Driven Comparison:** Used metrics like ROUGE and BERTScore to quantify differences in model performance.\n",
    "- **Comparative Benchmarking:** Compared a base pre-trained model (`Qwen3-0.6B`), a domain-fine-tuned version (our Qwen3 recipe bot), and an instruction-tuned baseline (`Flan-T5-small`).\n",
    "- **MLflow for Centralized Results:** Used MLflow to log all evaluation parameters, metrics, and qualitative artifacts (like per-sample predictions), enabling easy comparison and leaderboard creation via the UI.\n",
    "- **Qualitative Insights:** Recognized the value of inspecting generated outputs (available in MLflow artifacts) alongside quantitative metrics.\n",
    "\n",
    "This systematic evaluation approach is vital for iterating on LLM development, whether you're choosing a base model, assessing fine-tuning impact, or comparing different prompting strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Engaging Resources and Further Reading\n",
    "\n",
    "To deepen your understanding of LLM evaluation:\n",
    "\n",
    "- **MLflow Documentation:**\n",
    "    - [MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html)\n",
    "    - [Built-in Metrics for LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/metrics.html)\n",
    "- **Hugging Face Evaluate Library:**\n",
    "    - [Hugging Face `evaluate` Documentation](https://huggingface.co/docs/evaluate/index)\n",
    "    - [List of Available Metrics](https://huggingface.co/spaces/evaluate-metric)\n",
    "- **Key Evaluation Papers & Concepts:**\n",
    "    - [ROUGE Paper (Lin, 2004)](https://aclanthology.org/W04-1013/)\n",
    "    - [BERTScore Paper (Zhang et al., 2019)](https://arxiv.org/abs/1904.09675)\n",
    "    - [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena (Zheng et al., 2023 - for LLM-as-judge concepts)](https://arxiv.org/abs/2306.05685)\n",
    "- **LLM Evaluation Leaderboards & Platforms:**\n",
    "    - [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
    "    - [Chatbot Arena (LMSys)](https://chat.lmsys.org/)\n",
    "\n",
    "--- \n",
    "\n",
    "Fantastic work on completing this comprehensive LLM evaluation notebook! You're now well-equipped to assess and compare different language models using robust methodologies and MLflow.\n",
    "\n",
    "**Coming Up Next (Notebook 7):** We'll shift gears to building more dynamic and interactive AI systems by exploring Tool-Calling Agents with LangGraph, Ollama, and, of course, tracking it all with MLflow.\n",
    "\n",
    "![Keep Learning](https://memento.epfl.ch/image/23136/1440x810.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
